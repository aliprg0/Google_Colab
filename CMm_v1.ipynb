{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_5bxbCoe9do9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6989b236-4a32-4e4c-f39a-ca0862ce37b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 7.5 MB/s \n",
            "\u001b[?25hCollecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.8.0 requests-2.27.1 yfinance-0.1.70\n",
            "Collecting yahooquery\n",
            "  Downloading yahooquery-2.2.15-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting requests-futures>=1.0.0\n",
            "  Downloading requests_futures-1.0.0-py2.py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (4.64.0)\n",
            "Requirement already satisfied: lxml>=4.6.2 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (4.8.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yahooquery) (1.15.0)\n",
            "Requirement already satisfied: requests>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from requests-futures>=1.0.0->yahooquery) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2.0.12)\n",
            "Installing collected packages: requests-futures, yahooquery\n",
            "Successfully installed requests-futures-1.0.0 yahooquery-2.2.15\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import random \n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IB_YMoe09qVP"
      },
      "outputs": [],
      "source": [
        "clmns = [\n",
        "    \"Open 12-11\",\"Open 11-10\",\"Open 10-9\",\"Open 9-8\",\"Open 8-7\",\"Open 7-6\",\"Open 6-5\",\"Open 5-4\",\"Open 4-3\",\"Open 3-2\",\"Open 2-1\",\n",
        "    \"Close 12-11\",\"Close 11-10\",\"Close 10-9\",\"Close 9-8\",\"Close 8-7\",\"Close 7-6\",\"Close 6-5\",\"Close 5-4\",\"Close 4-3\",\"Close 3-2\",\"Close 2-1\",\n",
        "    \"High 12-11\",\"High 11-10\",\"High 10-9\",\"High 9-8\",\"High 8-7\",\"High 7-6\",\"High 6-5\",\"High 5-4\",\"High 4-3\",\"High 3-2\",\"High 2-1\",\n",
        "    \"Low 12-11\",\"Low 11-10\",\"Low 10-9\",\"Low 9-8\",\"Low 8-7\",\"Low 7-6\",\"Low 6-5\",\"Low 5-4\",\"Low 4-3\",\"Low 3-2\",\"Low 2-1\",\n",
        "    \"AdjV 12-11\",\"AdjV 11-10\",\"AdjV 10-9\",\"AdjV 9-8\",\"AdjV 8-7\",\"AdjV 7-6\",\"AdjV 6-5\",\"AdjV 5-4\",\"AdjV 4-3\",\"AdjV 3-2\",\"AdjV 2-1\",\n",
        "    \"Volume 12-11\",\"Volume 11-10\",\"Volume 10-9\",\"Volume 9-8\",\"Volume 8-7\",\"Volume 7-6\",\"Volume 6-5\",\"Volume 5-4\",\"Volume 4-3\",\"Volume 3-2\",\"Volume 2-1\",\n",
        "    \"suggestion\"]\n",
        "\n",
        "def work_with_dir():\n",
        "  if os.path.exists(\"/content/data/\"):\n",
        "    shutil.rmtree(\"/content/data/\", ignore_errors=True)\n",
        "    print(\"Data Folder Removed\")\n",
        "    os.mkdir(\"/content/data/\")\n",
        "\n",
        "  if not os.path.exists(\"/content/data/\"):\n",
        "    os.mkdir(\"/content/data/\")\n",
        "  \n",
        "  if not os.path.exists(\"/content/extracted/\"):\n",
        "    os.mkdir(\"/content/extracted/\")\n",
        "def read_txt_list():\n",
        "  with open(\"yahoo_stocklist.txt\",\"r\")as f:\n",
        "    lines = f.readlines()\n",
        "    nlines = []\n",
        "    for line in lines:\n",
        "       nlines.append(line.strip())\n",
        "    return nlines\n",
        "def read_syms_from_txt():\n",
        "  with open(\"syms.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  lst = []\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    lst.append(line)\n",
        "  symbols = lst\n",
        "  return symbols\n",
        "def get_crypto_syms():\n",
        "   # 'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in',\n",
        "   screens = [\n",
        "       'all_cryptocurrencies_us', 'all_cryptocurrencies_au', 'all_cryptocurrencies_ca', 'all_cryptocurrencies_eu', 'all_cryptocurrencies_gb', 'all_cryptocurrencies_in', ]\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   # print(len(symbols))\n",
        "   # pieces = 15\n",
        "   # new_arrays = np.array_split(symbols, pieces)\n",
        "   return symbols\n",
        "def spliting(data):\n",
        "  X = data.drop([\"yes\",\"no\"], axis=1)\n",
        "  y = data[[\"yes\",\"no\"]]\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.2)\n",
        "  print(xTrain.shape, end=\" \")\n",
        "  print(yTrain.shape)\n",
        "  print(xTest.shape, end=\" \")\n",
        "  print(yTest.shape)\n",
        "  return xTrain, xTest, yTrain, yTest\n",
        "def scaler(row):\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    row = scaler.fit_transform(row)\n",
        "    return row\n",
        "def process(data):\n",
        "    data = data.dropna()\n",
        "    row = []\n",
        "    if len(data.columns) == 7:\n",
        "      data = data.iloc[: , 1:]        \n",
        "    data = np.array(data)\n",
        "    llst = [0, 1, 2, 3, 4, 5]\n",
        "    for i in range(12, data.shape[0]):\n",
        "        grow = []\n",
        "        srow = []\n",
        "\n",
        "        for j in llst:\n",
        "           srow.append([\n",
        "               data[i-1][j] - data[i-2][j],\n",
        "               data[i-2][j] - data[i-3][j],\n",
        "               data[i-3][j] - data[i-4][j],\n",
        "               data[i-4][j] - data[i-5][j],\n",
        "               data[i-5][j] - data[i-6][j],\n",
        "               data[i-6][j] - data[i-7][j],\n",
        "               data[i-7][j] - data[i-8][j],\n",
        "               data[i-8][j] - data[i-9][j],\n",
        "               data[i-9][j] - data[i-10][j],\n",
        "               data[i-10][j] - data[i-11][j],\n",
        "               data[i-11][j] - data[i-12][j]\n",
        "           ])\n",
        "\n",
        "        for lst in srow:\n",
        "            mm = np.array(lst)\n",
        "            mm = np.reshape(mm, (-1, 1))\n",
        "            grow.append(scaler(mm))\n",
        "\n",
        "        sugg = \"no\"\n",
        "        if data[i][3] > data[i][0]:\n",
        "            sugg = \"yes\"\n",
        "\n",
        "        arr = np.array(grow).flatten()\n",
        "        arr = np.append(arr, sugg)\n",
        "        row.append(arr)\n",
        "\n",
        "\n",
        "    grow = []\n",
        "    srow = []\n",
        "    llst = []\n",
        "    data = []\n",
        "    arr = []\n",
        "    mm = []\n",
        "\n",
        "    return np.array(row)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data(symbols,periodd,intervall):\n",
        "  \n",
        "  indexx = 100\n",
        "  work_with_dir()\n",
        "  for symbol in symbols:\n",
        "    if ((symbols.index(symbol)+1) % 100 == 0):\n",
        "      print(f\" -- {indexx}\",end=\"\")\n",
        "      indexx = indexx + 100\n",
        "\n",
        "    try:\n",
        "        data = yf.download(symbol, period=periodd,interval=intervall, progress=False,show_errors=False)\n",
        "        if data.empty:\n",
        "           pass\n",
        "        else:\n",
        "           if data.shape[0] > 12:\n",
        "             data.to_csv(f\"/content/data/{symbol}.csv\")\n",
        "             \n",
        "    except:\n",
        "       print(\"Error!\")\n",
        "  print(\" \")\n",
        "  print(f\"Files In Data : {len(os.listdir('/content/data/'))}\")\n",
        "def each_file_proc(files,now,index):\n",
        "     data = []\n",
        "     unattached_dfs = []\n",
        "     files = list(files)\n",
        "     for file in files:\n",
        "        print(f\"{files.index(file)+1+index}\",end=\" \")\n",
        "        if (files.index(file)+index+1) % 40 == 0:\n",
        "          print(\" \")\n",
        "        address = f\"/content/data/{file}\"\n",
        "        unattached_dfs.append(process(pd.read_csv(address)))\n",
        "     data = np.array(unattached_dfs[0])\n",
        "     for i in unattached_dfs[1:]:\n",
        "           data = np.append(data, np.array(i), axis=0)\n",
        "        \n",
        "     unattached_dfs = []\n",
        "  \n",
        "     data = pd.DataFrame(data, columns=clmns)\n",
        "     sugg = data[\"suggestion\"]\n",
        "     data.drop(\"suggestion\",axis=1,inplace=True)\n",
        "     sugg = pd.get_dummies(sugg)\n",
        "     data = pd.concat([data,sugg],axis=1)\n",
        "     data = data.astype(float)\n",
        "     right_now = datetime.now().strftime(\"%H%M%S%f\")\n",
        "     data.to_csv(f\"/content/extracted/{now}/{right_now}.csv\")  \n",
        "def extract_data(pieces):\n",
        "  pd.options.mode.chained_assignment = None\n",
        "\n",
        "  files = os.listdir(\"/content/data/\")\n",
        "  new_files = np.array_split(files, pieces)\n",
        "  print(\"Processing File:\")\n",
        "  now = datetime.now().strftime(\"%H%M%S\")\n",
        "  os.mkdir(f\"/content/extracted/{now}/\")\n",
        "  \n",
        "  index = 0 \n",
        "  for files in new_files:\n",
        "     \n",
        "     each_file_proc(files,now,index)\n",
        "     index = index + len(files)\n",
        "  print(\" \")\n",
        "  return now\n",
        "def delete_all_csv(now):\n",
        "   path = f'/content/extracted/{now}/*.csv'\n",
        "   files = glob.glob(path)\n",
        "   for file in files:\n",
        "       os.remove(file)\n",
        "def make_df(now):\n",
        "   path = f'/content/extracted/{now}/*.parquet'\n",
        "   files = glob.glob(path)\n",
        "   #data = pd.DataFrame()\n",
        "   data = pd.DataFrame()\n",
        "   for adr in files:\n",
        "     data =pd.concat([data,pd.read_parquet(adr)])\n",
        "   if \"Unnamed: 0\" in data:\n",
        "     data.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
        "   print(data.shape)\n",
        "   xTrain,xTest,yTrain,yTest = spliting(data)\n",
        "   data.to_parquet(f'/content/extracted/{now}/data.parquet')\n",
        "   delete_all_csv(now)\n",
        "   data = []\n",
        "   return xTrain,xTest,yTrain,yTest\n",
        "def to_par(now,howmanyfiles): \n",
        "    files = os.listdir(f\"/content/extracted/{now}/\")\n",
        "    addresses = []\n",
        "    for file in files:\n",
        "      addresses.append(f\"/content/extracted/{now}/{file}\")\n",
        "    new_adr = np.array_split(addresses,howmanyfiles)\n",
        "    for adrs in new_adr:\n",
        "      datas = []\n",
        "      for adr in adrs:\n",
        "        datas.append(pd.read_csv(adr))\n",
        "      rnow = datetime.now().strftime(\"%H%M%S%f\")\n",
        "      datas = pd.concat(datas)\n",
        "      datas.to_parquet(f\"/content/extracted/{now}/part_{rnow}.parquet\")      "
      ],
      "metadata": {
        "id": "AMR8z1BIS-M_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIAuU_ILbU27"
      },
      "outputs": [],
      "source": [
        "symbols = get_crypto_syms()\n",
        "#symbols = read_txt_list()\n",
        "#symbols = read_syms_from_txt()\n",
        "#symbols = [\"btc-usd\",\"eth-usd\",\"trx-usd\"]\n",
        "\n",
        "print(f\"Symbols : {len(symbols)}\")\n",
        "download_data(symbols,\"7d\",\"5m\")\n",
        "folder_name = extract_data(100)\n",
        "to_par(folder_name,5)\n",
        "xTrain,xTest,yTrain,yTest = make_df(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xTrain,xTest,yTrain,yTest = spliting(data)"
      ],
      "metadata": {
        "id": "ZqY06GHGdIfD",
        "outputId": "584b82f2-2e6f-49d5-a80b-b11af776eec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1939475, 66) (1939475, 2)\n",
            "(484869, 66) (484869, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xN93WT9e8ueQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c69340-dfbb-4a19-a5c6-0ccf16707ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1024)              68608     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2)                 2050      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,219,458\n",
            "Trainable params: 3,219,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "model.add(Dense(1000, activation='relu', input_shape=(xTrain.shape[1],)))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(700, activation='relu'))\n",
        "model.add(Dense(700, activation='relu'))\n",
        "model.add(Dense(700, activation='relu'))\n",
        "model.add(Dense(700, activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adamax()\n",
        "\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SBxPzRd89uy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021c8028-79fa-4306-9477-f91783e1066c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1940/1940 [==============================] - 39s 19ms/step - loss: 0.6815 - accuracy: 0.5513 - val_loss: 0.6737 - val_accuracy: 0.5707\n",
            "Epoch 2/30\n",
            "1940/1940 [==============================] - 34s 17ms/step - loss: 0.6585 - accuracy: 0.5950 - val_loss: 0.6507 - val_accuracy: 0.6066\n",
            "Epoch 3/30\n",
            "1940/1940 [==============================] - 36s 19ms/step - loss: 0.6212 - accuracy: 0.6430 - val_loss: 0.6256 - val_accuracy: 0.6412\n",
            "Epoch 4/30\n",
            "1940/1940 [==============================] - 36s 18ms/step - loss: 0.5653 - accuracy: 0.6980 - val_loss: 0.5986 - val_accuracy: 0.6763\n",
            "Epoch 5/30\n",
            "1940/1940 [==============================] - 36s 18ms/step - loss: 0.4997 - accuracy: 0.7501 - val_loss: 0.5838 - val_accuracy: 0.7017\n",
            "Epoch 6/30\n",
            "1940/1940 [==============================] - 36s 19ms/step - loss: 0.4374 - accuracy: 0.7916 - val_loss: 0.5795 - val_accuracy: 0.7205\n",
            "Epoch 7/30\n",
            "1940/1940 [==============================] - 34s 17ms/step - loss: 0.3831 - accuracy: 0.8233 - val_loss: 0.5881 - val_accuracy: 0.7348\n",
            "Epoch 8/30\n",
            "1940/1940 [==============================] - 36s 19ms/step - loss: 0.3378 - accuracy: 0.8479 - val_loss: 0.6059 - val_accuracy: 0.7442\n",
            "Epoch 9/30\n",
            "1940/1940 [==============================] - 34s 17ms/step - loss: 0.2997 - accuracy: 0.8680 - val_loss: 0.6315 - val_accuracy: 0.7503\n",
            "Epoch 10/30\n",
            "1940/1940 [==============================] - 33s 17ms/step - loss: 0.2692 - accuracy: 0.8829 - val_loss: 0.6645 - val_accuracy: 0.7553\n",
            "Epoch 11/30\n",
            "1940/1940 [==============================] - 33s 17ms/step - loss: 0.2468 - accuracy: 0.8937 - val_loss: 0.6859 - val_accuracy: 0.7616\n",
            "Epoch 12/30\n",
            "1940/1940 [==============================] - 34s 17ms/step - loss: 0.2266 - accuracy: 0.9034 - val_loss: 0.7227 - val_accuracy: 0.7602\n",
            "Epoch 13/30\n",
            "1940/1940 [==============================] - 34s 18ms/step - loss: 0.2073 - accuracy: 0.9125 - val_loss: 0.7294 - val_accuracy: 0.7689\n",
            "Epoch 14/30\n",
            "1940/1940 [==============================] - 36s 18ms/step - loss: 0.1923 - accuracy: 0.9193 - val_loss: 0.7747 - val_accuracy: 0.7720\n",
            "Epoch 15/30\n",
            "1940/1940 [==============================] - 36s 18ms/step - loss: 0.1777 - accuracy: 0.9262 - val_loss: 0.8111 - val_accuracy: 0.7701\n",
            "Epoch 16/30\n",
            "1940/1940 [==============================] - 34s 17ms/step - loss: 0.1629 - accuracy: 0.9332 - val_loss: 0.9139 - val_accuracy: 0.7607\n",
            "Epoch 17/30\n",
            "1940/1940 [==============================] - 33s 17ms/step - loss: 0.1528 - accuracy: 0.9377 - val_loss: 0.8810 - val_accuracy: 0.7770\n",
            "Epoch 18/30\n",
            "1940/1940 [==============================] - 36s 18ms/step - loss: 0.1435 - accuracy: 0.9419 - val_loss: 0.8557 - val_accuracy: 0.7850\n",
            "Epoch 19/30\n",
            "1940/1940 [==============================] - 36s 19ms/step - loss: 0.1365 - accuracy: 0.9451 - val_loss: 0.8749 - val_accuracy: 0.7872\n",
            "Epoch 20/30\n",
            "1940/1940 [==============================] - 36s 19ms/step - loss: 0.1290 - accuracy: 0.9485 - val_loss: 0.8751 - val_accuracy: 0.7892\n",
            "Epoch 21/30\n",
            "1940/1940 [==============================] - 33s 17ms/step - loss: 0.1209 - accuracy: 0.9521 - val_loss: 0.8798 - val_accuracy: 0.7915\n",
            "Epoch 22/30\n",
            "1940/1940 [==============================] - 34s 18ms/step - loss: 0.1112 - accuracy: 0.9564 - val_loss: 0.9148 - val_accuracy: 0.7920\n",
            "Epoch 23/30\n",
            "1940/1940 [==============================] - 36s 19ms/step - loss: 0.1043 - accuracy: 0.9595 - val_loss: 0.9520 - val_accuracy: 0.7940\n",
            "Epoch 24/30\n",
            " 977/1940 [==============>...............] - ETA: 16s - loss: 0.1000 - accuracy: 0.9612"
          ]
        }
      ],
      "source": [
        "model.fit(xTrain,yTrain,epochs=30,batch_size=1000,validation_data=(xTest,yTest),shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8CGI7G0bxqG",
        "outputId": "f78b9c4d-8706-43c2-c009-1ebe628e862b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CMm_v1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}