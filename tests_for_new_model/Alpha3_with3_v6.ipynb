{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5bxbCoe9do9",
        "outputId": "cae2dcff-cf39-49e5-ab89-1dd45ce2c9dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 17.4 MB/s \n",
            "\u001b[?25hCollecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2022.5.18.1)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.8.0 requests-2.27.1 yfinance-0.1.70\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yahooquery\n",
            "  Downloading yahooquery-2.2.15-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting requests-futures>=1.0.0\n",
            "  Downloading requests_futures-1.0.0-py2.py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: lxml>=4.6.2 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (4.8.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yahooquery) (1.15.0)\n",
            "Requirement already satisfied: requests>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from requests-futures>=1.0.0->yahooquery) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2022.5.18.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2.10)\n",
            "Installing collected packages: requests-futures, yahooquery\n",
            "Successfully installed requests-futures-1.0.0 yahooquery-2.2.15\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tvdatafeed\n",
            "  Downloading tvdatafeed-1.2.1-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tvdatafeed) (57.4.0)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.2.0-py3-none-any.whl (983 kB)\n",
            "\u001b[K     |████████████████████████████████| 983 kB 13.2 MB/s \n",
            "\u001b[?25hCollecting websocket-client\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from tvdatafeed) (1.3.5)\n",
            "Collecting chromedriver-autoinstaller\n",
            "  Downloading chromedriver_autoinstaller-0.3.1-py3-none-any.whl (6.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tvdatafeed) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tvdatafeed) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tvdatafeed) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->tvdatafeed) (1.15.0)\n",
            "Collecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 46.9 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium->tvdatafeed) (21.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium->tvdatafeed) (2.10)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium->tvdatafeed) (2.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 59.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium->tvdatafeed) (2022.5.18.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium->tvdatafeed) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium->tvdatafeed) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium->tvdatafeed) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium->tvdatafeed) (4.2.0)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, websocket-client, selenium, chromedriver-autoinstaller, tvdatafeed\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 chromedriver-autoinstaller-0.3.1 cryptography-37.0.2 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.2.0 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 tvdatafeed-1.2.1 urllib3-1.26.9 websocket-client-1.3.2 wsproto-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 13.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly\n",
            "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "!pip install tvdatafeed\n",
        "!pip install tensorflow\n",
        "from tvDatafeed import TvDatafeed, Interval\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import random \n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IB_YMoe09qVP"
      },
      "outputs": [],
      "source": [
        "def work_with_dir():\n",
        "  if os.path.exists(\"/content/data/\"):\n",
        "    shutil.rmtree(\"/content/data/\", ignore_errors=True)\n",
        "    print(\"Data Folder Removed\")\n",
        "    os.mkdir(\"/content/data/\")\n",
        "  if not os.path.exists(\"/content/data/\"):\n",
        "    os.mkdir(\"/content/data/\")\n",
        "  if not os.path.exists(\"/content/extracted/\"):\n",
        "    os.mkdir(\"/content/extracted/\")\n",
        "def get_crypto_syms():\n",
        "   # 'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in',\n",
        "   screens = [\n",
        "       'all_cryptocurrencies_us', 'all_cryptocurrencies_au', 'all_cryptocurrencies_ca', 'all_cryptocurrencies_eu', 'all_cryptocurrencies_gb', 'all_cryptocurrencies_in', ]\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   # print(len(symbols))\n",
        "   # pieces = 15\n",
        "   # new_arrays = np.array_split(symbols, pieces)\n",
        "   return symbols\n",
        "def spliting(data):\n",
        "  X = data.drop([\"suggestion\"], axis=1)\n",
        "  y = data[[\"suggestion\"]]\n",
        "  y = pd.get_dummies(y)\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.2,random_state=99)\n",
        "  print(xTrain.shape, end=\" \")\n",
        "  print(yTrain.shape)\n",
        "  print(xTest.shape, end=\" \")\n",
        "  print(yTest.shape)\n",
        "  return xTrain, xTest, yTrain, yTest\n",
        "def download_data(symbols,periodd,intervall):\n",
        "  indexx = 100\n",
        "  work_with_dir()\n",
        "  for symbol in symbols:\n",
        "    if ((symbols.index(symbol)+1) % 100 == 0):\n",
        "      print(f\" -- {indexx}\",end=\"\")\n",
        "      indexx = indexx + 100\n",
        "    try:\n",
        "        data = yf.download(symbol, period=periodd,interval=intervall, progress=False,show_errors=False)\n",
        "        if data.empty:\n",
        "           pass\n",
        "        else:\n",
        "             data.to_csv(f\"/content/data/{symbol}.csv\")         \n",
        "    except:\n",
        "       print(\"Error!\")\n",
        "  print(\" \")\n",
        "def each_file_proc(files,now,index,min_range,max_range,how_many_future_candles,how_many_past_candles,each_row_past,interval_for_that):\n",
        "     data = []\n",
        "     unattached_dfs = []\n",
        "     files = list(files)\n",
        "     for file in files:\n",
        "        #print(f\"{files.index(file)+1+index}\",end=\" \")\n",
        "        if (files.index(file)+index+1) % 10 == 0:\n",
        "           print(f\"{files.index(file)+1+index}\",end=\" \")\n",
        "           if (files.index(file)+index+1) % 400 == 0:\n",
        "             print(\" \")\n",
        "\n",
        "        address = f\"/content/data/{file}\"\n",
        "        try:\n",
        "            unattached_dfs.append(process(pd.read_csv(address),min_range,max_range,how_many_future_candles,how_many_past_candles,each_row_past,interval_for_that))\n",
        "        except:\n",
        "          pass\n",
        "     nud = []\n",
        "     for i in unattached_dfs:\n",
        "       if i.size != 0:\n",
        "         nud.append(i)\n",
        "     data = np.concatenate(nud)\n",
        "     data = pd.DataFrame(data, columns=clmns)\n",
        "     right_now = datetime.now().strftime(\"%H%M%S%f\")\n",
        "     data.to_csv(f\"/content/extracted/{now}/{right_now}.csv\")  \n",
        "def extract_data(pieces,min_range,max_range,how_many_future_candles,how_many_past_candles,each_row_past,interval_for_that):\n",
        "  print(f\"Files In Data : {len(os.listdir('/content/data/'))}\")\n",
        "  pd.options.mode.chained_assignment = None\n",
        "  files = os.listdir(\"/content/data/\")\n",
        "  new_files = np.array_split(files, pieces)\n",
        "  print(\"Processing File:\")\n",
        "  now = datetime.now().strftime(\"%H%M%S\")\n",
        "  os.mkdir(f\"/content/extracted/{now}/\")\n",
        "  index = 0 \n",
        "  for files in new_files:\n",
        "     each_file_proc(files,now,index,min_range,max_range,how_many_future_candles,how_many_past_candles,each_row_past,interval_for_that)\n",
        "     index = index + len(files)\n",
        "  print(\" \")\n",
        "  return now\n",
        "def delete_all_csv(now):\n",
        "   path = f'/content/extracted/{now}/*.csv'\n",
        "   files = glob.glob(path)\n",
        "   for file in files:\n",
        "       os.remove(file)\n",
        "def make_df(now):\n",
        "   path = f'/content/extracted/{now}/*.parquet'\n",
        "   files = glob.glob(path)\n",
        "   #data = pd.DataFrame()\n",
        "   data = pd.DataFrame()\n",
        "   for adr in files:\n",
        "     data =pd.concat([data,pd.read_parquet(adr)])\n",
        "   if \"Unnamed: 0\" in data:\n",
        "     data.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
        "   print(data.shape)\n",
        "   xTrain,xTest,yTrain,yTest = spliting(data)\n",
        "   data.to_parquet(f'/content/extracted/{now}/data.parquet')\n",
        "   delete_all_csv(now)\n",
        "   data = []\n",
        "   return xTrain,xTest,yTrain,yTest\n",
        "def to_par(now,howmanyfiles): \n",
        "    files = os.listdir(f\"/content/extracted/{now}/\")\n",
        "    addresses = []\n",
        "    for file in files:\n",
        "      addresses.append(f\"/content/extracted/{now}/{file}\")\n",
        "    new_adr = np.array_split(addresses,howmanyfiles)\n",
        "    for adrs in new_adr:\n",
        "      datas = []\n",
        "      for adr in adrs:\n",
        "        datas.append(pd.read_csv(adr))\n",
        "      rnow = datetime.now().strftime(\"%H%M%S%f\")\n",
        "      datas = pd.concat(datas)\n",
        "      datas.to_parquet(f\"/content/extracted/{now}/part_{rnow}.parquet\")   \n",
        "def scaler(row,min_range,max_range):\n",
        "    scaler = MinMaxScaler(feature_range=(min_range, max_range))\n",
        "    row = scaler.fit_transform(row)\n",
        "    return row\n",
        "def make_y(data, i,how_many_future_candles,how_many_past_candles):\n",
        "  mean_of_next = []\n",
        "  for ix in range(i,i+how_many_future_candles):\n",
        "    mean_of_next.append(data[ix][3])\n",
        "  #mean_of_next = [element / 2 for element in mean_of_next]\n",
        "  mean_of_next = sum(mean_of_next)/how_many_future_candles\n",
        "  mean_of_previous = []\n",
        "  for ix in range(i-how_many_past_candles,i):\n",
        "    mean_of_previous.append(data[ix][3])\n",
        "  #mean_of_previous = [element / 2 for element in mean_of_previous]\n",
        "  mean_of_previous = sum(mean_of_previous)/how_many_past_candles\n",
        "  if mean_of_next > mean_of_previous:\n",
        "    return \"buy\"\n",
        "  else:\n",
        "    return \"sell\"\n",
        "def process(data,min_range,max_range,how_many_future_candles,how_many_past_candles,each_row_past,interval_for_that):\n",
        "    data = data.dropna()\n",
        "    row = []\n",
        "    if len(data.columns) == 7:\n",
        "      data = data.iloc[:, 1:]\n",
        "    data = np.array(data)\n",
        "    for i in range(each_row_past, data.shape[0]-10):\n",
        "        opens = []\n",
        "        highs = []\n",
        "        lows = []\n",
        "        closes = []           \n",
        "        for ix in range(each_row_past, 0, interval_for_that):\n",
        "            opens.append([(data[i-ix][0]+data[i-ix+1][0]+data[i-ix+2][0]) /3 - (data[i-ix+3][0]+data[i-ix+4][0]+data[i-ix+5][0])/3])\n",
        "            closes.append([(data[i-ix][3]+data[i-ix+1][3]+data[i-ix+2][3]) /3 - (data[i-ix+3][3]+data[i-ix+4][3]+data[i-ix+5][3])/3])\n",
        "        sugg = make_y(data, i,how_many_future_candles,how_many_past_candles)\n",
        "        arr = np.concatenate((np.array(opens).flatten(), np.array(highs).flatten()\n",
        "        , np.array(lows).flatten(), np.array(closes).flatten()), axis=0).reshape(-1, 1)\n",
        "        arr = scaler(arr.reshape(-1, 1),min_range,max_range)\n",
        "        arr = np.append(arr, sugg)\n",
        "        row.append(arr)\n",
        "    return np.array(row)\n",
        "def start(min_range,max_range,how_many_future_candles,how_many_past_candles,each_row_past,interval_for_that,into_how_many_csv,csv_to_parquet):\n",
        "     folder_name = extract_data(into_how_many_csv,min_range,max_range,how_many_future_candles,how_many_past_candles,each_row_past,interval_for_that)\n",
        "     to_par(folder_name,csv_to_parquet)\n",
        "     xTrain,xTest,yTrain,yTest = make_df(folder_name)\n",
        "     return xTrain,xTest,yTrain,yTest  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMR8z1BIS-M_",
        "outputId": "c1a8d7fd-02e0-4f2a-c19a-d875a1eceeae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbols : 1500\n",
            " -- 100 -- 200 -- 300 -- 400 -- 500 -- 600 -- 700 -- 800 -- 900 -- 1000 -- 1100 -- 1200 -- 1300 -- 1400 -- 1500 \n"
          ]
        }
      ],
      "source": [
        "symbols = get_crypto_syms()\n",
        "print(f\"Symbols : {len(symbols)}\")\n",
        "download_data(symbols,\"max\",\"1wk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "R8KVpQmiuj9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4bb77d1-2cc1-463b-e40f-198b6c450f1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files In Data : 1498\n",
            "Processing File:\n",
            "10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400  \n",
            "410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800  \n",
            "810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200  \n",
            "1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490  \n",
            "(62267, 51)\n",
            "(49813, 50) (49813, 2)\n",
            "(12454, 50) (12454, 2)\n",
            "suggestion_buy  suggestion_sell\n",
            "0               1                  25149\n",
            "1               0                  24664\n",
            "dtype: int64\n",
            "suggestion_buy  suggestion_sell\n",
            "0               1                  6255\n",
            "1               0                  6199\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "clmns = list(range(1, 51))\n",
        "clmns.append(\"suggestion\")\n",
        "\n",
        "min_range,max_range = -10,10\n",
        "how_many_future_candles = 3\n",
        "how_many_past_candles = 3\n",
        "each_row_past,interval_for_that = 150,-6\n",
        "\n",
        "into_how_many_csv = 50\n",
        "csv_to_parquet = 10\n",
        "\n",
        "xTrain,xTest,yTrain,yTest = start(min_range,max_range,how_many_future_candles,how_many_past_candles,each_row_past,interval_for_that,into_how_many_csv,csv_to_parquet)\n",
        "print(yTrain.value_counts())\n",
        "print(yTest.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN93WT9e8ueQ",
        "outputId": "1a9869ec-545e-4326-ed10-5939749778b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_11 (Dense)            (None, 512)               26112     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 815,106\n",
            "Trainable params: 815,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512, activation='relu', input_shape=(xTrain.shape[1],)))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adamax()\n",
        "\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(xTrain,yTrain,epochs=30,batch_size=32,validation_data=(xTest,yTest))\n",
        "acr = str(round(model.evaluate(xTest,yTest)[1],4)).replace(\"0.\",\"\")\n",
        "model.save(f\"MCG136_{acr}.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Obu1YaJrjMnC",
        "outputId": "60f05918-f79c-460e-e6ce-92de3a5c098d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.4647 - accuracy: 0.7782 - val_loss: 0.3944 - val_accuracy: 0.8195\n",
            "Epoch 2/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.3241 - accuracy: 0.8583 - val_loss: 0.2937 - val_accuracy: 0.8762\n",
            "Epoch 3/30\n",
            "1557/1557 [==============================] - 13s 9ms/step - loss: 0.2283 - accuracy: 0.9061 - val_loss: 0.2533 - val_accuracy: 0.9005\n",
            "Epoch 4/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.1773 - accuracy: 0.9277 - val_loss: 0.1987 - val_accuracy: 0.9264\n",
            "Epoch 5/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.1457 - accuracy: 0.9437 - val_loss: 0.1688 - val_accuracy: 0.9383\n",
            "Epoch 6/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.1246 - accuracy: 0.9528 - val_loss: 0.1588 - val_accuracy: 0.9435\n",
            "Epoch 7/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.1100 - accuracy: 0.9573 - val_loss: 0.1693 - val_accuracy: 0.9418\n",
            "Epoch 8/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0995 - accuracy: 0.9613 - val_loss: 0.1524 - val_accuracy: 0.9505\n",
            "Epoch 9/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0905 - accuracy: 0.9651 - val_loss: 0.1378 - val_accuracy: 0.9546\n",
            "Epoch 10/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0824 - accuracy: 0.9683 - val_loss: 0.1336 - val_accuracy: 0.9633\n",
            "Epoch 11/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0764 - accuracy: 0.9690 - val_loss: 0.1332 - val_accuracy: 0.9599\n",
            "Epoch 12/30\n",
            "1557/1557 [==============================] - 13s 9ms/step - loss: 0.0710 - accuracy: 0.9715 - val_loss: 0.1348 - val_accuracy: 0.9603\n",
            "Epoch 13/30\n",
            "1557/1557 [==============================] - 13s 9ms/step - loss: 0.0665 - accuracy: 0.9724 - val_loss: 0.1327 - val_accuracy: 0.9607\n",
            "Epoch 14/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0635 - accuracy: 0.9737 - val_loss: 0.1236 - val_accuracy: 0.9678\n",
            "Epoch 15/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0579 - accuracy: 0.9758 - val_loss: 0.1364 - val_accuracy: 0.9633\n",
            "Epoch 16/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0563 - accuracy: 0.9763 - val_loss: 0.1329 - val_accuracy: 0.9657\n",
            "Epoch 17/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0540 - accuracy: 0.9765 - val_loss: 0.1397 - val_accuracy: 0.9667\n",
            "Epoch 18/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0512 - accuracy: 0.9783 - val_loss: 0.1396 - val_accuracy: 0.9668\n",
            "Epoch 19/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0486 - accuracy: 0.9788 - val_loss: 0.1370 - val_accuracy: 0.9654\n",
            "Epoch 20/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0475 - accuracy: 0.9797 - val_loss: 0.1388 - val_accuracy: 0.9649\n",
            "Epoch 21/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0448 - accuracy: 0.9798 - val_loss: 0.1402 - val_accuracy: 0.9713\n",
            "Epoch 22/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0433 - accuracy: 0.9812 - val_loss: 0.1361 - val_accuracy: 0.9707\n",
            "Epoch 23/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0428 - accuracy: 0.9810 - val_loss: 0.1360 - val_accuracy: 0.9703\n",
            "Epoch 24/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0406 - accuracy: 0.9825 - val_loss: 0.1490 - val_accuracy: 0.9710\n",
            "Epoch 25/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0411 - accuracy: 0.9819 - val_loss: 0.1492 - val_accuracy: 0.9712\n",
            "Epoch 26/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0389 - accuracy: 0.9823 - val_loss: 0.1572 - val_accuracy: 0.9730\n",
            "Epoch 27/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0390 - accuracy: 0.9825 - val_loss: 0.1619 - val_accuracy: 0.9682\n",
            "Epoch 28/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0380 - accuracy: 0.9831 - val_loss: 0.1656 - val_accuracy: 0.9729\n",
            "Epoch 29/30\n",
            "1557/1557 [==============================] - 13s 9ms/step - loss: 0.0386 - accuracy: 0.9829 - val_loss: 0.1598 - val_accuracy: 0.9709\n",
            "Epoch 30/30\n",
            "1557/1557 [==============================] - 14s 9ms/step - loss: 0.0365 - accuracy: 0.9834 - val_loss: 0.1739 - val_accuracy: 0.9695\n",
            "390/390 [==============================] - 1s 3ms/step - loss: 0.1739 - accuracy: 0.9695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fEcDYXMtSPUz"
      },
      "outputs": [],
      "source": [
        "def process_for_prediction(data,index,min_range,max_range,each_row_past,interval_for_that):\n",
        "        i = index\n",
        "        if \"symbol\" in data.columns:\n",
        "              data.drop(\"symbol\",axis=1,inplace=True) \n",
        "        if \"datetime\" in data.columns:\n",
        "              data.drop(\"datetime\",axis=1,inplace=True)\n",
        "        if \"Adj Close\" in data.columns:\n",
        "              data.drop(\"Adj Close\",axis=1,inplace=True)\n",
        "\n",
        "        data = np.array(data)\n",
        "\n",
        "        opens = []\n",
        "        highs = []\n",
        "        lows = []\n",
        "        closes = []\n",
        "        for ix in range(each_row_past, 0, interval_for_that):\n",
        "            opens.append([(data[i-ix][0]+data[i-ix+1][0]+data[i-ix+2][0]) /3 - (data[i-ix+3][0]+data[i-ix+4][0]+data[i-ix+5][0])/3])\n",
        "            closes.append([(data[i-ix][3]+data[i-ix+1][3]+data[i-ix+2][3]) /3 - (data[i-ix+3][3]+data[i-ix+4][3]+data[i-ix+5][3])/3])\n",
        "\n",
        "        arr = np.concatenate((np.array(opens).flatten(), np.array(highs).flatten()\n",
        "        , np.array(lows).flatten(), np.array(closes).flatten()), axis=0).reshape(-1, 1)\n",
        "        arr = scaler(arr.reshape(-1, 1),min_range,max_range)\n",
        "        return arr,0\n",
        "def make_prediction_for_yf(symbol,period,timeframe,index,min_range,max_range,each_row_past,interval_for_that):\n",
        "    raw_data, mean_of_previous = process_for_prediction(\n",
        "        yf.download(symbol, period=period, interval=timeframe), index,min_range,max_range,each_row_past,interval_for_that)\n",
        "    return f\"\"\"YF   : {model.predict(np.array(raw_data).reshape(1,-1)).tolist()}\n",
        "MP   : {mean_of_previous}\n",
        "\"\"\"\n",
        "def make_prediction_for_tv(symbol,exchange,timeframe,tindex,min_range,max_range,each_row_past,interval_for_that):\n",
        "    tv = TvDatafeed()\n",
        "    raw_data, mean_of_previous = process_for_prediction(\n",
        "       tv.get_hist(symbol=symbol, exchange=exchange, interval=timeframe, n_bars=5000), tindex,min_range,max_range,each_row_past,interval_for_that)\n",
        "    return f\"\"\"TVB  : {model.predict(np.array(raw_data).reshape(1,-1)).tolist()}\n",
        "MP   : {mean_of_previous}\n",
        "\"\"\"\n",
        "def make_prediction(ysymbol,period,timeframe,tsymbol,texchange,ttimeframe,tindex,index):\n",
        "  print(make_prediction_for_yf(ysymbol,period,timeframe,index,min_range,max_range,each_row_past,interval_for_that))\n",
        "  print(make_prediction_for_tv(tsymbol,texchange,ttimeframe,tindex,min_range,max_range,each_row_past,interval_for_that))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RuUzTsN-SfnH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "4ddfbd9e-daad-4397-a119-76682c2dafed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-add90c4ec85f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"btc-usd\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"1d\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"btcusd\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"bitstamp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mInterval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_daily\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-afc2506f2286>\u001b[0m in \u001b[0;36mmake_prediction\u001b[0;34m(ysymbol, period, timeframe, tsymbol, texchange, ttimeframe, tindex, index)\u001b[0m\n\u001b[1;32m     36\u001b[0m \"\"\"\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mysymbol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperiod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtsymbol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtexchange\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mttimeframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_prediction_for_yf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mysymbol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperiod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meach_row_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterval_for_that\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_prediction_for_tv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsymbol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtexchange\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mttimeframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meach_row_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterval_for_that\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-afc2506f2286>\u001b[0m in \u001b[0;36mmake_prediction_for_yf\u001b[0;34m(symbol, period, timeframe, index, min_range, max_range, each_row_past, interval_for_that)\u001b[0m\n\u001b[1;32m     27\u001b[0m     return f\"\"\"YF   : {model.predict(np.array(raw_data).reshape(1,-1)).tolist()}\n\u001b[1;32m     28\u001b[0m \u001b[0mMP\u001b[0m   \u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmean_of_previous\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \"\"\"\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_prediction_for_tv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexchange\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meach_row_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterval_for_that\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTvDatafeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 25), found shape=(None, 50)\n"
          ]
        }
      ],
      "source": [
        "make_prediction(\"btc-usd\",\"max\",\"1d\",\"btcusd\",\"bitstamp\",Interval.in_daily,-1,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bqkwjROb3lL"
      },
      "outputs": [],
      "source": [
        "yf.download(\"btc-usd\",period=\"max\",interval=\"1mo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF-mwJc6Vmsm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Alpha3_with3_v6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}