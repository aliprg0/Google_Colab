{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqIra2YIBJsl"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "!pip install tvdatafeed\n",
        "from tvDatafeed import TvDatafeed, Interval\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import random \n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from tvDatafeed import TvDatafeed, Interval\n",
        "import multiprocessing\n",
        "import time\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "Cku-whunBURi"
      },
      "outputs": [],
      "source": [
        "def get_nasq_syms():\n",
        "  with open(\"watchlist_NASDAQ.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  line = lines[0].split(\",\")\n",
        "  symbols = []\n",
        "  for l in line:\n",
        "    x = l.split(\":\")\n",
        "    symbols.append(x[1])\n",
        "  return symbols\n",
        "def download_data_p(tv,sym,exc,tf):\n",
        "    data =tv.get_hist(sym,exchange=exc, interval=tf, n_bars=50000)\n",
        "    if data.empty:\n",
        "      pass\n",
        "    else:\n",
        "         try:\n",
        "           if np.array(data).shape[0] > 15:\n",
        "             data.to_csv(f\"/content/data/{sym}.csv\")\n",
        "         except:\n",
        "            pass\n",
        "def download_data(symbols):\n",
        "  work_with_dir()\n",
        "  tv = TvDatafeed()\n",
        "  for sym in symbols:\n",
        "    print(f'{symbols.index(sym)+1}/{len(symbols)} ',end=\" \")\n",
        "    p = multiprocessing.Process(target=download_data_p, name=\"dd\", args=(tv,sym,\"KUCOIN\",Interval.in_1_hour))\n",
        "    p.start()\n",
        "    time.sleep(2)\n",
        "    p.terminate()\n",
        "    p.join()\n",
        "def get_tbinances():\n",
        "  with open(\"bitr.txt\",\"r\") as f:\n",
        "     lines = f.readlines()\n",
        "  nl = []\n",
        "  for line in lines:\n",
        "    nl.append(line.strip().split(\":\")[1])\n",
        "  return nl\n",
        "def work_with_dir():\n",
        "  if os.path.exists(\"/content/data/\"):\n",
        "    shutil.rmtree(\"/content/data/\", ignore_errors=True)\n",
        "    print(\"Data Folder Removed\")\n",
        "    os.mkdir(\"/content/data/\")\n",
        "  if not os.path.exists(\"/content/data/\"):\n",
        "    os.mkdir(\"/content/data/\")\n",
        "  if not os.path.exists(\"/content/extracted/\"):\n",
        "    os.mkdir(\"/content/extracted/\")\n",
        "def get_crypto_syms():\n",
        "   # 'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in',\n",
        "   screens = [\n",
        "       'all_cryptocurrencies_us', 'all_cryptocurrencies_au', 'all_cryptocurrencies_ca', 'all_cryptocurrencies_eu', 'all_cryptocurrencies_gb', 'all_cryptocurrencies_in', ]\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   # print(len(symbols))\n",
        "   # pieces = 15\n",
        "   # new_arrays = np.array_split(symbols, pieces)\n",
        "   return symbols\n",
        "def spliting(data):\n",
        "  X = data.drop([\"buy\",\"sell\"], axis=1)\n",
        "  y = data[[\"buy\",\"sell\"]]\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.1)\n",
        "  print(xTrain.shape, end=\" \")\n",
        "  print(yTrain.shape)\n",
        "  print(xTest.shape, end=\" \")\n",
        "  print(yTest.shape)\n",
        "  return xTrain, xTest, yTrain, yTest\n",
        "def each_file_proc(files,now,index):\n",
        "     data = []\n",
        "     unattached_dfs = []\n",
        "     files = list(files)\n",
        "     for file in files:\n",
        "        print(f\"{files.index(file)+1+index}\",end=\" \")\n",
        "        if (files.index(file)+index+1) % 40 == 0:\n",
        "          print(\" \")\n",
        "        address = f\"/content/data/{file}\"\n",
        "        unattached_dfs.append(process(pd.read_csv(address)))\n",
        "     nud = []\n",
        "     for i in unattached_dfs:\n",
        "       if i.size != 0:\n",
        "         nud.append(i)\n",
        "     data = np.concatenate(nud)\n",
        "     unattached_dfs = []\n",
        "     data = pd.DataFrame(data, columns=clmns)\n",
        "     sugg = data[\"suggestion\"]\n",
        "     data.drop(\"suggestion\",axis=1,inplace=True)\n",
        "     sugg = pd.get_dummies(sugg)\n",
        "     data = pd.concat([data,sugg],axis=1)\n",
        "     data = data.astype(float)\n",
        "     right_now = datetime.now().strftime(\"%H%M%S%f\")\n",
        "     data.to_csv(f\"/content/extracted/{now}/{right_now}.csv\")  \n",
        "def extract_data(pieces):\n",
        "  pd.options.mode.chained_assignment = None\n",
        "  files = os.listdir(\"/content/data/\")\n",
        "  new_files = np.array_split(files, pieces)\n",
        "  print(\"Processing File:\")\n",
        "  now = datetime.now().strftime(\"%H%M%S\")\n",
        "  os.mkdir(f\"/content/extracted/{now}/\")\n",
        "  \n",
        "  index = 0 \n",
        "  for files in new_files:\n",
        "     \n",
        "     each_file_proc(files,now,index)\n",
        "     index = index + len(files)\n",
        "  print(\" \")\n",
        "  return now\n",
        "def delete_all_csv(now):\n",
        "   path = f'/content/extracted/{now}/*.csv'\n",
        "   files = glob.glob(path)\n",
        "   for file in files:\n",
        "       os.remove(file)\n",
        "def make_df(now):\n",
        "   path = f'/content/extracted/{now}/*.parquet'\n",
        "   files = glob.glob(path)\n",
        "   #data = pd.DataFrame()\n",
        "   data = pd.DataFrame()\n",
        "   for adr in files:\n",
        "     data =pd.concat([data,pd.read_parquet(adr)])\n",
        "   if \"Unnamed: 0\" in data:\n",
        "     data.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
        "   print(data.shape)\n",
        "   xTrain,xTest,yTrain,yTest = spliting(data)\n",
        "   data.to_parquet(f'/content/extracted/{now}/data.parquet')\n",
        "   delete_all_csv(now)\n",
        "   data = []\n",
        "   return xTrain,xTest,yTrain,yTest\n",
        "def to_par(now,howmanyfiles): \n",
        "    files = os.listdir(f\"/content/extracted/{now}/\")\n",
        "    addresses = []\n",
        "    for file in files:\n",
        "      addresses.append(f\"/content/extracted/{now}/{file}\")\n",
        "    new_adr = np.array_split(addresses,howmanyfiles)\n",
        "    for adrs in new_adr:\n",
        "      datas = []\n",
        "      for adr in adrs:\n",
        "        datas.append(pd.read_csv(adr))\n",
        "      rnow = datetime.now().strftime(\"%H%M%S%f\")\n",
        "      datas = pd.concat(datas)\n",
        "      datas.to_parquet(f\"/content/extracted/{now}/part_{rnow}.parquet\")   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZHam_kdREbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43194afc-6bb2-460e-c9ed-8f55154ac56c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tvDatafeed.main:you are using nologin method, data you access may be limited\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbols: 1143\n",
            "Data Folder Removed\n",
            "1/1143  2/1143  3/1143  4/1143  5/1143  6/1143  7/1143  8/1143  9/1143  10/1143  11/1143  12/1143  13/1143  14/1143  15/1143  16/1143  17/1143  18/1143  19/1143  20/1143  21/1143  22/1143  23/1143  24/1143  25/1143  26/1143  27/1143  28/1143  29/1143  30/1143  31/1143  32/1143  33/1143  34/1143  35/1143  36/1143  37/1143  38/1143  39/1143  40/1143  41/1143  42/1143  43/1143  44/1143  45/1143  46/1143  47/1143  48/1143  49/1143  50/1143  51/1143  52/1143  53/1143  54/1143  55/1143  56/1143  57/1143  58/1143  59/1143  60/1143  61/1143  62/1143  63/1143  64/1143  65/1143  66/1143  67/1143  68/1143  69/1143  70/1143  71/1143  72/1143  73/1143  74/1143  75/1143  76/1143  77/1143  78/1143  79/1143  80/1143  81/1143  82/1143  83/1143  84/1143  85/1143  86/1143  87/1143  88/1143  89/1143  90/1143  91/1143  92/1143  93/1143  94/1143  95/1143  96/1143  97/1143  98/1143  99/1143  100/1143  101/1143  102/1143  103/1143  104/1143  105/1143  106/1143  107/1143  108/1143  109/1143  110/1143  111/1143  112/1143  113/1143  114/1143  115/1143  116/1143  117/1143  118/1143  119/1143  120/1143  121/1143  122/1143  123/1143  124/1143  125/1143  126/1143  127/1143  128/1143  129/1143  130/1143  131/1143  132/1143  133/1143  134/1143  135/1143  136/1143  137/1143  138/1143  139/1143  140/1143  141/1143  142/1143  143/1143  144/1143  145/1143  146/1143  147/1143  148/1143  149/1143  150/1143  151/1143  152/1143  153/1143  154/1143  155/1143  156/1143  157/1143  158/1143  159/1143  160/1143  161/1143  162/1143  163/1143  164/1143  165/1143  166/1143  167/1143  168/1143  169/1143  170/1143  171/1143  172/1143  173/1143  174/1143  175/1143  176/1143  177/1143  178/1143  179/1143  180/1143  181/1143  182/1143  183/1143  184/1143  185/1143  186/1143  187/1143  188/1143  189/1143  190/1143  191/1143  192/1143  193/1143  194/1143  195/1143  196/1143  197/1143  198/1143  199/1143  200/1143  201/1143  202/1143  203/1143  204/1143  205/1143  206/1143  207/1143  208/1143  209/1143  210/1143  211/1143  212/1143  213/1143  214/1143  215/1143  216/1143  217/1143  218/1143  219/1143  220/1143  221/1143  222/1143  223/1143  224/1143  225/1143  226/1143  227/1143  228/1143  229/1143  230/1143  231/1143  232/1143  233/1143  234/1143  235/1143  236/1143  237/1143  238/1143  239/1143  240/1143  241/1143  242/1143  243/1143  244/1143  245/1143  246/1143  247/1143  248/1143  249/1143  250/1143  251/1143  252/1143  253/1143  254/1143  255/1143  256/1143  257/1143  258/1143  259/1143  260/1143  261/1143  262/1143  263/1143  264/1143  265/1143  266/1143  267/1143  268/1143  269/1143  270/1143  271/1143  272/1143  273/1143  274/1143  275/1143  276/1143  277/1143  278/1143  279/1143  280/1143  281/1143  282/1143  283/1143  284/1143  285/1143  286/1143  287/1143  288/1143  289/1143  290/1143  291/1143  292/1143  293/1143  294/1143  295/1143  296/1143  297/1143  298/1143  299/1143  300/1143  301/1143  302/1143  303/1143  304/1143  305/1143  306/1143  307/1143  308/1143  309/1143  310/1143  311/1143  312/1143  313/1143  314/1143  315/1143  316/1143  317/1143  318/1143  319/1143  320/1143  321/1143  322/1143  323/1143  324/1143  325/1143  326/1143  327/1143  328/1143  329/1143  330/1143  331/1143  332/1143  333/1143  334/1143  335/1143  336/1143  337/1143  338/1143  339/1143  340/1143  341/1143  342/1143  343/1143  344/1143  345/1143  346/1143  347/1143  348/1143  349/1143  350/1143  351/1143  352/1143  353/1143  354/1143  355/1143  356/1143  357/1143  358/1143  359/1143  360/1143  361/1143  362/1143  363/1143  364/1143  365/1143  366/1143  367/1143  368/1143  369/1143  370/1143  371/1143  372/1143  373/1143  374/1143  375/1143  376/1143  377/1143  378/1143  379/1143  380/1143  381/1143  382/1143  383/1143  384/1143  385/1143  386/1143  387/1143  388/1143  389/1143  390/1143  391/1143  392/1143  393/1143  394/1143  395/1143  396/1143  397/1143  398/1143  399/1143  400/1143  401/1143  402/1143  403/1143  404/1143  405/1143  406/1143  407/1143  408/1143  409/1143  410/1143  411/1143  412/1143  413/1143  414/1143  415/1143  416/1143  417/1143  418/1143  419/1143  420/1143  421/1143  422/1143  423/1143  424/1143  425/1143  426/1143  427/1143  428/1143  429/1143  430/1143  431/1143  432/1143  433/1143  434/1143  435/1143  436/1143  437/1143  438/1143  439/1143  440/1143  441/1143  442/1143  443/1143  444/1143  445/1143  446/1143  447/1143  448/1143  449/1143  450/1143  451/1143  452/1143  453/1143  454/1143  455/1143  456/1143  457/1143  458/1143  459/1143  460/1143  461/1143  462/1143  463/1143  464/1143  465/1143  466/1143  467/1143  468/1143  469/1143  470/1143  471/1143  472/1143  473/1143  474/1143  475/1143  476/1143  477/1143  478/1143  479/1143  480/1143  481/1143  482/1143  483/1143  484/1143  485/1143  486/1143  487/1143  488/1143  489/1143  490/1143  491/1143  492/1143  493/1143  494/1143  495/1143  496/1143  497/1143  498/1143  499/1143  500/1143  501/1143  502/1143  503/1143  504/1143  505/1143  506/1143  507/1143  508/1143  509/1143  510/1143  511/1143  512/1143  513/1143  514/1143  515/1143  516/1143  517/1143  518/1143  519/1143  520/1143  521/1143  522/1143  523/1143  524/1143  525/1143  526/1143  527/1143  528/1143  529/1143  530/1143  531/1143  532/1143  533/1143  534/1143  535/1143  536/1143  537/1143  538/1143  539/1143  540/1143  541/1143  542/1143  543/1143  544/1143  545/1143  546/1143  547/1143  548/1143  549/1143  550/1143  551/1143  552/1143  553/1143  554/1143  555/1143  556/1143  557/1143  558/1143  559/1143  560/1143  561/1143  562/1143  563/1143  564/1143  565/1143  566/1143  567/1143  568/1143  569/1143  570/1143  571/1143  572/1143  573/1143  574/1143  575/1143  576/1143  577/1143  578/1143  579/1143  580/1143  581/1143  582/1143  583/1143  584/1143  585/1143  586/1143  587/1143  588/1143  589/1143  590/1143  591/1143  592/1143  593/1143  594/1143  595/1143  596/1143  597/1143  598/1143  599/1143  600/1143  601/1143  602/1143  603/1143  604/1143  605/1143  606/1143  607/1143  608/1143  609/1143  610/1143  611/1143  612/1143  613/1143  614/1143  615/1143  616/1143  617/1143  618/1143  619/1143  620/1143  621/1143  622/1143  623/1143  624/1143  625/1143  626/1143  627/1143  628/1143  629/1143  630/1143  631/1143  632/1143  633/1143  634/1143  635/1143  636/1143  637/1143  638/1143  639/1143  640/1143  641/1143  642/1143  643/1143  644/1143  645/1143  646/1143  647/1143  648/1143  649/1143  650/1143  651/1143  652/1143  653/1143  654/1143  655/1143  656/1143  657/1143  658/1143  659/1143  660/1143  661/1143  662/1143  663/1143  664/1143  665/1143  666/1143  667/1143  668/1143  669/1143  670/1143  671/1143  672/1143  673/1143  674/1143  675/1143  676/1143  677/1143  678/1143  679/1143  680/1143  681/1143  682/1143  683/1143  684/1143  685/1143  686/1143  687/1143  688/1143  689/1143  690/1143  691/1143  692/1143  693/1143  694/1143  695/1143  696/1143  697/1143  698/1143  699/1143  700/1143  701/1143  702/1143  703/1143  704/1143  705/1143  706/1143  707/1143  708/1143  709/1143  710/1143  711/1143  712/1143  713/1143  714/1143  715/1143  716/1143  717/1143  718/1143  719/1143  720/1143  721/1143  722/1143  723/1143  724/1143  725/1143  726/1143  727/1143  728/1143  729/1143  730/1143  731/1143  732/1143  733/1143  734/1143  735/1143  736/1143  737/1143  738/1143  739/1143  740/1143  741/1143  742/1143  743/1143  744/1143  745/1143  746/1143  747/1143  748/1143  749/1143  750/1143  751/1143  752/1143  753/1143  754/1143  755/1143  756/1143  757/1143  758/1143  759/1143  760/1143  761/1143  762/1143  763/1143  764/1143  765/1143  766/1143  767/1143  768/1143  769/1143  770/1143  771/1143  772/1143  773/1143  774/1143  775/1143  776/1143  777/1143  778/1143  779/1143  780/1143  781/1143  782/1143  783/1143  784/1143  785/1143  786/1143  787/1143  788/1143  789/1143  790/1143  791/1143  792/1143  793/1143  794/1143  795/1143  796/1143  797/1143  798/1143  799/1143  800/1143  801/1143  802/1143  803/1143  804/1143  805/1143  806/1143  807/1143  808/1143  809/1143  810/1143  811/1143  812/1143  813/1143  814/1143  815/1143  816/1143  817/1143  818/1143  819/1143  820/1143  821/1143  822/1143  823/1143  824/1143  825/1143  826/1143  827/1143  828/1143  829/1143  830/1143  831/1143  832/1143  833/1143  834/1143  835/1143  836/1143  837/1143  838/1143  839/1143  840/1143  841/1143  842/1143  843/1143  844/1143  845/1143  846/1143  847/1143  848/1143  849/1143  850/1143  851/1143  852/1143  853/1143  854/1143  855/1143  856/1143  857/1143  858/1143  859/1143  860/1143  861/1143  862/1143  863/1143  864/1143  865/1143  866/1143  867/1143  868/1143  869/1143  870/1143  871/1143  872/1143  873/1143  874/1143  875/1143  876/1143  877/1143  878/1143  879/1143  880/1143  881/1143  882/1143  883/1143  884/1143  885/1143  886/1143  887/1143  888/1143  889/1143  890/1143  891/1143  892/1143  893/1143  894/1143  895/1143  896/1143  897/1143  898/1143  899/1143  900/1143  901/1143  "
          ]
        }
      ],
      "source": [
        "symbols = get_tbinances()\n",
        "print(f\"Symbols: {len(symbols)}\")\n",
        "download_data(symbols,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "_8RDjQHDEGaT"
      },
      "outputs": [],
      "source": [
        "clmns = list(range(1, 61))\n",
        "clmns.append(\"suggestion\")\n",
        "def scaler(row):\n",
        "    scaler = MinMaxScaler(feature_range=(-10,10))\n",
        "    row = scaler.fit_transform(row)\n",
        "    return row\n",
        "def make_y(data,i):\n",
        "  sugg = \"sell\"\n",
        "  if data[i][3] > data[i-1][3]:\n",
        "      sugg = \"buy\"\n",
        "  return sugg\n",
        "def process(data):\n",
        "    data = data.dropna()\n",
        "    row = []\n",
        "    data.drop(\"symbol\",axis=1,inplace=True)\n",
        "    data.drop(\"datetime\",axis=1,inplace=True)\n",
        "    if len(data.columns) == 7:\n",
        "      data = data.iloc[:, 1:]\n",
        "    data = np.array(data)\n",
        "    for i in range(31, data.shape[0]-1):\n",
        "        grow = []\n",
        "        ggggrow = []\n",
        "        for x in range(1, 31):\n",
        "            grow.append([data[i-x][0] - data[i-(1+x)][0]] )\n",
        "            ggggrow.append([data[i-x][3] - data[i-(1+x)][3]] )\n",
        "        arr = np.array(grow).flatten()\n",
        "        arr4 = np.array(ggggrow).flatten()\n",
        "        sugg = make_y(data,i)\n",
        "        arr = np.concatenate((arr, arr4), axis=0).reshape(-1,1)\n",
        "        arr = scaler(arr.reshape(-1, 1))\n",
        "        arr = np.append(arr, sugg)\n",
        "        row.append(arr)\n",
        "    grow = []\n",
        "    ggrow = []\n",
        "    gggrow = []\n",
        "    arr = []\n",
        "    return np.array(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nacRKx3ZMy2X"
      },
      "outputs": [],
      "source": [
        "folder_name = extract_data(200)\n",
        "to_par(folder_name,100)\n",
        "xTrain,xTest,yTrain,yTest = make_df(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5asREmlN2U-"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512, activation='relu', input_shape=(xTrain.shape[1],)))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adamax()\n",
        "\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjwc42r1N5yA"
      },
      "outputs": [],
      "source": [
        "model.fit(xTrain,yTrain,epochs=25,batch_size=128,validation_data=(xTest,yTest))\n",
        "acr = str(round(model.evaluate(xTest,yTest)[1],4)).replace(\"0.\",\"\")\n",
        "model.save(f\"TMSG127_{acr}.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "_SmcH5O8F2NJ"
      },
      "outputs": [],
      "source": [
        "def process_for_prediction(data,index):\n",
        "        i = index\n",
        "        if \"symbol\" in data.columns:\n",
        "              data.drop(\"symbol\",axis=1,inplace=True) \n",
        "        if \"datetime\" in data.columns:\n",
        "              data.drop(\"datetime\",axis=1,inplace=True)\n",
        "        if \"Adj Close\" in data.columns:\n",
        "              data.drop(\"Adj Close\",axis=1,inplace=True)\n",
        "\n",
        "        data = np.array(data)\n",
        "        grow = []\n",
        "        ggggrow = []\n",
        "        for x in range(1, 31):\n",
        "            grow.append([data[i-x][0] - data[i-(1+x)][0]] )\n",
        "            ggggrow.append([data[i-x][3] - data[i-(1+x)][3]] )\n",
        "        arr = np.array(grow).flatten()\n",
        "        arr4 = np.array(ggggrow).flatten()\n",
        "        sugg = make_y(data,i)\n",
        "        arr = np.concatenate((arr, arr4), axis=0).reshape(-1,1)\n",
        "        arr = scaler(arr.reshape(-1, 1))\n",
        "        return arr\n",
        "def make_prediction_for_yf(symbol,period,timeframe,index):\n",
        "    raw_data = process_for_prediction(yf.download(symbol,period=period,interval=timeframe),index)\n",
        "    return f\"YF  : {model.predict(np.array(raw_data).reshape(1,-1))}\"\n",
        "def make_prediction_for_tv(symbol,exchange,timeframe,tindex):\n",
        "   tv = TvDatafeed()\n",
        "   raw_data = process_for_prediction(tv.get_hist(symbol=symbol,exchange=exchange,interval=timeframe,n_bars=35),tindex)\n",
        "   return  f\"TVB : {model.predict(np.array(raw_data).reshape(1,-1))}\"\n",
        "def make_prediction(ysymbol,period,timeframe,tsymbol,texchange,ttimeframe,tindex,index):\n",
        "  print(make_prediction_for_yf(ysymbol,period,timeframe,index))\n",
        "  print(make_prediction_for_tv(tsymbol,texchange,ttimeframe,tindex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "X2HrWAvOF5IQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0359b4f2-a47f-4622-ea21-da3203dd8c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tvDatafeed.main:you are using nologin method, data you access may be limited\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "YF  : [[0.78266525 0.21543488]]\n",
            "TVB : [[0.7848438  0.21402735]]\n"
          ]
        }
      ],
      "source": [
        "make_prediction(\"eth-usd\",\"max\",\"1mo\",\"ethusd\",\"bitstamp\",Interval.in_monthly,-1,-2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TG136.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}