{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliprg0/Google_Colab/blob/main/Binary_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5bxbCoe9do9"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import tensorflow as tf\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sys import getsizeof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IB_YMoe09qVP"
      },
      "outputs": [],
      "source": [
        "def read_syms_from_txt():  \n",
        "  with open(\"syms.txt\",\"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  lst = []\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    lst.append(line)\n",
        "  symbols = lst\n",
        "  return symbols\n",
        "\n",
        "def get_crypto_syms():\n",
        "   screens = ['all_cryptocurrencies_au',\n",
        " 'all_cryptocurrencies_ca',\n",
        " 'all_cryptocurrencies_eu',\n",
        " 'all_cryptocurrencies_gb',\n",
        " 'all_cryptocurrencies_in',\n",
        " 'all_cryptocurrencies_us']\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   #print(len(symbols))\n",
        "   #pieces = 15\n",
        "   # new_arrays = np.array_split(symbols, pieces)\n",
        "   return symbols\n",
        "\n",
        "def process(dfs): \n",
        "   fixed_dfs = []\n",
        "   for df in dfs:\n",
        "       fixed_df = []\n",
        "       df = np.array(df)\n",
        "       for i in range(20, df.shape[0]-1):\n",
        "           twenty_days_ago = i-20\n",
        "           ninteen_days_ago = i-19\n",
        "           eighteen_days_ago = i-18\n",
        "           seventeen_days_ago = i-17\n",
        "           sixteen_days_ago = i-16\n",
        "           fifteen_days_ago = i-15\n",
        "           fourteen_days_ago = i-14\n",
        "           thirteen_days_ago = i-13\n",
        "           twelve_days_ago = i-12\n",
        "           eleven_days_ago = i-11\n",
        "           ten_days_ago = i-10\n",
        "           nine_days_ago = i-9\n",
        "           eight_days_ago = i-8\n",
        "           seven_days_ago = i-7\n",
        "           six_days_ago = i-6\n",
        "           five_days_ago = i-5\n",
        "           four_days_ago = i-4\n",
        "           three_days_ago = i-3\n",
        "           two_days_ago = i - 2\n",
        "           yesterday = i - 1\n",
        "           today = i\n",
        "           tomorrow = i + 1\n",
        "           if df[tomorrow][3] > df[today][3]:\n",
        "               future = 1\n",
        "           else:\n",
        "               future = 0\n",
        "\n",
        "\n",
        "           row = [df[twenty_days_ago][0],df[twenty_days_ago][1],df[twenty_days_ago][2],df[twenty_days_ago][3],\n",
        "                  df[ninteen_days_ago][0],df[ninteen_days_ago][1],df[ninteen_days_ago][2],df[ninteen_days_ago][3],\n",
        "                  df[eighteen_days_ago][0],df[eighteen_days_ago][1],df[eighteen_days_ago][2],df[eighteen_days_ago][3],\n",
        "                  df[seventeen_days_ago][0],df[seventeen_days_ago][1],df[seventeen_days_ago][2],df[seventeen_days_ago][3],\n",
        "                  df[sixteen_days_ago][0],df[sixteen_days_ago][1],df[sixteen_days_ago][2],df[sixteen_days_ago][3],\n",
        "                  df[fifteen_days_ago][0],df[fifteen_days_ago][1],df[fifteen_days_ago][2],df[fifteen_days_ago][3],\n",
        "                  df[fourteen_days_ago][0],df[fourteen_days_ago][1],df[fourteen_days_ago][2],df[fourteen_days_ago][3],\n",
        "                  df[thirteen_days_ago][0],df[thirteen_days_ago][1],df[thirteen_days_ago][2],df[thirteen_days_ago][3],\n",
        "                  df[twelve_days_ago][0],df[twelve_days_ago][1],df[twelve_days_ago][2],df[twelve_days_ago][3],\n",
        "                  df[eleven_days_ago][0],df[eleven_days_ago][1],df[eleven_days_ago][2],df[eleven_days_ago][3],\n",
        "                  df[ten_days_ago][0],df[ten_days_ago][1],df[ten_days_ago][2],df[ten_days_ago][3], \n",
        "                  df[nine_days_ago][0],df[nine_days_ago][1],df[nine_days_ago][2],df[nine_days_ago][3],\n",
        "                  df[eight_days_ago][0],df[eight_days_ago][1],df[eight_days_ago][2],df[eight_days_ago][3],\n",
        "                  df[seven_days_ago][0],df[seven_days_ago][1],df[seven_days_ago][2],df[seven_days_ago][3],\n",
        "                  df[six_days_ago][0],df[six_days_ago][1],df[six_days_ago][2],df[six_days_ago][3],\n",
        "                  df[five_days_ago][0],df[five_days_ago][1], df[five_days_ago][2], df[five_days_ago][3], \n",
        "                  df[four_days_ago][0],df[four_days_ago][1],df[four_days_ago][2],df[four_days_ago][3],\n",
        "                  df[three_days_ago][0],df[three_days_ago][1],df[three_days_ago][2],df[three_days_ago][3],\n",
        "                  df[two_days_ago][0],df[two_days_ago][1],df[two_days_ago][2],df[two_days_ago][3],\n",
        "                  df[yesterday][0],df[yesterday][1],df[yesterday][2],df[yesterday][3],\n",
        "                  df[today][0],df[today][1],df[today][2],df[today][3],\n",
        "                  future]\n",
        "\n",
        "           fixed_df.append(row)\n",
        "\n",
        "\n",
        "       arrayed_fixed_df = np.array([fixed_df])\n",
        "       two_d_fixed_df = []\n",
        "       for i in range(0, arrayed_fixed_df.shape[1]):\n",
        "          two_d_fixed_df.append(arrayed_fixed_df[0][i])\n",
        "    \n",
        "       mm = np.array(two_d_fixed_df)\n",
        "\n",
        "\n",
        "       column_names = [\"20_candels_open\",\"20candels_high\",\"20_candels_low\",\"20_candels_close\",\n",
        "                       \"19_candels_open\",\"19_candels_high\",\"19_candels_low\",\"19_candels_close\",\n",
        "                       \"18_candels_open\",\"18_candels_high\",\"18_candels_low\",\"18_candels_close\",\n",
        "                       \"17_candels_open\",\"17_candels_high\",\"17_candels_low\",\"17_candels_close\",\n",
        "                       \"16_candels_open\",\"16_candels_high\",\"16_candels_low\",\"16_candels_close\",\n",
        "                       \"15_candels_open\",\"15_candels_high\",\"15_candels_low\",\"15_candels_close\",\n",
        "                       \"14_candels_open\",\"14_candels_high\",\"14_candels_low\",\"14_candels_close\",\n",
        "                       \"13_candels_open\",\"13_candels_high\",\"13_candels_low\",\"13_candels_close\",\n",
        "                       \"12_candels_open\",\"12_candels_high\",\"12_candels_low\",\"12_candels_close\",\n",
        "                       \"11_candels_open\",\"11_candels_high\",\"11_candels_low\",\"11_candels_close\",\n",
        "                       \"10_candels_open\",\"10_candels_high\",\"10_candels_low\",\"10_candels_close\",\n",
        "                       \"9_candels_open\",\"9_candels_high\",\"9_candels_low\",\"9_candels_close\",\n",
        "                       \"8_candels_open\",\"8_candels_high\",\"8_candels_low\",\"8_candels_close\",\n",
        "                       \"7_candels_open\",\"7_candels_high\",\"7_candels_low\",\"7_candels_close\",\n",
        "                       \"6_candels_open\",\"6_candels_high\",\"6_candels_low\",\"6_candels_close\",\n",
        "                       \"5_candels_open\",\"5_candels_high\",\"5_candels_low\",\"5_candels_close\",\n",
        "                       \"4_candels_open\",\"4_candels_high\",\"4_candels_low\",\"4_candels_close\",\n",
        "                       \"3_candels_open\",\"3_candels_high\",\"3_candels_low\",\"3_candels_close\",\n",
        "                       \"2_candels_open\",\"2_candels_high\",\"2_candels_low\",\"2_candels_close\",\n",
        "                       \"1_candels_open\",\"1_candels_high\",\"1_candels_low\",\"1_candels_close\",\n",
        "                       \"0_candels_open\",\"0_candels_high\",\"0_candels_low\",\"0_candels_close\",\n",
        "                       \"Suggestion\"]\n",
        "    \n",
        "       column_names_without_result = [\"20_candels_open\",\"20candels_high\",\"20_candels_low\",\"20_candels_close\",\n",
        "                       \"19_candels_open\",\"19_candels_high\",\"19_candels_low\",\"19_candels_close\",\n",
        "                       \"18_candels_open\",\"18_candels_high\",\"18_candels_low\",\"18_candels_close\",\n",
        "                       \"17_candels_open\",\"17_candels_high\",\"17_candels_low\",\"17_candels_close\",\n",
        "                       \"16_candels_open\",\"16_candels_high\",\"16_candels_low\",\"16_candels_close\",\n",
        "                       \"15_candels_open\",\"15_candels_high\",\"15_candels_low\",\"15_candels_close\",\n",
        "                       \"14_candels_open\",\"14_candels_high\",\"14_candels_low\",\"14_candels_close\",\n",
        "                       \"13_candels_open\",\"13_candels_high\",\"13_candels_low\",\"13_candels_close\",\n",
        "                       \"12_candels_open\",\"12_candels_high\",\"12_candels_low\",\"12_candels_close\",\n",
        "                       \"11_candels_open\",\"11_candels_high\",\"11_candels_low\",\"11_candels_close\",\n",
        "                       \"10_candels_open\",\"10_candels_high\",\"10_candels_low\",\"10_candels_close\",\n",
        "                       \"9_candels_open\",\"9_candels_high\",\"9_candels_low\",\"9_candels_close\",\n",
        "                       \"8_candels_open\",\"8_candels_high\",\"8_candels_low\",\"8_candels_close\",\n",
        "                       \"7_candels_open\",\"7_candels_high\",\"7_candels_low\",\"7_candels_close\",\n",
        "                       \"6_candels_open\",\"6_candels_high\",\"6_candels_low\",\"6_candels_close\",\n",
        "                       \"5_candels_open\",\"5_candels_high\",\"5_candels_low\",\"5_candels_close\",\n",
        "                       \"4_candels_open\",\"4_candels_high\",\"4_candels_low\",\"4_candels_close\",\n",
        "                       \"3_candels_open\",\"3_candels_high\",\"3_candels_low\",\"3_candels_close\",\n",
        "                       \"2_candels_open\",\"2_candels_high\",\"2_candels_low\",\"2_candels_close\",\n",
        "                       \"1_candels_open\",\"1_candels_high\",\"1_candels_low\",\"1_candels_close\",\n",
        "                       \"0_candels_open\",\"0_candels_high\",\"0_candels_low\",\"0_candels_close\",]\n",
        "\n",
        "       dff = pd.DataFrame(mm, columns=column_names)\n",
        "       scaler = MinMaxScaler()\n",
        "       result = dff[\"Suggestion\"]\n",
        "       dff = dff.drop([\"Suggestion\"],axis=1)\n",
        "       scaled = pd.DataFrame(scaler.fit_transform(dff.T).T, columns=column_names_without_result,dtype=object)\n",
        "\n",
        "       scaled[\"Suggestion\"] = result\n",
        "       \n",
        "       fixed_dfs.append(scaled)\n",
        "\n",
        "   data = fixed_dfs[0]\n",
        "   for i in range(1,len(dfs)):\n",
        "     data = pd.concat([data,fixed_dfs[i]], ignore_index = True)\n",
        "   data = data.astype(float)\n",
        "   data = data.dropna()\n",
        "   ttext = getsizeof(data)\n",
        "   #data.to_parquet(f\"{name}.parquet\")\n",
        "   return data\n",
        "   \n",
        "def spliting(data):\n",
        "  X = data.drop([\"Suggestion\"],axis=1)\n",
        "  y = data[\"Suggestion\"]\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.1,random_state=99)\n",
        "  print(xTrain.shape,end=\" \")\n",
        "  print(yTrain.shape)\n",
        "  print(xTest.shape,end=\" \")\n",
        "  print(yTest.shape)\n",
        "  return xTrain, xTest, yTrain, yTest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "symbols = [\"MSFT\",\"AAPL\",\"GOOG\",\"TSLA\",\"AMZN\"]\n",
        "#symbols = [\"BTC-USD\",\"LTC-USD\",\"TRX-USD\",\"XRP-USD\",\"ETH-USD\",\"BNB-USD\",\"DASH-USD\",\"VET-USD\",\"LINK-USD\",\"ADA-USD\",\"DOT-USD\",\"SOL-USD\",\"BCH-USD\",\"FTT-USD\",\"FIL-USD\",\"XMR-USD\"]\n",
        "#symbols = [\"AAPL\",\"MSFT\",\"TSLA\",\"GOOG\"]\n",
        "#symbols = [\"BTC-USD\",\"ETH-USD\"]\n",
        "#symbols = [\"BTC-USD\"]\n",
        "\n",
        "#symbols = get_crypto_syms()\n",
        "dfs = []\n",
        "for symbol in symbols:\n",
        "           data = yf.download(symbol,period=\"MAX\",interval=\"1d\",progress=False)\n",
        "           if data.empty :\n",
        "             print(\"Passing...\")\n",
        "           else:\n",
        "               dfs.append(data)\n",
        "ndfs = dfs\n",
        "dfs = []\n",
        "for df in ndfs:\n",
        "        if df.shape[0] > 21:\n",
        "          dfs.append(df)\n",
        "data = process(dfs)\n",
        "#df = pd.read_parquet(\"/content/drive/MyDrive/Colab Files/250_crypto_1d_scaled_yf.parquet\")\n",
        "xTrain, xTest, yTrain, yTest = spliting(data)\n",
        "data"
      ],
      "metadata": {
        "id": "hIAuU_ILbU27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rI1oRc9VRwO"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512, activation='relu', kernel_initializer='he_normal', input_shape=(xTrain.shape[1],)))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(32, activation='relu', kernel_initializer='he_normal'))\n",
        "\n",
        "#model.add(Dense(2000, activation='relu', kernel_initializer='he_normal'))\n",
        "#model.add(Dense(1500, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDTaW4WnX330"
      },
      "outputs": [],
      "source": [
        "model.fit(xTrain,yTrain,epochs=15,batch_size=16,validation_data=(xTest,yTest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46M8IzrdKgf8"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(xTrain,yTrain)\n",
        "print(f\"Random Forest :  {clf.score(xTest,yTest)}\")\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(xTrain, yTrain)\n",
        "print(f\"Logistic Regression: {logisticRegr.score(xTest, yTest)}\")\n",
        "nc = NearestCentroid()\n",
        "nc.fit(xTrain,yTrain)\n",
        "print(f\"Nearest Centroid: {nc.score(xTest,yTest)}\")\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(xTrain, yTrain)\n",
        "print(f\"GaussianNB: {gnb.score(xTest,yTest)}\")\n",
        "clf2 = KNeighborsClassifier(n_neighbors=2)\n",
        "clf2.fit(xTrain, yTrain)\n",
        "print(f\"K-Neighbors: {clf2.score(xTest,yTest)}\")\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(xTrain, yTrain)\n",
        "print(f\"Decision Tree: {tree.score(xTest,yTest)}\")\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
        "   max_depth=1, random_state=0).fit(xTrain, yTrain)\n",
        "gb.fit(xTrain,yTrain)\n",
        "print(f\"Gradient Boost: {gb.score(xTest, yTest)}\")\n",
        "svm = SVC()\n",
        "svm.fit(xTrain,yTrain)\n",
        "print(f\"SVM: {svm.score(xTest,yTest)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Binary_v2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN12eLH9AGiLpEVu+AmDrdx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}