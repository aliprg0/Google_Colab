{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5bxbCoe9do9"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "!pip install tvdatafeed\n",
        "!pip install tensorflow\n",
        "from tvDatafeed import TvDatafeed, Interval\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import random \n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "IB_YMoe09qVP"
      },
      "outputs": [],
      "source": [
        "def work_with_dir():\n",
        "  if os.path.exists(\"/content/data/\"):\n",
        "    shutil.rmtree(\"/content/data/\", ignore_errors=True)\n",
        "    print(\"Data Folder Removed\")\n",
        "    os.mkdir(\"/content/data/\")\n",
        "  if not os.path.exists(\"/content/data/\"):\n",
        "    os.mkdir(\"/content/data/\")\n",
        "  if not os.path.exists(\"/content/extracted/\"):\n",
        "    os.mkdir(\"/content/extracted/\")\n",
        "def get_crypto_syms():\n",
        "   # 'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in',\n",
        "   screens = [\n",
        "       'all_cryptocurrencies_us', 'all_cryptocurrencies_au', 'all_cryptocurrencies_ca', 'all_cryptocurrencies_eu', 'all_cryptocurrencies_gb', 'all_cryptocurrencies_in', ]\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   # print(len(symbols))\n",
        "   # pieces = 15\n",
        "   # new_arrays = np.array_split(symbols, pieces)\n",
        "   return symbols\n",
        "def spliting(data):\n",
        "  X = data.drop([\"buy\",\"sell\"], axis=1)\n",
        "  y = data[[\"buy\",\"sell\"]]\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.1)\n",
        "  print(xTrain.shape, end=\" \")\n",
        "  print(yTrain.shape)\n",
        "  print(xTest.shape, end=\" \")\n",
        "  print(yTest.shape)\n",
        "  return xTrain, xTest, yTrain, yTest\n",
        "def download_data(symbols,periodd,intervall):\n",
        "  indexx = 100\n",
        "  work_with_dir()\n",
        "  for symbol in symbols:\n",
        "    if ((symbols.index(symbol)+1) % 100 == 0):\n",
        "      print(f\" -- {indexx}\",end=\"\")\n",
        "      indexx = indexx + 100\n",
        "    try:\n",
        "        data = yf.download(symbol, period=periodd,interval=intervall, progress=False,show_errors=False)\n",
        "        if data.empty:\n",
        "           pass\n",
        "        else:\n",
        "             data.to_csv(f\"/content/data/{symbol}.csv\")         \n",
        "    except:\n",
        "       print(\"Error!\")\n",
        "  print(\" \")\n",
        "  print(f\"Files In Data : {len(os.listdir('/content/data/'))}\")\n",
        "def each_file_proc(files,now,index):\n",
        "     print(f\"Files In Data : {len(os.listdir('/content/data/'))}\")\n",
        "     data = []\n",
        "     unattached_dfs = []\n",
        "     files = list(files)\n",
        "     for file in files:\n",
        "        print(f\"{files.index(file)+1+index}\",end=\" \")\n",
        "        if (files.index(file)+index+1) % 40 == 0:\n",
        "          print(\" \")\n",
        "        address = f\"/content/data/{file}\"\n",
        "        try:\n",
        "            unattached_dfs.append(process(pd.read_csv(address)))\n",
        "        except:\n",
        "          pass\n",
        "     nud = []\n",
        "     for i in unattached_dfs:\n",
        "       if i.size != 0:\n",
        "         nud.append(i)\n",
        "     unattached_dfs = []\n",
        "     data = np.concatenate(nud)\n",
        "     unattached_dfs = []\n",
        "     unattached_dfs = []\n",
        "     data = pd.DataFrame(data, columns=clmns)\n",
        "     sugg = data[\"suggestion\"]\n",
        "     data.drop(\"suggestion\",axis=1,inplace=True)\n",
        "     sugg = pd.get_dummies(sugg)\n",
        "     data = pd.concat([data,sugg],axis=1)\n",
        "     data = data.astype(float)\n",
        "     right_now = datetime.now().strftime(\"%H%M%S%f\")\n",
        "     data.to_csv(f\"/content/extracted/{now}/{right_now}.csv\")  \n",
        "def extract_data(pieces):\n",
        "  pd.options.mode.chained_assignment = None\n",
        "  files = os.listdir(\"/content/data/\")\n",
        "  new_files = np.array_split(files, pieces)\n",
        "  print(\"Processing File:\")\n",
        "  now = datetime.now().strftime(\"%H%M%S\")\n",
        "  os.mkdir(f\"/content/extracted/{now}/\")\n",
        "  \n",
        "  index = 0 \n",
        "  for files in new_files:\n",
        "     \n",
        "     each_file_proc(files,now,index)\n",
        "     index = index + len(files)\n",
        "  print(\" \")\n",
        "  return now\n",
        "def delete_all_csv(now):\n",
        "   path = f'/content/extracted/{now}/*.csv'\n",
        "   files = glob.glob(path)\n",
        "   for file in files:\n",
        "       os.remove(file)\n",
        "def make_df(now):\n",
        "   path = f'/content/extracted/{now}/*.parquet'\n",
        "   files = glob.glob(path)\n",
        "   #data = pd.DataFrame()\n",
        "   data = pd.DataFrame()\n",
        "   for adr in files:\n",
        "     data =pd.concat([data,pd.read_parquet(adr)])\n",
        "   if \"Unnamed: 0\" in data:\n",
        "     data.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
        "   print(data.shape)\n",
        "   xTrain,xTest,yTrain,yTest = spliting(data)\n",
        "   data.to_parquet(f'/content/extracted/{now}/data.parquet')\n",
        "   delete_all_csv(now)\n",
        "   data = []\n",
        "   return xTrain,xTest,yTrain,yTest\n",
        "def to_par(now,howmanyfiles): \n",
        "    files = os.listdir(f\"/content/extracted/{now}/\")\n",
        "    addresses = []\n",
        "    for file in files:\n",
        "      addresses.append(f\"/content/extracted/{now}/{file}\")\n",
        "    new_adr = np.array_split(addresses,howmanyfiles)\n",
        "    for adrs in new_adr:\n",
        "      datas = []\n",
        "      for adr in adrs:\n",
        "        datas.append(pd.read_csv(adr))\n",
        "      rnow = datetime.now().strftime(\"%H%M%S%f\")\n",
        "      datas = pd.concat(datas)\n",
        "      datas.to_parquet(f\"/content/extracted/{now}/part_{rnow}.parquet\")     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMR8z1BIS-M_"
      },
      "outputs": [],
      "source": [
        "symbols = get_crypto_syms()\n",
        "print(f\"Symbols : {len(symbols)}\")\n",
        "download_data(symbols,\"max\",\"1mo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "R8KVpQmiuj9S"
      },
      "outputs": [],
      "source": [
        "clmns = list(range(1, 61))\n",
        "clmns.append(\"suggestion\")\n",
        "def scaler(row):\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    row = scaler.fit_transform(row)\n",
        "    return row\n",
        "def make_y(data,i):\n",
        "  sugg = \"sell\"\n",
        "  if data[i][3] > data[i-1][3]:\n",
        "      sugg = \"buy\"\n",
        "  return sugg\n",
        "def process(data):\n",
        "    data = data.dropna()\n",
        "    row = []\n",
        "    if len(data.columns) == 7:\n",
        "      data = data.iloc[:, 1:]\n",
        "    data = np.array(data)\n",
        "    for i in range(31, data.shape[0]-1):\n",
        "        grow = []\n",
        "        ggrow = []\n",
        "        gggrow = []\n",
        "        ggggrow = []\n",
        "        for x in range(1, 31):\n",
        "            grow.append([data[i-x][0] - data[i-(1+x)][0]] )\n",
        "            ggggrow.append([data[i-x][3] - data[i-(1+x)][3]] )\n",
        "        arr = np.array(grow).flatten()\n",
        "        arr4 = np.array(ggggrow).flatten()\n",
        "        sugg = make_y(data,i)\n",
        "        arr = np.concatenate((arr, arr4), axis=0).reshape(-1,1)\n",
        "        arr = scaler(arr.reshape(-1, 1))\n",
        "        arr = np.append(arr, sugg)\n",
        "        row.append(arr)\n",
        "    grow = []\n",
        "    ggrow = []\n",
        "    gggrow = []\n",
        "    arr = []\n",
        "    return np.array(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT3YU88Edyb3"
      },
      "outputs": [],
      "source": [
        "folder_name = extract_data(1)\n",
        "to_par(folder_name,1)\n",
        "xTrain,xTest,yTrain,yTest = make_df(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN93WT9e8ueQ"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512, activation='relu', input_shape=(xTrain.shape[1],)))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adamax(learning_rate=0.01)\n",
        "\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SBxPzRd89uy"
      },
      "outputs": [],
      "source": [
        "model.fit(xTrain,yTrain,epochs=25,batch_size=128,validation_data=(xTest,yTest))\n",
        "acr = str(round(model.evaluate(xTest,yTest)[1],4)).replace(\"0.\",\"\")\n",
        "model.save(f\"DCG127_{acr}.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "fEcDYXMtSPUz"
      },
      "outputs": [],
      "source": [
        "def process_for_prediction(data,index):\n",
        "        i = index\n",
        "        if \"symbol\" in data.columns:\n",
        "              data.drop(\"symbol\",axis=1,inplace=True) \n",
        "        if \"datetime\" in data.columns:\n",
        "              data.drop(\"datetime\",axis=1,inplace=True)\n",
        "        if \"Adj Close\" in data.columns:\n",
        "              data.drop(\"Adj Close\",axis=1,inplace=True)\n",
        "\n",
        "        data = np.array(data)\n",
        "        grow = []\n",
        "        ggggrow = []\n",
        "        for x in range(1, 31):\n",
        "            grow.append([data[i-x][0] - data[i-(1+x)][0]] )\n",
        "            ggggrow.append([data[i-x][3] - data[i-(1+x)][3]] )\n",
        "        arr = np.array(grow).flatten()\n",
        "        arr4 = np.array(ggggrow).flatten()\n",
        "        sugg = make_y(data,i)\n",
        "        arr = np.concatenate((arr, arr4), axis=0).reshape(-1,1)\n",
        "        arr = scaler(arr.reshape(-1, 1))\n",
        "        return arr\n",
        "def make_prediction_for_yf(symbol,period,timeframe,index):\n",
        "    raw_data = process_for_prediction(yf.download(symbol,period=period,interval=timeframe),index)\n",
        "    return f\"YF  : {model.predict(np.array(raw_data).reshape(1,-1))}\"\n",
        "def make_prediction_for_tv(symbol,exchange,timeframe,tindex):\n",
        "   tv = TvDatafeed()\n",
        "   raw_data = process_for_prediction(tv.get_hist(symbol=symbol,exchange=exchange,interval=timeframe,n_bars=35),tindex)\n",
        "   return  f\"TVB : {model.predict(np.array(raw_data).reshape(1,-1))}\"\n",
        "def make_prediction(ysymbol,period,timeframe,tsymbol,texchange,ttimeframe,tindex,index):\n",
        "  print(make_prediction_for_yf(ysymbol,period,timeframe,index))\n",
        "  print(make_prediction_for_tv(tsymbol,texchange,ttimeframe,tindex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuUzTsN-SfnH"
      },
      "outputs": [],
      "source": [
        "make_prediction(\"ltc-usd\",\"max\",\"1mo\",\"ltcusdt\",\"binance\",Interval.in_monthly,-1,-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bqkwjROb3lL"
      },
      "outputs": [],
      "source": [
        "yf.download(\"btc-usd\",period=\"max\",interval=\"1mo\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "G135.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}