{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_5bxbCoe9do9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3037f8cd-1bd3-4c1e-d535-91cdbe2c95ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 15.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.5)\n",
            "Collecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.8.0 requests-2.27.1 yfinance-0.1.70\n",
            "Collecting yahooquery\n",
            "  Downloading yahooquery-2.2.15-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (4.64.0)\n",
            "Collecting requests-futures>=1.0.0\n",
            "  Downloading requests_futures-1.0.0-py2.py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (1.3.5)\n",
            "Requirement already satisfied: lxml>=4.6.2 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (4.8.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yahooquery) (1.15.0)\n",
            "Requirement already satisfied: requests>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from requests-futures>=1.0.0->yahooquery) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2.10)\n",
            "Installing collected packages: requests-futures, yahooquery\n",
            "Successfully installed requests-futures-1.0.0 yahooquery-2.2.15\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import tensorflow as tf\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sys import getsizeof\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IB_YMoe09qVP"
      },
      "outputs": [],
      "source": [
        "clmns = ['15 Open', '15 High', '15 Low', '15 Close', '15 Volume', '14 Open', '14 High', '14 Low', '14 Close', '14 Volume', '13 Open', '13 High', '13 Low', '13 Close', '13 Volume', '12 Open', '12 High', '12 Low', '12 Close', '12 Volume', '11 Open', '11 High', '11 Low', '11 Close', '11 Volume', '10 Open', '10 High', '10 Low', '10 Close', '10 Volume', '9 Open', '9 High', '9 Low', '9 Close', '9 Volume', '8 Open', '8 High', '8 Low', '8 Close', '8 Volume', '7 Open', '7 High', '7 Low', '7 Close', '7 Volume', '6 Open', '6 High', '6 Low', '6 Close', '6 Volume', '5 Open', '5 High', '5 Low', '5 Close', '5 Volume', '4 Open', '4 High', '4 Low', '4 Close', '4 Volume', '3 Open', '3 High', '3 Low', '3 Close', '3 Volume', '2 Open', '2 High', '2 Low', '2 Close', '2 Volume', '1 Open', '1 High', '1 Low', '1 Close', '1 Volume',\"suggestion\"]\n",
        "clmnsws = ['15 Open', '15 High', '15 Low', '15 Close', '15 Volume', '14 Open', '14 High', '14 Low', '14 Close', '14 Volume', '13 Open', '13 High', '13 Low', '13 Close', '13 Volume', '12 Open', '12 High', '12 Low', '12 Close', '12 Volume', '11 Open', '11 High', '11 Low', '11 Close', '11 Volume', '10 Open', '10 High', '10 Low', '10 Close', '10 Volume', '9 Open', '9 High', '9 Low', '9 Close', '9 Volume', '8 Open', '8 High', '8 Low', '8 Close', '8 Volume', '7 Open', '7 High', '7 Low', '7 Close', '7 Volume', '6 Open', '6 High', '6 Low', '6 Close', '6 Volume', '5 Open', '5 High', '5 Low', '5 Close', '5 Volume', '4 Open', '4 High', '4 Low', '4 Close', '4 Volume', '3 Open', '3 High', '3 Low', '3 Close', '3 Volume', '2 Open', '2 High', '2 Low', '2 Close', '2 Volume', '1 Open', '1 High', '1 Low', '1 Close', '1 Volume']\n",
        "def read_syms_from_txt():  \n",
        "  with open(\"syms.txt\",\"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  lst = []\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    lst.append(line)\n",
        "  symbols = lst\n",
        "  return symbols\n",
        "def get_crypto_syms():\n",
        "   #'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in',\n",
        "   screens = [\n",
        "   'all_cryptocurrencies_us','all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in',]\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   #print(len(symbols))\n",
        "   #pieces = 15\n",
        "   # new_arrays = np.array_split(symbols, pieces)\n",
        "   return symbols\n",
        "def spliting(data):\n",
        "  X = data.drop([\"suggestion\"],axis=1)\n",
        "  y = data[\"suggestion\"]\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.1)\n",
        "  print(xTrain.shape,end=\" \")\n",
        "  print(yTrain.shape)\n",
        "  print(xTest.shape,end=\" \")\n",
        "  print(yTest.shape)\n",
        "  return xTrain, xTest, yTrain, yTest\n",
        "def extract_data(df):\n",
        "    rows = []\n",
        "    for each in range(15,df.shape[0]-1):\n",
        "        sugg = 0\n",
        "        if df[each][3] > df[each][0]:\n",
        "          sugg = 1\n",
        "        row = [\n",
        "                df[each-15][0],\n",
        "                df[each-15][1],\n",
        "                df[each-15][2],\n",
        "                df[each-15][3],\n",
        "                df[each-15][4],\n",
        "                df[each-14][0],\n",
        "                df[each-14][1],\n",
        "                df[each-14][2],\n",
        "                df[each-14][3],\n",
        "                df[each-14][4],\n",
        "                df[each-13][0],\n",
        "                df[each-13][1],\n",
        "                df[each-13][2],\n",
        "                df[each-13][3],\n",
        "                df[each-13][4],\n",
        "                df[each-12][0],\n",
        "                df[each-12][1],\n",
        "                df[each-12][2],\n",
        "                df[each-12][3],\n",
        "                df[each-12][4],\n",
        "                df[each-11][0],\n",
        "                df[each-11][1],\n",
        "                df[each-11][2],\n",
        "                df[each-11][3],\n",
        "                df[each-11][4],\n",
        "                df[each-10][0],\n",
        "                df[each-10][1],\n",
        "                df[each-10][2],\n",
        "                df[each-10][3],\n",
        "                df[each-10][4],\n",
        "                df[each-9][0],\n",
        "                df[each-9][1],\n",
        "                df[each-9][2],\n",
        "                df[each-9][3],\n",
        "                df[each-9][4],\n",
        "                df[each-8][0],\n",
        "                df[each-8][1],\n",
        "                df[each-8][2],\n",
        "                df[each-8][3],\n",
        "                df[each-8][4],\n",
        "                df[each-7][0],\n",
        "                df[each-7][1],\n",
        "                df[each-7][2],\n",
        "                df[each-7][3],\n",
        "                df[each-7][4],\n",
        "                df[each-6][0],\n",
        "                df[each-6][1],\n",
        "                df[each-6][2],\n",
        "                df[each-6][3],\n",
        "                df[each-6][4],\n",
        "                df[each-5][0],\n",
        "                df[each-5][1],\n",
        "                df[each-5][2],\n",
        "                df[each-5][3],\n",
        "                df[each-5][4],\n",
        "                df[each-4][0],\n",
        "                df[each-4][1],\n",
        "                df[each-4][2],\n",
        "                df[each-4][3],\n",
        "                df[each-4][4],\n",
        "                df[each-3][0],\n",
        "                df[each-3][1],\n",
        "                df[each-3][2],\n",
        "                df[each-3][3],\n",
        "                df[each-3][4],\n",
        "                df[each-2][0],\n",
        "                df[each-2][1],\n",
        "                df[each-2][2],\n",
        "                df[each-2][3],\n",
        "                df[each-2][4],\n",
        "                df[each-1][0],\n",
        "                df[each-1][2],\n",
        "                df[each-1][1],\n",
        "                df[each-1][3],\n",
        "                df[each-1][4],\n",
        "                sugg\n",
        "        ]\n",
        "        rows.append(row)\n",
        "    return rows\n",
        "def row_scaler(df):\n",
        "  scaler = MinMaxScaler(feature_range=(1,5))\n",
        "  last_column = df.iloc[: , -1]\n",
        "  df = df.drop(columns=df.columns[-1], axis=1)\n",
        "  scaled = pd.DataFrame(scaler.fit_transform(df.T).T,dtype=object,columns = clmnsws)\n",
        "  scaled[\"suggestion\"] = last_column\n",
        "  return scaled\n",
        "def column_scaler(df):\n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  last_column = df.iloc[: , -1]\n",
        "  df = df.drop(columns=df.columns[-1], axis=1)\n",
        "  df_scaled = scaler.fit_transform(df.to_numpy())\n",
        "  df_scaled = pd.DataFrame(df_scaled,columns = clmnsws)\n",
        "  df_scaled[\"suggestion\"] = last_column\n",
        "  return df_scaled\n",
        "def process(df): \n",
        "      df = df.dropna()\n",
        "      df = np.array(df)\n",
        "      df = extract_data(df)\n",
        "      df = pd.DataFrame(df,columns = clmns)\n",
        "      df = row_scaler(df)\n",
        "      #df = column_scaler(df)\n",
        "      return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIAuU_ILbU27",
        "outputId": "3c0ae393-cf67-46c5-dddc-236738bbd628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1 Failed download:\n",
            "- XNO-AUD: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- OXEN-AUD: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- VAL-AUD: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- ICP-CAD: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- XNO-CAD: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- OXEN-CAD: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- VAL-CAD: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- ICP-EUR: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- XNO-EUR: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- OXEN-EUR: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- VAL-EUR: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- ICP-GBP: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- XNO-GBP: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- OXEN-GBP: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- VAL-GBP: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- ICP-INR: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- XNO-INR: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- OXEN-INR: No data found for this date range, symbol may be delisted\n",
            "Passing...\n",
            "\n",
            "1 Failed download:\n",
            "- VAL-INR: No data found, symbol may be delisted\n",
            "Passing...\n",
            "(1583494, 75) (1583494,)\n",
            "(175944, 75) (175944,)\n",
            "(1759438, 76)\n"
          ]
        }
      ],
      "source": [
        "def get_data(symbols):\n",
        "    unattached_dfs = []\n",
        "    for symbol in symbols:\n",
        "           data = yf.download(symbol,period=\"MAX\",interval=\"1d\",progress=False)\n",
        "           if data.empty :\n",
        "             print(\"Passing...\")\n",
        "           else:\n",
        "               if data.shape[0] > 14:\n",
        "                   unattached_dfs.append(process(data))\n",
        "    symbols = []\n",
        "    symbol = []\n",
        "    data = []\n",
        "\n",
        "    data = pd.concat(unattached_dfs)\n",
        "    data = data.astype(float)\n",
        "\n",
        "    xTrain, xTest, yTrain, yTest = spliting(data)\n",
        "    print(data.shape)\n",
        "    return xTrain, xTest, yTrain, yTest\n",
        "\n",
        "\n",
        "#symbols = [\"MSFT\",\"AAPL\",\"GOOG\",\"TSLA\",\"AMZN\"]\n",
        "#symbols = [\"BTC-USD\",\"LTC-USD\",\"TRX-USD\",\"XRP-USD\",\"ETH-USD\",\"BNB-USD\",\"DASH-USD\",\"VET-USD\",\"LINK-USD\",\"ADA-USD\",\"DOT-USD\",\"SOL-USD\",\"BCH-USD\",\"FTT-USD\",\"FIL-USD\",\"XMR-USD\"]\n",
        "#symbols = [\"AAPL\",\"MSFT\",\"TSLA\",\"GOOG\"]\n",
        "#symbols = [\"BTC-USD\",\"ETH-USD\"]\n",
        "#symbols = [\"BTC-USD\"]\n",
        "#symbols = read_syms_from_txt()\n",
        "symbols = get_crypto_syms()\n",
        "\n",
        "#pieces = 20\n",
        "#new_arrays = np.array_split(symbols, pieces)\n",
        "#for symbols in new_arrays:\n",
        "xTrain, xTest, yTrain, yTest= get_data(symbols)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN93WT9e8ueQ"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model = Sequential()\n",
        "adam = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.1)\n",
        "\n",
        "\n",
        "model.add(Dense(2000, activation='relu', input_shape=(xTrain.shape[1],)))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SBxPzRd89uy",
        "outputId": "131338d6-7808-4a0a-d486-117ecc77653f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "143/143 [==============================] - 185s 1s/step - loss: 0.6884 - accuracy: 0.5380 - val_loss: 0.6949 - val_accuracy: 0.5231\n",
            "Epoch 2/50\n",
            "143/143 [==============================] - 176s 1s/step - loss: 0.6862 - accuracy: 0.5449 - val_loss: 0.6821 - val_accuracy: 0.5594\n",
            "Epoch 3/50\n",
            "143/143 [==============================] - 177s 1s/step - loss: 0.6808 - accuracy: 0.5593 - val_loss: 0.6757 - val_accuracy: 0.5688\n",
            "Epoch 4/50\n",
            "143/143 [==============================] - 176s 1s/step - loss: 0.6749 - accuracy: 0.5708 - val_loss: 0.6667 - val_accuracy: 0.5851\n",
            "Epoch 5/50\n",
            "143/143 [==============================] - 175s 1s/step - loss: 0.6656 - accuracy: 0.5863 - val_loss: 0.6638 - val_accuracy: 0.5899\n",
            "Epoch 6/50\n",
            "143/143 [==============================] - 175s 1s/step - loss: 0.6517 - accuracy: 0.6047 - val_loss: 0.6442 - val_accuracy: 0.6115\n",
            "Epoch 7/50\n",
            "143/143 [==============================] - 178s 1s/step - loss: 0.6348 - accuracy: 0.6236 - val_loss: 0.6312 - val_accuracy: 0.6258\n",
            "Epoch 8/50\n",
            "143/143 [==============================] - 174s 1s/step - loss: 0.6170 - accuracy: 0.6419 - val_loss: 0.6132 - val_accuracy: 0.6453\n",
            "Epoch 9/50\n",
            "143/143 [==============================] - 174s 1s/step - loss: 0.6002 - accuracy: 0.6586 - val_loss: 0.5964 - val_accuracy: 0.6608\n",
            "Epoch 10/50\n",
            "143/143 [==============================] - 173s 1s/step - loss: 0.5812 - accuracy: 0.6760 - val_loss: 0.5844 - val_accuracy: 0.6712\n",
            "Epoch 11/50\n",
            "143/143 [==============================] - 175s 1s/step - loss: 0.5622 - accuracy: 0.6928 - val_loss: 0.5703 - val_accuracy: 0.6866\n",
            "Epoch 12/50\n",
            "143/143 [==============================] - 174s 1s/step - loss: 0.5409 - accuracy: 0.7107 - val_loss: 0.5494 - val_accuracy: 0.7039\n",
            "Epoch 13/50\n",
            "143/143 [==============================] - 173s 1s/step - loss: 0.5187 - accuracy: 0.7287 - val_loss: 0.5401 - val_accuracy: 0.7128\n",
            "Epoch 14/50\n",
            "143/143 [==============================] - 174s 1s/step - loss: 0.4936 - accuracy: 0.7487 - val_loss: 0.5203 - val_accuracy: 0.7297\n",
            "Epoch 15/50\n",
            "143/143 [==============================] - 174s 1s/step - loss: 0.4682 - accuracy: 0.7670 - val_loss: 0.5016 - val_accuracy: 0.7437\n",
            "Epoch 16/50\n",
            "143/143 [==============================] - 173s 1s/step - loss: 0.4448 - accuracy: 0.7826 - val_loss: 0.4807 - val_accuracy: 0.7613\n",
            "Epoch 17/50\n",
            "143/143 [==============================] - 174s 1s/step - loss: 0.4190 - accuracy: 0.8001 - val_loss: 0.4698 - val_accuracy: 0.7732\n",
            "Epoch 18/50\n",
            "143/143 [==============================] - 174s 1s/step - loss: 0.3980 - accuracy: 0.8137 - val_loss: 0.4549 - val_accuracy: 0.7829\n",
            "Epoch 19/50\n",
            "143/143 [==============================] - 172s 1s/step - loss: 0.3801 - accuracy: 0.8246 - val_loss: 0.4390 - val_accuracy: 0.7957\n",
            "Epoch 20/50\n",
            "143/143 [==============================] - 172s 1s/step - loss: 0.3549 - accuracy: 0.8398 - val_loss: 0.4223 - val_accuracy: 0.8069\n",
            "Epoch 21/50\n",
            "143/143 [==============================] - 170s 1s/step - loss: 0.3355 - accuracy: 0.8512 - val_loss: 0.4075 - val_accuracy: 0.8178\n",
            "Epoch 22/50\n",
            "143/143 [==============================] - 170s 1s/step - loss: 0.3218 - accuracy: 0.8588 - val_loss: 0.3852 - val_accuracy: 0.8325\n",
            "Epoch 23/50\n",
            "143/143 [==============================] - 171s 1s/step - loss: 0.3056 - accuracy: 0.8680 - val_loss: 0.3859 - val_accuracy: 0.8334\n",
            "Epoch 24/50\n",
            "143/143 [==============================] - 174s 1s/step - loss: 0.2908 - accuracy: 0.8764 - val_loss: 0.3732 - val_accuracy: 0.8419\n",
            "Epoch 25/50\n",
            "143/143 [==============================] - 181s 1s/step - loss: 0.2801 - accuracy: 0.8819 - val_loss: 0.3643 - val_accuracy: 0.8487\n",
            "Epoch 26/50\n",
            " 42/143 [=======>......................] - ETA: 1:57 - loss: 0.2601 - accuracy: 0.8924"
          ]
        }
      ],
      "source": [
        "model.fit(xTrain,yTrain,epochs=50,batch_size=12345,validation_data=(xTest,yTest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC6W88bOvyxe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Files/BinaryTest_v10_hourtesting.csv\")\n",
        "data = data.drop(\"Unnamed: 0\",axis=1)\n",
        "xTrain, xTest, yTrain, yTest = spliting(data)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAKTl3fbHIE6",
        "outputId": "f1927a70-eb9d-462a-e41a-b12142cfbe08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "files = os.listdir(\"/content/drive/MyDrive/Colab Files/\")"
      ],
      "metadata": {
        "id": "bic_W4SLNWTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame()\n",
        "\n",
        "for i in files:\n",
        "    address = f\"/content/drive/MyDrive/Colab Files/{i}\"\n",
        "    df = pd.read_parquet(address)\n",
        "    data = pd.concat([data,df])\n",
        "    break\n",
        "\n",
        "data \n",
        "xTrain, xTest, yTrain, yTest = spliting(data)"
      ],
      "metadata": {
        "id": "ocGkgt5WNrDb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "MP_v02.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}