{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliprg0/Google_Colab/blob/main/TV_g121t.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "!pip install tvdatafeed\n",
        "from tvDatafeed import TvDatafeed, Interval\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import random \n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from tvDatafeed import TvDatafeed, Interval\n",
        "import multiprocessing\n",
        "import time\n",
        "import glob"
      ],
      "metadata": {
        "id": "BqIra2YIBJsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nasq_syms():\n",
        "  with open(\"watchlist_NASDAQ.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  line = lines[0].split(\",\")\n",
        "  symbols = []\n",
        "  for l in line:\n",
        "    x = l.split(\":\")\n",
        "    symbols.append(x[1])\n",
        "  return symbols\n",
        "def download_data_p(tv,sym):\n",
        "    data =tv.get_hist(sym,exchange=\"binance\", interval=Interval.in_weekly, n_bars=50000)\n",
        "    if data.empty:\n",
        "      pass\n",
        "    else:\n",
        "         try:\n",
        "           if np.array(data).shape[0] > 15:\n",
        "             data.to_csv(f\"/content/data/{sym}.csv\")\n",
        "         except:\n",
        "            pass\n",
        "def download_data(symbols):\n",
        "  work_with_dir()\n",
        "  tv = TvDatafeed()\n",
        "  for sym in symbols[:100]:\n",
        "    print(symbols.index(sym)+1, \"/\", len(symbols))\n",
        "    p = multiprocessing.Process(target=download_data_p, name=\"dd\", args=(tv,sym,))\n",
        "    p.start()\n",
        "    time.sleep(2)\n",
        "    p.terminate()\n",
        "    p.join()"
      ],
      "metadata": {
        "id": "Cku-whunBURi"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"binance_tv.txt\",\"r\") as f:\n",
        "  lines = f.readlines()\n",
        "nl = []\n",
        "for line in lines:\n",
        "  nl.append(line.strip().split(\":\")[1])"
      ],
      "metadata": {
        "id": "q1JQGtGtK7A8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_data(nl)\n",
        "#download_data(get_nasq_syms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZHam_kdREbf",
        "outputId": "ed023df5-3ef0-47de-8485-8761b8703363"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "you are using nologin method, data you access may be limited\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Folder Removed\n",
            "1 / 574\n",
            "2 / 574\n",
            "3 / 574\n",
            "4 / 574\n",
            "5 / 574\n",
            "6 / 574\n",
            "7 / 574\n",
            "8 / 574\n",
            "9 / 574\n",
            "10 / 574\n",
            "11 / 574\n",
            "12 / 574\n",
            "13 / 574\n",
            "14 / 574\n",
            "15 / 574\n",
            "16 / 574\n",
            "17 / 574\n",
            "18 / 574\n",
            "19 / 574\n",
            "20 / 574\n",
            "21 / 574\n",
            "22 / 574\n",
            "23 / 574\n",
            "24 / 574\n",
            "25 / 574\n",
            "26 / 574\n",
            "27 / 574\n",
            "28 / 574\n",
            "29 / 574\n",
            "30 / 574\n",
            "31 / 574\n",
            "32 / 574\n",
            "33 / 574\n",
            "34 / 574\n",
            "35 / 574\n",
            "36 / 574\n",
            "37 / 574\n",
            "38 / 574\n",
            "39 / 574\n",
            "40 / 574\n",
            "41 / 574\n",
            "42 / 574\n",
            "43 / 574\n",
            "44 / 574\n",
            "45 / 574\n",
            "46 / 574\n",
            "47 / 574\n",
            "48 / 574\n",
            "49 / 574\n",
            "50 / 574\n",
            "51 / 574\n",
            "52 / 574\n",
            "53 / 574\n",
            "54 / 574\n",
            "55 / 574\n",
            "56 / 574\n",
            "57 / 574\n",
            "58 / 574\n",
            "59 / 574\n",
            "60 / 574\n",
            "61 / 574\n",
            "62 / 574\n",
            "63 / 574\n",
            "64 / 574\n",
            "65 / 574\n",
            "66 / 574\n",
            "67 / 574\n",
            "68 / 574\n",
            "69 / 574\n",
            "70 / 574\n",
            "71 / 574\n",
            "72 / 574\n",
            "73 / 574\n",
            "74 / 574\n",
            "75 / 574\n",
            "76 / 574\n",
            "77 / 574\n",
            "78 / 574\n",
            "79 / 574\n",
            "80 / 574\n",
            "81 / 574\n",
            "82 / 574\n",
            "83 / 574\n",
            "84 / 574\n",
            "85 / 574\n",
            "86 / 574\n",
            "87 / 574\n",
            "88 / 574\n",
            "89 / 574\n",
            "90 / 574\n",
            "91 / 574\n",
            "92 / 574\n",
            "93 / 574\n",
            "94 / 574\n",
            "95 / 574\n",
            "96 / 574\n",
            "97 / 574\n",
            "98 / 574\n",
            "99 / 574\n",
            "100 / 574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clmns = [\n",
        "       \"Open 1\",\"Close 1\",\"Open 2\",\"Close 2\",\"Open 3\",\"Close 3\",\"Open 4\",\"Close 4\",\"Open 5\",\"Close 5\",\"Open 6\",\"Close 6\",\"Open 7\",\"Close 7\",\"Open 8\",\"Close 8\",\"Open 9\",\"Close 9\",\"Open 10\",\"Close 10\",\"Open 11\",\"Close 11\",\"Open 12\",\"Close 12\",\"Open 13\",\"Close 13\",\"Open 14\",\"Close 14\",\"Open 15\",\"Close 15\",\n",
        "       \"Open 1\",\"Close 1\",\"Open 2\",\"Close 2\",\"Open 3\",\"Close 3\",\"Open 4\",\"Close 4\",\"Open 5\",\"Close 5\",\"Open 6\",\"Close 6\",\"Open 7\",\"Close 7\",\"Open 8\",\"Close 8\",\"Open 9\",\"Close 9\",\"Open 10\",\"Close 10\",\"Open 11\",\"Close 11\",\"Open 12\",\"Close 12\",\"Open 13\",\"Close 13\",\"Open 14\",\"Close 14\",\"Open 15\",\"Close 15\",\n",
        "       \"Open 1\",\"Close 1\",\"Open 2\",\"Close 2\",\"Open 3\",\"Close 3\",\"Open 4\",\"Close 4\",\"Open 5\",\"Close 5\",\"Open 6\",\"Close 6\",\"Open 7\",\"Close 7\",\"Open 8\",\"Close 8\",\"Open 9\",\"Close 9\",\"Open 10\",\"Close 10\",\"Open 11\",\"Close 11\",\"Open 12\",\"Close 12\",\"Open 13\",\"Close 13\",\"Open 14\",\"Close 14\",\"Open 15\",\"Close 15\",\n",
        "       \"Open 1\",\"Close 1\",\"Open 2\",\"Close 2\",\"Open 3\",\"Close 3\",\"Open 4\",\"Close 4\",\"Open 5\",\"Close 5\",\"Open 6\",\"Close 6\",\"Open 7\",\"Close 7\",\"Open 8\",\"Close 8\",\"Open 9\",\"Close 9\",\"Open 10\",\"Close 10\",\"Open 11\",\"Close 11\",\"Open 12\",\"Close 12\",\"Open 13\",\"Close 13\",\"Open 14\",\"Close 14\",\"Open 15\",\"Close 15\",\n",
        "\n",
        "    \"suggestion\"]\n",
        "def work_with_dir():\n",
        "  if os.path.exists(\"/content/data/\"):\n",
        "    shutil.rmtree(\"/content/data/\", ignore_errors=True)\n",
        "    print(\"Data Folder Removed\")\n",
        "    os.mkdir(\"/content/data/\")\n",
        "\n",
        "  if not os.path.exists(\"/content/data/\"):\n",
        "    os.mkdir(\"/content/data/\")\n",
        "  \n",
        "  if not os.path.exists(\"/content/extracted/\"):\n",
        "    os.mkdir(\"/content/extracted/\")\n",
        "def read_txt_list():\n",
        "  with open(\"yahoo_stocklist.txt\",\"r\")as f:\n",
        "    lines = f.readlines()\n",
        "    nlines = []\n",
        "    for line in lines:\n",
        "       nlines.append(line.strip())\n",
        "    return nlines\n",
        "def read_syms_from_txt():\n",
        "  with open(\"syms.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  lst = []\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    lst.append(line)\n",
        "  symbols = lst\n",
        "  return symbols\n",
        "def get_crypto_syms():\n",
        "   # 'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in',\n",
        "   screens = [\n",
        "       'all_cryptocurrencies_us', 'all_cryptocurrencies_au', 'all_cryptocurrencies_ca', 'all_cryptocurrencies_eu', 'all_cryptocurrencies_gb', 'all_cryptocurrencies_in', ]\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   # print(len(symbols))\n",
        "   # pieces = 15\n",
        "   # new_arrays = np.array_split(symbols, pieces)\n",
        "   return symbols\n",
        "def spliting(data):\n",
        "  X = data.drop([\"buy\",\"sell\"], axis=1)\n",
        "  y = data[[\"buy\",\"sell\"]]\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.1)\n",
        "  print(xTrain.shape, end=\" \")\n",
        "  print(yTrain.shape)\n",
        "  print(xTest.shape, end=\" \")\n",
        "  print(yTest.shape)\n",
        "  return xTrain, xTest, yTrain, yTest\n",
        "def scaler(row):\n",
        "    scaler = MinMaxScaler(feature_range=(-10,10))\n",
        "    row = scaler.fit_transform(row)\n",
        "    return row\n",
        "def process(data):\n",
        "    row = []\n",
        "    data.drop(\"symbol\",axis=1,inplace=True)\n",
        "    data.drop(\"datetime\",axis=1,inplace=True)\n",
        "    data = np.array(data)\n",
        "    for i in range(31, data.shape[0]-1):\n",
        "        grow = []\n",
        "        ggrow = []\n",
        "        gggrow = []\n",
        "        ggggrow = []\n",
        "        for x in range(1, 31):\n",
        "            grow.append([data[i-x][0] - data[i-(x-1)][0]] )\n",
        "            ggrow.append([data[i-x][1] - data[i-(x-1)][1]] )\n",
        "            gggrow.append([data[i-x][2] - data[i-(x-1)][2]] )\n",
        "            ggggrow.append([data[i-x][3] - data[i-(x-1)][3]] )\n",
        "\n",
        "        sugg = \"sell\"\n",
        "        if data[i][3] > data[i][0]:\n",
        "            sugg = \"buy\"\n",
        "        arr = np.array(grow).flatten()\n",
        "        arr2 = np.array(ggrow).flatten()\n",
        "        arr3 = np.array(gggrow).flatten()\n",
        "        arr4 = np.array(ggggrow).flatten()\n",
        "\n",
        "        arr = scaler(arr.reshape(-1, 1))\n",
        "        arr2 = scaler(arr2.reshape(-1, 1))\n",
        "        arr3 = scaler(arr3.reshape(-1, 1))\n",
        "        arr4 = scaler(arr4.reshape(-1, 1))\n",
        "\n",
        "        arr = np.concatenate((arr, arr2, arr3, arr4), axis=0).reshape(-1,1)\n",
        "        arr = np.append(arr, sugg)\n",
        "        row.append(arr)\n",
        "    grow = []\n",
        "    ggrow = []\n",
        "    gggrow = []\n",
        "    arr = []\n",
        "    return np.array(row)"
      ],
      "metadata": {
        "id": "Yc7-yg1RMYBQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def each_file_proc(files,now,index):\n",
        "     data = []\n",
        "     unattached_dfs = []\n",
        "     files = list(files)\n",
        "     for file in files:\n",
        "        print(f\"{files.index(file)+1+index}\",end=\" \")\n",
        "        if (files.index(file)+index+1) % 40 == 0:\n",
        "          print(\" \")\n",
        "        address = f\"/content/data/{file}\"\n",
        "        try:\n",
        "           unattached_dfs.append(process(pd.read_csv(address)))\n",
        "        except:\n",
        "          print(\" EP \")\n",
        "     if np.array(unattached_dfs[0]).shape[0] == 0:\n",
        "            print(\" Null \")\n",
        "            data = np.array(unattached_dfs[1])\n",
        "            for z in unattached_dfs[2:]:\n",
        "               try: \n",
        "                  data = np.append(data, np.array(z), axis=0)\n",
        "               except:\n",
        "                  pass\n",
        "     else:\n",
        "            data = np.array(unattached_dfs[0])\n",
        "            for z in unattached_dfs[1:]:\n",
        "               try: \n",
        "                  data = np.append(data, np.array(z), axis=0)\n",
        "               except:\n",
        "                  pass\n",
        "     unattached_dfs = []\n",
        "     data = pd.DataFrame(data, columns=clmns)\n",
        "     sugg = data[\"suggestion\"]\n",
        "     data.drop(\"suggestion\",axis=1,inplace=True)\n",
        "     sugg = pd.get_dummies(sugg)\n",
        "     data = pd.concat([data,sugg],axis=1)\n",
        "     data = data.astype(float)\n",
        "     right_now = datetime.now().strftime(\"%H%M%S%f\")\n",
        "     data.to_csv(f\"/content/extracted/{now}/{right_now}.csv\")  \n",
        "def extract_data(pieces):\n",
        "  pd.options.mode.chained_assignment = None\n",
        "  print(f\"Files In Data : {len(os.listdir('/content/data/'))}\")\n",
        "  files = os.listdir(\"/content/data/\")\n",
        "  new_files = np.array_split(files, pieces)\n",
        "  print(\"Processing File:\")\n",
        "  now = datetime.now().strftime(\"%H%M%S\")\n",
        "  os.mkdir(f\"/content/extracted/{now}/\")\n",
        "  \n",
        "  index = 0 \n",
        "  for files in new_files:\n",
        "     \n",
        "     each_file_proc(files,now,index)\n",
        "     index = index + len(files)\n",
        "  print(\" \")\n",
        "  return now\n",
        "def delete_all_csv(now):\n",
        "   path = f'/content/extracted/{now}/*.csv'\n",
        "   files = glob.glob(path)\n",
        "   for file in files:\n",
        "       os.remove(file)\n",
        "def make_df(now):\n",
        "   path = f'/content/extracted/{now}/*.parquet'\n",
        "   files = glob.glob(path)\n",
        "   #data = pd.DataFrame()\n",
        "   data = pd.DataFrame()\n",
        "   for adr in files:\n",
        "     data =pd.concat([data,pd.read_parquet(adr)])\n",
        "   if \"Unnamed: 0\" in data:\n",
        "     data.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
        "   print(data.shape)\n",
        "   xTrain,xTest,yTrain,yTest = spliting(data)\n",
        "   data.to_parquet(f'/content/extracted/{now}/data.parquet')\n",
        "   delete_all_csv(now)\n",
        "   data = []\n",
        "   return xTrain,xTest,yTrain,yTest\n",
        "def to_par(now,howmanyfiles): \n",
        "    files = os.listdir(f\"/content/extracted/{now}/\")\n",
        "    addresses = []\n",
        "    for file in files:\n",
        "      addresses.append(f\"/content/extracted/{now}/{file}\")\n",
        "    new_adr = np.array_split(addresses,howmanyfiles)\n",
        "    for adrs in new_adr:\n",
        "      datas = []\n",
        "      for adr in adrs:\n",
        "        datas.append(pd.read_csv(adr))\n",
        "      rnow = datetime.now().strftime(\"%H%M%S%f\")\n",
        "      datas = pd.concat(datas)\n",
        "      datas.to_parquet(f\"/content/extracted/{now}/part_{rnow}.parquet\")      "
      ],
      "metadata": {
        "id": "RwcfzqyPMq27"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_name = extract_data(1)\n",
        "to_par(folder_name,1)\n",
        "xTrain,xTest,yTrain,yTest = make_df(folder_name)"
      ],
      "metadata": {
        "id": "nacRKx3ZMy2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1105348d-3807-4292-a6c6-5c2eb5e255e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files In Data : 92\n",
            "Processing File:\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  \n",
            "41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Dense(750, activation='relu', input_shape=(xTrain.shape[1],)))\n",
        "model.add(Dense(750, activation='relu'))\n",
        "model.add(Dense(750, activation='relu'))\n",
        "model.add(Dense(750, activation='relu'))\n",
        "model.add(Dense(750, activation='relu'))\n",
        "model.add(Dense(750, activation='relu'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adamax()\n",
        "\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "G5asREmlN2U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(xTrain,yTrain,epochs=30,batch_size=16,validation_data=(xTest,yTest))"
      ],
      "metadata": {
        "id": "pjwc42r1N5yA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "TV_g121t.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}