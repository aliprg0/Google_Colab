{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5bxbCoe9do9"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import tensorflow as tf\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sys import getsizeof\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IB_YMoe09qVP"
      },
      "outputs": [],
      "source": [
        "def read_syms_from_txt():  \n",
        "  with open(\"syms.txt\",\"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  lst = []\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    lst.append(line)\n",
        "  symbols = lst\n",
        "  return symbols\n",
        "def get_crypto_syms():\n",
        "  #'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in',\n",
        "   screens = [\n",
        " 'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in','all_cryptocurrencies_us']\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   #print(len(symbols))\n",
        "   #pieces = 15\n",
        "   # new_arrays = np.array_split(symbols, pieces)\n",
        "   return symbols\n",
        "def spliting(data):\n",
        "  X = data.drop([\"suggestion\"],axis=1)\n",
        "  y = data[\"suggestion\"]\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.1)\n",
        "  print(xTrain.shape,end=\" \")\n",
        "  print(yTrain.shape)\n",
        "  print(xTest.shape,end=\" \")\n",
        "  print(yTest.shape)\n",
        "  return xTrain, xTest, yTrain, yTest\n",
        "def extract_data(df):\n",
        "    rows = []\n",
        "    for each in range(13,df.shape[0]-1):\n",
        "        sugg = 0\n",
        "        if df[each][3] > df[each][0]:\n",
        "          sugg = 1\n",
        "        row = [\n",
        "                df[each-12][3],\n",
        "                df[each-11][3],\n",
        "                df[each-10][3],\n",
        "                df[each-9][3],\n",
        "                df[each-8][3],\n",
        "                df[each-7][3],\n",
        "                df[each-6][3],\n",
        "                df[each-5][3],\n",
        "                df[each-4][3],\n",
        "                df[each-3][3],\n",
        "                df[each-2][3],\n",
        "                df[each-1][3],\n",
        "                sugg\n",
        "        ]\n",
        "        rows.append(row)\n",
        "    return rows\n",
        "def row_scaler(df):\n",
        "  scaler = MinMaxScaler(feature_range=(0,5))\n",
        "  last_column = df.iloc[: , -1]\n",
        "  df = df.drop(columns=df.columns[-1], axis=1)\n",
        "  scaled = pd.DataFrame(scaler.fit_transform(df.T).T,dtype=object,columns=[\"12_candels_close\",\"11_candels_close\",\"10_candels_close\",\"9_candels_close\",\"8_candels_close\",\"7_candels_close\", \"6_candels_close\",\"5_candels_close\",\"4_candels_close\",\"3_candels_close\",\"2_candels_close\",\"1_candels_close\"])\n",
        "  scaled[\"suggestion\"] = last_column\n",
        "  return scaled\n",
        "def column_scaler(df):\n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  last_column = df.iloc[: , -1]\n",
        "  df = df.drop(columns=df.columns[-1], axis=1)\n",
        "  df_scaled = scaler.fit_transform(df.to_numpy())\n",
        "  df_scaled = pd.DataFrame(df_scaled, columns=[\"12_candels_close\",\"11_candels_close\",\"10_candels_close\",\"9_candels_close\",\"8_candels_close\",\"7_candels_close\", \"6_candels_close\",\"5_candels_close\",\"4_candels_close\",\"3_candels_close\",\"2_candels_close\",\"1_candels_close\"])\n",
        "  df_scaled[\"suggestion\"] = last_column\n",
        "  return df_scaled\n",
        "def stick_dfs(dfs):\n",
        "  dataframe = dfs[0]\n",
        "  for i in range(1,len(dfs)):\n",
        "     dataframe = pd.concat([dataframe,dfs[i]], ignore_index = True)\n",
        "  dataframe = dataframe.dropna()\n",
        "  dataframe = dataframe.astype(float)\n",
        "  return dataframe\n",
        "def process(dfs): \n",
        "   fixed_dfs = []\n",
        "   for df in dfs:\n",
        "      df = np.array(df)\n",
        "      df = extract_data(df)\n",
        "      df = pd.DataFrame(df,columns = [\"12_candels_close\",\"11_candels_close\",\"10_candels_close\",\"9_candels_close\",\"8_candels_close\",\"7_candels_close\", \"6_candels_close\",\"5_candels_close\",\"4_candels_close\",\"3_candels_close\",\"2_candels_close\",\"1_candels_close\",\"suggestion\"])\n",
        "      df = row_scaler(df)\n",
        "      #df = column_scaler(df)\n",
        "      fixed_dfs.append(df)\n",
        "   df = stick_dfs(fixed_dfs)\n",
        "   return df   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIAuU_ILbU27"
      },
      "outputs": [],
      "source": [
        "#symbols = [\"MSFT\",\"AAPL\",\"GOOG\",\"TSLA\",\"AMZN\"]\n",
        "#symbols = [\"BTC-USD\",\"LTC-USD\",\"TRX-USD\",\"XRP-USD\",\"ETH-USD\",\"BNB-USD\",\"DASH-USD\",\"VET-USD\",\"LINK-USD\",\"ADA-USD\",\"DOT-USD\",\"SOL-USD\",\"BCH-USD\",\"FTT-USD\",\"FIL-USD\",\"XMR-USD\"]\n",
        "#symbols = [\"AAPL\",\"MSFT\",\"TSLA\",\"GOOG\"]\n",
        "#symbols = [\"BTC-USD\",\"ETH-USD\"]\n",
        "#symbols = [\"BTC-USD\"]\n",
        "\n",
        "#symbols = get_crypto_syms()\n",
        "symbols = read_syms_from_txt()\n",
        "dfs = []\n",
        "for symbol in symbols:\n",
        "           data = yf.download(symbol,period=\"MAX\",interval=\"1d\",progress=False)\n",
        "           if data.empty :\n",
        "             print(\"Passing...\")\n",
        "           else:\n",
        "               dfs.append(data)\n",
        "ndfs = dfs\n",
        "dfs = []\n",
        "for df in ndfs:\n",
        "        if df.shape[0] > 13:\n",
        "          dfs.append(df)\n",
        "data = process(dfs)\n",
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%H%M%S\")\n",
        "data.to_csv(f\"{current_time}.csv\")\n",
        "xTrain, xTest, yTrain, yTest = spliting(data)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "address = \"/content/gdrive/MyDrive/12c_05sclaer.csv\"\n",
        "data = pd.read_csv(address)\n",
        "data = data.drop([\"Unnamed: 0\"],axis=1)\n",
        "xTrain, xTest, yTrain, yTest = spliting(data)\n",
        "data"
      ],
      "metadata": {
        "id": "RF0pHswQw7nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1rI1oRc9VRwO",
        "outputId": "1938e89e-1941-4b83-8ccd-329b274cdb96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_27 (Dense)            (None, 6000)              78000     \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 5000)              30005000  \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 4000)              20004000  \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 2000)              8002000   \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 700)               1400700   \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 300)               210300    \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 1)                 301       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 59,700,301\n",
            "Trainable params: 59,700,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "adam = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.0004,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=True,\n",
        "    name=\"Adam\")\n",
        "\n",
        "\n",
        "model.add(Dense(6000, activation='relu', input_shape=(xTrain.shape[1],)))\n",
        "model.add(Dense(5000, activation='relu'))\n",
        "model.add(Dense(4000, activation='relu'))\n",
        "model.add(Dense(2000, activation='relu'))\n",
        "model.add(Dense(700, activation='relu'))\n",
        "model.add(Dense(300, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDTaW4WnX330"
      },
      "outputs": [],
      "source": [
        "model.fit(xTrain,yTrain,epochs=111,batch_size=40000,validation_data=(xTest,yTest))\n",
        "model.save(f\"{model.evaluate(xTest,yTest)}.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46M8IzrdKgf8"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(xTrain,yTrain)\n",
        "print(f\"Random Forest :  {clf.score(xTest,yTest)}\")\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(xTrain, yTrain)\n",
        "print(f\"Logistic Regression: {logisticRegr.score(xTest, yTest)}\")\n",
        "nc = NearestCentroid()\n",
        "nc.fit(xTrain,yTrain)\n",
        "print(f\"Nearest Centroid: {nc.score(xTest,yTest)}\")\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(xTrain, yTrain)\n",
        "print(f\"GaussianNB: {gnb.score(xTest,yTest)}\")\n",
        "clf2 = KNeighborsClassifier(n_neighbors=2)\n",
        "clf2.fit(xTrain, yTrain)\n",
        "print(f\"K-Neighbors: {clf2.score(xTest,yTest)}\")\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(xTrain, yTrain)\n",
        "print(f\"Decision Tree: {tree.score(xTest,yTest)}\")\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
        "   max_depth=1, random_state=0).fit(xTrain, yTrain)\n",
        "gb.fit(xTrain,yTrain)\n",
        "print(f\"Gradient Boost: {gb.score(xTest, yTest)}\")\n",
        "svm = SVC()\n",
        "svm.fit(xTrain,yTrain)\n",
        "print(f\"SVM: {svm.score(xTest,yTest)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "files = os.listdir(\"/content/gdrive/MyDrive/Colab Files/\")\n",
        "\n",
        "data = pd.DataFrame()\n",
        "for file in files:\n",
        "  address = f\"/content/gdrive/MyDrive/Colab Files/{file}\"\n",
        "  df = pd.read_csv(address)\n",
        "  data = pd.concat([data,df])"
      ],
      "metadata": {
        "id": "SFwh6-67na1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symbols = read_syms_from_txt()\n",
        "len(symbols)\n",
        "pieces = 15\n",
        "new_arrays = np.array_split(symbols, pieces)\n",
        "for i in new_arrays:\n",
        "  dfs = []\n",
        "  for x in i:\n",
        "        data = yf.download(x,period=\"MAX\",interval=\"1d\",progress=False)\n",
        "        if data.empty :\n",
        "             print(\"Passing...\")\n",
        "        else:\n",
        "               dfs.append(data)\n",
        "  ndfs = dfs\n",
        "  dfs = []\n",
        "  for df in ndfs:\n",
        "        if df.shape[0] > 12:\n",
        "          dfs.append(df)\n",
        "  data = process(dfs)\n",
        "  data.to_csv(f\"{x}.csv\")         "
      ],
      "metadata": {
        "id": "IL0Gg7OMYIQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")\n"
      ],
      "metadata": {
        "id": "Nu3DTgF_qBLr",
        "outputId": "6ce25e8a-ccb1-4231-fdb6-f1d1db996dff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V83nKaaTxAhO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Binary_v42.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}