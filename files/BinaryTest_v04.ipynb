{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_5bxbCoe9do9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6432e1e8-9390-4ce8-92f7-bde9db53b9f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 24.5 MB/s \n",
            "\u001b[?25hCollecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 590 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.8.0 requests-2.27.1 yfinance-0.1.70\n",
            "Collecting yahooquery\n",
            "  Downloading yahooquery-2.2.15-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting requests-futures>=1.0.0\n",
            "  Downloading requests_futures-1.0.0-py2.py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: lxml>=4.6.2 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (4.8.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yahooquery) (1.15.0)\n",
            "Requirement already satisfied: requests>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from requests-futures>=1.0.0->yahooquery) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2021.10.8)\n",
            "Installing collected packages: requests-futures, yahooquery\n",
            "Successfully installed requests-futures-1.0.0 yahooquery-2.2.15\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import tensorflow as tf\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sys import getsizeof\n",
        "from datetime import datetime\n",
        "from numpy import array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IB_YMoe09qVP"
      },
      "outputs": [],
      "source": [
        "def read_syms_from_txt():  \n",
        "  with open(\"syms.txt\",\"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  lst = []\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    lst.append(line)\n",
        "  symbols = lst\n",
        "  return symbols\n",
        "def get_crypto_syms():\n",
        "  #'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in',\n",
        "   screens = [\n",
        "    'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in','all_cryptocurrencies_us']\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   #print(len(symbols))\n",
        "   #pieces = 15\n",
        "   # new_arrays = np.array_split(symbols, pieces)\n",
        "   return symbols\n",
        "def spliting(data):\n",
        "  X = data.drop([\"suggestion\"],axis=1)\n",
        "  y = data[\"suggestion\"]\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.1)\n",
        "  print(xTrain.shape,end=\" \")\n",
        "  print(yTrain.shape)\n",
        "  print(xTest.shape,end=\" \")\n",
        "  print(yTest.shape)\n",
        "  return xTrain, xTest, yTrain, yTest\n",
        "def extract_data(df):\n",
        "    rows = []\n",
        "    for each in range(11,df.shape[0]-1):\n",
        "        sugg = 0\n",
        "        if df[each][3] > df[each][0]:\n",
        "          sugg = 1\n",
        "        row = [\n",
        "                df[each-12][0],\n",
        "                df[each-12][3],\n",
        "                df[each-12][4],\n",
        "                df[each-11][0],\n",
        "                df[each-11][3],\n",
        "                df[each-11][4],\n",
        "                df[each-10][0],\n",
        "                df[each-10][3],\n",
        "                df[each-10][4],\n",
        "                df[each-9][0],\n",
        "                df[each-9][3],\n",
        "                df[each-9][4],\n",
        "                df[each-8][0],\n",
        "                df[each-8][3],\n",
        "                df[each-8][4],\n",
        "                df[each-7][0],\n",
        "                df[each-7][3],\n",
        "                df[each-7][4],\n",
        "                df[each-6][0],\n",
        "                df[each-6][3],\n",
        "                df[each-6][4],\n",
        "                df[each-5][0],\n",
        "                df[each-5][3],\n",
        "                df[each-5][4],\n",
        "                df[each-4][0],\n",
        "                df[each-4][3],\n",
        "                df[each-4][4],\n",
        "                df[each-3][0],\n",
        "                df[each-3][3],\n",
        "                df[each-3][4],\n",
        "                df[each-2][0],\n",
        "                df[each-2][3],\n",
        "                df[each-2][4],\n",
        "                df[each-1][0],\n",
        "                df[each-1][3],\n",
        "                df[each-1][4],\n",
        "                sugg\n",
        "        ]\n",
        "        rows.append(row)\n",
        "    return rows\n",
        "def row_scaler(df):\n",
        "  scaler = MinMaxScaler(feature_range=(1,10))\n",
        "  last_column = df.iloc[: , -1]\n",
        "  df = df.drop(columns=df.columns[-1], axis=1)\n",
        "  scaled = pd.DataFrame(scaler.fit_transform(df.T).T,dtype=object)\n",
        "  scaled[\"suggestion\"] = last_column\n",
        "  return scaled\n",
        "def column_scaler(df):\n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  last_column = df.iloc[: , -1]\n",
        "  df = df.drop(columns=df.columns[-1], axis=1)\n",
        "  df_scaled = scaler.fit_transform(df.to_numpy())\n",
        "  df_scaled = pd.DataFrame(df_scaled)\n",
        "  df_scaled[\"suggestion\"] = last_column\n",
        "  return df_scaled\n",
        "def stick_dfs(dfs):\n",
        "  dataframe = dfs[0]\n",
        "  for i in range(1,len(dfs)):\n",
        "     dataframe = pd.concat([dataframe,dfs[i]], ignore_index = True)\n",
        "  dataframe = dataframe.dropna()\n",
        "  dataframe = dataframe.astype(float)\n",
        "  return dataframe\n",
        "def process(dfs): \n",
        "   fixed_dfs = []\n",
        "   for df in dfs:\n",
        "      df = np.array(df)\n",
        "      df = extract_data(df)\n",
        "      df = pd.DataFrame(df)\n",
        "      df = row_scaler(df)\n",
        "      #df = column_scaler(df)\n",
        "      fixed_dfs.append(df)\n",
        "   df = stick_dfs(fixed_dfs)\n",
        "   return df   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIAuU_ILbU27"
      },
      "outputs": [],
      "source": [
        "#symbols = [\"MSFT\",\"AAPL\",\"GOOG\",\"TSLA\",\"AMZN\"]\n",
        "#symbols = [\"BTC-USD\",\"LTC-USD\",\"TRX-USD\",\"XRP-USD\",\"ETH-USD\",\"BNB-USD\",\"DASH-USD\",\"VET-USD\",\"LINK-USD\",\"ADA-USD\",\"DOT-USD\",\"SOL-USD\",\"BCH-USD\",\"FTT-USD\",\"FIL-USD\",\"XMR-USD\"]\n",
        "#symbols = [\"AAPL\",\"MSFT\",\"TSLA\",\"GOOG\"]\n",
        "#symbols = [\"BTC-USD\",\"ETH-USD\"]\n",
        "#symbols = [\"BTC-USD\"]\n",
        "\n",
        "symbols = get_crypto_syms()\n",
        "#symbols = read_syms_from_txt()\n",
        "dfs = []\n",
        "for symbol in symbols:\n",
        "           data = yf.download(symbol,period=\"MAX\",interval=\"1d\",progress=False)\n",
        "           if data.empty :\n",
        "             print(\"Passing...\")\n",
        "           else:\n",
        "               dfs.append(data)\n",
        "ndfs = dfs\n",
        "dfs = []\n",
        "for df in ndfs:\n",
        "        if df.shape[0] > 13:\n",
        "          dfs.append(df)\n",
        "\n",
        "data = process(dfs)\n",
        "xTrain, xTest, yTrain, yTest = spliting(data)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "data = df.read_csv(\"/content/drive/MyDrive/Colab Files/df_BinaryTesting02.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UC6W88bOvyxe",
        "outputId": "9ec307df-818e-43c8-c169-4d137a0c3fea"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 0         1         2         3         4         5  \\\n",
              "0        10.000000  9.962342  9.962342  1.011749  1.011339  1.011339   \n",
              "1        10.000000  9.725496  9.725496  9.666847  5.515231  5.515231   \n",
              "2        10.000000  5.736592  5.736592  5.693888  2.277124  2.277124   \n",
              "3         7.035992  2.642286  2.642286  2.622124  4.954872  4.954872   \n",
              "4         2.622124  4.954872  4.954872  4.820620  3.236222  3.236222   \n",
              "...            ...       ...       ...       ...       ...       ...   \n",
              "1765264   1.642061  3.063500  3.063500  3.061348  1.052867  1.052867   \n",
              "1765265   3.937438  2.182552  2.182552  2.183959  2.136360  2.136360   \n",
              "1765266   2.183959  2.136360  2.136360  2.138111  2.463915  2.463915   \n",
              "1765267   2.138111  2.463915  2.463915  2.463166  6.006860  6.006860   \n",
              "1765268   2.463166  6.006860  6.006860  6.011186  2.630554  2.630554   \n",
              "\n",
              "                6         7         8         9  ...        27         28  \\\n",
              "0        1.011252  1.005053  1.005053  1.004991  ...  1.005890   1.004207   \n",
              "1        5.473647  2.146474  2.146474  2.131205  ...  4.930512   4.281240   \n",
              "2        2.261444  4.075505  4.075505  3.971105  ...  4.356526   3.832417   \n",
              "3        4.820620  3.236222  3.236222  3.281911  ...  4.634102   1.128308   \n",
              "4        3.281911  4.226854  4.226854  4.216962  ...  1.085915   1.000000   \n",
              "...           ...       ...       ...       ...  ...       ...        ...   \n",
              "1765264  1.054477  1.000000  1.000000  1.002004  ...  3.074995   8.558728   \n",
              "1765265  2.138111  2.463915  2.463915  2.463166  ...  8.740223  10.000000   \n",
              "1765266  2.463166  6.006860  6.006860  6.011186  ...  9.999899   6.141116   \n",
              "1765267  6.011186  2.630554  2.630554  2.630223  ...  6.140918   1.000000   \n",
              "1765268  2.630223  3.048352  3.048352  3.047308  ...  1.000924   3.114194   \n",
              "\n",
              "                29        30         31         32        33        34  \\\n",
              "0         1.004207  1.004180   1.003210   1.003210  1.003046  1.002284   \n",
              "1         4.281240  4.171352   3.660986   3.660986  3.654794  1.000000   \n",
              "2         3.832417  3.826059   1.099779   1.099779  1.066812  1.000000   \n",
              "3         1.128308  1.085915   1.000000   1.000000  1.104318  2.741943   \n",
              "4         1.000000  1.104318   2.741943   2.741943  2.822847  2.140218   \n",
              "...            ...       ...        ...        ...       ...       ...   \n",
              "1765264   8.558728  8.558176  10.000000  10.000000  9.999884  5.583475   \n",
              "1765265  10.000000  9.999899   6.141116   6.141116  6.140918  1.000000   \n",
              "1765266   6.141116  6.140918   1.000000   1.000000  1.000924  3.114194   \n",
              "1765267   1.000000  1.000924   3.114194   3.114194  3.114139  5.564232   \n",
              "1765268   3.114194  3.114139   5.564232   5.564232  5.563657  1.767651   \n",
              "\n",
              "               35  suggestion  \n",
              "0        1.002284         0.0  \n",
              "1        1.000000         0.0  \n",
              "2        1.000000         1.0  \n",
              "3        2.741943         0.0  \n",
              "4        2.140218         0.0  \n",
              "...           ...         ...  \n",
              "1765264  5.583475         0.0  \n",
              "1765265  1.000000         1.0  \n",
              "1765266  3.114194         1.0  \n",
              "1765267  5.564232         0.0  \n",
              "1765268  1.767651         0.0  \n",
              "\n",
              "[1765269 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3609b8c4-e756-472b-99f0-d6f48da9cc5a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>suggestion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.962342</td>\n",
              "      <td>9.962342</td>\n",
              "      <td>1.011749</td>\n",
              "      <td>1.011339</td>\n",
              "      <td>1.011339</td>\n",
              "      <td>1.011252</td>\n",
              "      <td>1.005053</td>\n",
              "      <td>1.005053</td>\n",
              "      <td>1.004991</td>\n",
              "      <td>...</td>\n",
              "      <td>1.005890</td>\n",
              "      <td>1.004207</td>\n",
              "      <td>1.004207</td>\n",
              "      <td>1.004180</td>\n",
              "      <td>1.003210</td>\n",
              "      <td>1.003210</td>\n",
              "      <td>1.003046</td>\n",
              "      <td>1.002284</td>\n",
              "      <td>1.002284</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.725496</td>\n",
              "      <td>9.725496</td>\n",
              "      <td>9.666847</td>\n",
              "      <td>5.515231</td>\n",
              "      <td>5.515231</td>\n",
              "      <td>5.473647</td>\n",
              "      <td>2.146474</td>\n",
              "      <td>2.146474</td>\n",
              "      <td>2.131205</td>\n",
              "      <td>...</td>\n",
              "      <td>4.930512</td>\n",
              "      <td>4.281240</td>\n",
              "      <td>4.281240</td>\n",
              "      <td>4.171352</td>\n",
              "      <td>3.660986</td>\n",
              "      <td>3.660986</td>\n",
              "      <td>3.654794</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>5.736592</td>\n",
              "      <td>5.736592</td>\n",
              "      <td>5.693888</td>\n",
              "      <td>2.277124</td>\n",
              "      <td>2.277124</td>\n",
              "      <td>2.261444</td>\n",
              "      <td>4.075505</td>\n",
              "      <td>4.075505</td>\n",
              "      <td>3.971105</td>\n",
              "      <td>...</td>\n",
              "      <td>4.356526</td>\n",
              "      <td>3.832417</td>\n",
              "      <td>3.832417</td>\n",
              "      <td>3.826059</td>\n",
              "      <td>1.099779</td>\n",
              "      <td>1.099779</td>\n",
              "      <td>1.066812</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.035992</td>\n",
              "      <td>2.642286</td>\n",
              "      <td>2.642286</td>\n",
              "      <td>2.622124</td>\n",
              "      <td>4.954872</td>\n",
              "      <td>4.954872</td>\n",
              "      <td>4.820620</td>\n",
              "      <td>3.236222</td>\n",
              "      <td>3.236222</td>\n",
              "      <td>3.281911</td>\n",
              "      <td>...</td>\n",
              "      <td>4.634102</td>\n",
              "      <td>1.128308</td>\n",
              "      <td>1.128308</td>\n",
              "      <td>1.085915</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.104318</td>\n",
              "      <td>2.741943</td>\n",
              "      <td>2.741943</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.622124</td>\n",
              "      <td>4.954872</td>\n",
              "      <td>4.954872</td>\n",
              "      <td>4.820620</td>\n",
              "      <td>3.236222</td>\n",
              "      <td>3.236222</td>\n",
              "      <td>3.281911</td>\n",
              "      <td>4.226854</td>\n",
              "      <td>4.226854</td>\n",
              "      <td>4.216962</td>\n",
              "      <td>...</td>\n",
              "      <td>1.085915</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.104318</td>\n",
              "      <td>2.741943</td>\n",
              "      <td>2.741943</td>\n",
              "      <td>2.822847</td>\n",
              "      <td>2.140218</td>\n",
              "      <td>2.140218</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1765264</th>\n",
              "      <td>1.642061</td>\n",
              "      <td>3.063500</td>\n",
              "      <td>3.063500</td>\n",
              "      <td>3.061348</td>\n",
              "      <td>1.052867</td>\n",
              "      <td>1.052867</td>\n",
              "      <td>1.054477</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.002004</td>\n",
              "      <td>...</td>\n",
              "      <td>3.074995</td>\n",
              "      <td>8.558728</td>\n",
              "      <td>8.558728</td>\n",
              "      <td>8.558176</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.999884</td>\n",
              "      <td>5.583475</td>\n",
              "      <td>5.583475</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1765265</th>\n",
              "      <td>3.937438</td>\n",
              "      <td>2.182552</td>\n",
              "      <td>2.182552</td>\n",
              "      <td>2.183959</td>\n",
              "      <td>2.136360</td>\n",
              "      <td>2.136360</td>\n",
              "      <td>2.138111</td>\n",
              "      <td>2.463915</td>\n",
              "      <td>2.463915</td>\n",
              "      <td>2.463166</td>\n",
              "      <td>...</td>\n",
              "      <td>8.740223</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.999899</td>\n",
              "      <td>6.141116</td>\n",
              "      <td>6.141116</td>\n",
              "      <td>6.140918</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1765266</th>\n",
              "      <td>2.183959</td>\n",
              "      <td>2.136360</td>\n",
              "      <td>2.136360</td>\n",
              "      <td>2.138111</td>\n",
              "      <td>2.463915</td>\n",
              "      <td>2.463915</td>\n",
              "      <td>2.463166</td>\n",
              "      <td>6.006860</td>\n",
              "      <td>6.006860</td>\n",
              "      <td>6.011186</td>\n",
              "      <td>...</td>\n",
              "      <td>9.999899</td>\n",
              "      <td>6.141116</td>\n",
              "      <td>6.141116</td>\n",
              "      <td>6.140918</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000924</td>\n",
              "      <td>3.114194</td>\n",
              "      <td>3.114194</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1765267</th>\n",
              "      <td>2.138111</td>\n",
              "      <td>2.463915</td>\n",
              "      <td>2.463915</td>\n",
              "      <td>2.463166</td>\n",
              "      <td>6.006860</td>\n",
              "      <td>6.006860</td>\n",
              "      <td>6.011186</td>\n",
              "      <td>2.630554</td>\n",
              "      <td>2.630554</td>\n",
              "      <td>2.630223</td>\n",
              "      <td>...</td>\n",
              "      <td>6.140918</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000924</td>\n",
              "      <td>3.114194</td>\n",
              "      <td>3.114194</td>\n",
              "      <td>3.114139</td>\n",
              "      <td>5.564232</td>\n",
              "      <td>5.564232</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1765268</th>\n",
              "      <td>2.463166</td>\n",
              "      <td>6.006860</td>\n",
              "      <td>6.006860</td>\n",
              "      <td>6.011186</td>\n",
              "      <td>2.630554</td>\n",
              "      <td>2.630554</td>\n",
              "      <td>2.630223</td>\n",
              "      <td>3.048352</td>\n",
              "      <td>3.048352</td>\n",
              "      <td>3.047308</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000924</td>\n",
              "      <td>3.114194</td>\n",
              "      <td>3.114194</td>\n",
              "      <td>3.114139</td>\n",
              "      <td>5.564232</td>\n",
              "      <td>5.564232</td>\n",
              "      <td>5.563657</td>\n",
              "      <td>1.767651</td>\n",
              "      <td>1.767651</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1765269 rows × 37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3609b8c4-e756-472b-99f0-d6f48da9cc5a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3609b8c4-e756-472b-99f0-d6f48da9cc5a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3609b8c4-e756-472b-99f0-d6f48da9cc5a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1rI1oRc9VRwO",
        "outputId": "afbbcf8e-101b-4876-b817-3ff5bc5d88f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_27 (Dense)            (None, 4096)              151552    \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 1024)              4195328   \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 256)               262400    \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 100)               25700     \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,635,081\n",
            "Trainable params: 4,635,081\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Dense(4096, activation='relu', input_shape=(xTrain.shape[1],)))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JDTaW4WnX330",
        "outputId": "4f658a14-407f-4cda-d1b9-92c90e7085ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "194/194 [==============================] - 12s 59ms/step - loss: 0.7243 - accuracy: 0.5272 - val_loss: 0.6875 - val_accuracy: 0.5420\n",
            "Epoch 2/200\n",
            "194/194 [==============================] - 11s 58ms/step - loss: 0.6863 - accuracy: 0.5457 - val_loss: 0.6847 - val_accuracy: 0.5502\n",
            "Epoch 3/200\n",
            "194/194 [==============================] - 11s 59ms/step - loss: 0.6836 - accuracy: 0.5542 - val_loss: 0.6817 - val_accuracy: 0.5584\n",
            "Epoch 4/200\n",
            "194/194 [==============================] - 11s 58ms/step - loss: 0.6810 - accuracy: 0.5606 - val_loss: 0.6796 - val_accuracy: 0.5632\n",
            "Epoch 5/200\n",
            "194/194 [==============================] - 12s 61ms/step - loss: 0.6783 - accuracy: 0.5659 - val_loss: 0.6761 - val_accuracy: 0.5697\n",
            "Epoch 6/200\n",
            "194/194 [==============================] - 12s 61ms/step - loss: 0.6744 - accuracy: 0.5739 - val_loss: 0.6750 - val_accuracy: 0.5715\n",
            "Epoch 7/200\n",
            "194/194 [==============================] - 12s 60ms/step - loss: 0.6708 - accuracy: 0.5792 - val_loss: 0.6697 - val_accuracy: 0.5801\n",
            "Epoch 8/200\n",
            "194/194 [==============================] - 12s 60ms/step - loss: 0.6660 - accuracy: 0.5877 - val_loss: 0.6668 - val_accuracy: 0.5855\n",
            "Epoch 9/200\n",
            "194/194 [==============================] - 12s 61ms/step - loss: 0.6598 - accuracy: 0.5964 - val_loss: 0.6577 - val_accuracy: 0.5984\n",
            "Epoch 10/200\n",
            "194/194 [==============================] - 12s 60ms/step - loss: 0.6517 - accuracy: 0.6064 - val_loss: 0.6479 - val_accuracy: 0.6118\n",
            "Epoch 11/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.6423 - accuracy: 0.6174 - val_loss: 0.6412 - val_accuracy: 0.6178\n",
            "Epoch 12/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.6311 - accuracy: 0.6298 - val_loss: 0.6314 - val_accuracy: 0.6290\n",
            "Epoch 13/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.6179 - accuracy: 0.6439 - val_loss: 0.6288 - val_accuracy: 0.6349\n",
            "Epoch 14/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.6032 - accuracy: 0.6592 - val_loss: 0.6060 - val_accuracy: 0.6559\n",
            "Epoch 15/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.5858 - accuracy: 0.6758 - val_loss: 0.5941 - val_accuracy: 0.6676\n",
            "Epoch 16/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.5661 - accuracy: 0.6934 - val_loss: 0.5802 - val_accuracy: 0.6826\n",
            "Epoch 17/200\n",
            "194/194 [==============================] - 12s 62ms/step - loss: 0.5443 - accuracy: 0.7121 - val_loss: 0.5652 - val_accuracy: 0.6961\n",
            "Epoch 18/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.5211 - accuracy: 0.7305 - val_loss: 0.5470 - val_accuracy: 0.7140\n",
            "Epoch 19/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.4967 - accuracy: 0.7491 - val_loss: 0.5288 - val_accuracy: 0.7294\n",
            "Epoch 20/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.4720 - accuracy: 0.7667 - val_loss: 0.5117 - val_accuracy: 0.7429\n",
            "Epoch 21/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.4480 - accuracy: 0.7829 - val_loss: 0.4936 - val_accuracy: 0.7574\n",
            "Epoch 22/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.4243 - accuracy: 0.7983 - val_loss: 0.4835 - val_accuracy: 0.7667\n",
            "Epoch 23/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.4032 - accuracy: 0.8116 - val_loss: 0.4644 - val_accuracy: 0.7817\n",
            "Epoch 24/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.3805 - accuracy: 0.8258 - val_loss: 0.4564 - val_accuracy: 0.7903\n",
            "Epoch 25/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.3620 - accuracy: 0.8368 - val_loss: 0.4407 - val_accuracy: 0.8024\n",
            "Epoch 26/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.3440 - accuracy: 0.8470 - val_loss: 0.4328 - val_accuracy: 0.8079\n",
            "Epoch 27/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.3272 - accuracy: 0.8563 - val_loss: 0.4216 - val_accuracy: 0.8180\n",
            "Epoch 28/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.3121 - accuracy: 0.8645 - val_loss: 0.4172 - val_accuracy: 0.8238\n",
            "Epoch 29/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.2975 - accuracy: 0.8727 - val_loss: 0.4084 - val_accuracy: 0.8296\n",
            "Epoch 30/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.2843 - accuracy: 0.8795 - val_loss: 0.4000 - val_accuracy: 0.8379\n",
            "Epoch 31/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.2722 - accuracy: 0.8857 - val_loss: 0.3991 - val_accuracy: 0.8405\n",
            "Epoch 32/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.2622 - accuracy: 0.8910 - val_loss: 0.3919 - val_accuracy: 0.8468\n",
            "Epoch 33/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.2517 - accuracy: 0.8963 - val_loss: 0.3969 - val_accuracy: 0.8467\n",
            "Epoch 34/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.2426 - accuracy: 0.9007 - val_loss: 0.3816 - val_accuracy: 0.8568\n",
            "Epoch 35/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.2323 - accuracy: 0.9057 - val_loss: 0.3856 - val_accuracy: 0.8567\n",
            "Epoch 36/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.2252 - accuracy: 0.9090 - val_loss: 0.3839 - val_accuracy: 0.8600\n",
            "Epoch 37/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.2162 - accuracy: 0.9135 - val_loss: 0.3738 - val_accuracy: 0.8676\n",
            "Epoch 38/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.2100 - accuracy: 0.9160 - val_loss: 0.3816 - val_accuracy: 0.8654\n",
            "Epoch 39/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.2037 - accuracy: 0.9192 - val_loss: 0.3750 - val_accuracy: 0.8705\n",
            "Epoch 40/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1961 - accuracy: 0.9225 - val_loss: 0.3728 - val_accuracy: 0.8738\n",
            "Epoch 41/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1914 - accuracy: 0.9246 - val_loss: 0.3735 - val_accuracy: 0.8750\n",
            "Epoch 42/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1852 - accuracy: 0.9276 - val_loss: 0.3721 - val_accuracy: 0.8769\n",
            "Epoch 43/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1817 - accuracy: 0.9290 - val_loss: 0.3704 - val_accuracy: 0.8803\n",
            "Epoch 44/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1764 - accuracy: 0.9314 - val_loss: 0.3737 - val_accuracy: 0.8795\n",
            "Epoch 45/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1706 - accuracy: 0.9339 - val_loss: 0.3744 - val_accuracy: 0.8801\n",
            "Epoch 46/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1681 - accuracy: 0.9349 - val_loss: 0.3754 - val_accuracy: 0.8849\n",
            "Epoch 47/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1634 - accuracy: 0.9372 - val_loss: 0.3787 - val_accuracy: 0.8833\n",
            "Epoch 48/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.1614 - accuracy: 0.9377 - val_loss: 0.3739 - val_accuracy: 0.8864\n",
            "Epoch 49/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1563 - accuracy: 0.9401 - val_loss: 0.3767 - val_accuracy: 0.8863\n",
            "Epoch 50/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1528 - accuracy: 0.9418 - val_loss: 0.3755 - val_accuracy: 0.8879\n",
            "Epoch 51/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1484 - accuracy: 0.9435 - val_loss: 0.3811 - val_accuracy: 0.8877\n",
            "Epoch 52/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1479 - accuracy: 0.9436 - val_loss: 0.3790 - val_accuracy: 0.8896\n",
            "Epoch 53/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.1452 - accuracy: 0.9450 - val_loss: 0.3800 - val_accuracy: 0.8913\n",
            "Epoch 54/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1407 - accuracy: 0.9467 - val_loss: 0.3792 - val_accuracy: 0.8915\n",
            "Epoch 55/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1383 - accuracy: 0.9477 - val_loss: 0.3816 - val_accuracy: 0.8928\n",
            "Epoch 56/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1362 - accuracy: 0.9486 - val_loss: 0.3852 - val_accuracy: 0.8918\n",
            "Epoch 57/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1340 - accuracy: 0.9497 - val_loss: 0.3794 - val_accuracy: 0.8950\n",
            "Epoch 58/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1290 - accuracy: 0.9518 - val_loss: 0.3841 - val_accuracy: 0.8955\n",
            "Epoch 59/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1289 - accuracy: 0.9516 - val_loss: 0.3798 - val_accuracy: 0.8976\n",
            "Epoch 60/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.1255 - accuracy: 0.9533 - val_loss: 0.3814 - val_accuracy: 0.8987\n",
            "Epoch 61/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1257 - accuracy: 0.9528 - val_loss: 0.3879 - val_accuracy: 0.8977\n",
            "Epoch 62/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1225 - accuracy: 0.9545 - val_loss: 0.3917 - val_accuracy: 0.8985\n",
            "Epoch 63/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1212 - accuracy: 0.9548 - val_loss: 0.3879 - val_accuracy: 0.8983\n",
            "Epoch 64/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1194 - accuracy: 0.9556 - val_loss: 0.3907 - val_accuracy: 0.8992\n",
            "Epoch 65/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1161 - accuracy: 0.9569 - val_loss: 0.3847 - val_accuracy: 0.9010\n",
            "Epoch 66/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1176 - accuracy: 0.9562 - val_loss: 0.3881 - val_accuracy: 0.8999\n",
            "Epoch 67/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1140 - accuracy: 0.9578 - val_loss: 0.3930 - val_accuracy: 0.9004\n",
            "Epoch 68/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1121 - accuracy: 0.9586 - val_loss: 0.3925 - val_accuracy: 0.9010\n",
            "Epoch 69/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1120 - accuracy: 0.9585 - val_loss: 0.3955 - val_accuracy: 0.9028\n",
            "Epoch 70/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1115 - accuracy: 0.9584 - val_loss: 0.3897 - val_accuracy: 0.9042\n",
            "Epoch 71/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1095 - accuracy: 0.9593 - val_loss: 0.4016 - val_accuracy: 0.9025\n",
            "Epoch 72/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1075 - accuracy: 0.9604 - val_loss: 0.3982 - val_accuracy: 0.9022\n",
            "Epoch 73/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1058 - accuracy: 0.9609 - val_loss: 0.3966 - val_accuracy: 0.9029\n",
            "Epoch 74/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1056 - accuracy: 0.9611 - val_loss: 0.3960 - val_accuracy: 0.9050\n",
            "Epoch 75/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1024 - accuracy: 0.9623 - val_loss: 0.3978 - val_accuracy: 0.9051\n",
            "Epoch 76/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.1034 - accuracy: 0.9618 - val_loss: 0.4007 - val_accuracy: 0.9039\n",
            "Epoch 77/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.1025 - accuracy: 0.9621 - val_loss: 0.4058 - val_accuracy: 0.9037\n",
            "Epoch 78/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.1002 - accuracy: 0.9632 - val_loss: 0.3972 - val_accuracy: 0.9053\n",
            "Epoch 79/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0998 - accuracy: 0.9632 - val_loss: 0.4021 - val_accuracy: 0.9044\n",
            "Epoch 80/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0972 - accuracy: 0.9644 - val_loss: 0.4037 - val_accuracy: 0.9060\n",
            "Epoch 81/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0976 - accuracy: 0.9640 - val_loss: 0.4011 - val_accuracy: 0.9080\n",
            "Epoch 82/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0965 - accuracy: 0.9646 - val_loss: 0.4171 - val_accuracy: 0.9042\n",
            "Epoch 83/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0954 - accuracy: 0.9651 - val_loss: 0.4083 - val_accuracy: 0.9071\n",
            "Epoch 84/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0927 - accuracy: 0.9661 - val_loss: 0.4138 - val_accuracy: 0.9064\n",
            "Epoch 85/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0935 - accuracy: 0.9658 - val_loss: 0.4113 - val_accuracy: 0.9059\n",
            "Epoch 86/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0926 - accuracy: 0.9662 - val_loss: 0.4109 - val_accuracy: 0.9051\n",
            "Epoch 87/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0918 - accuracy: 0.9665 - val_loss: 0.4117 - val_accuracy: 0.9085\n",
            "Epoch 88/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0917 - accuracy: 0.9662 - val_loss: 0.4151 - val_accuracy: 0.9060\n",
            "Epoch 89/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0894 - accuracy: 0.9673 - val_loss: 0.4209 - val_accuracy: 0.9059\n",
            "Epoch 90/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0889 - accuracy: 0.9675 - val_loss: 0.4183 - val_accuracy: 0.9067\n",
            "Epoch 91/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0892 - accuracy: 0.9675 - val_loss: 0.4174 - val_accuracy: 0.9063\n",
            "Epoch 92/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0875 - accuracy: 0.9682 - val_loss: 0.4136 - val_accuracy: 0.9096\n",
            "Epoch 93/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0861 - accuracy: 0.9686 - val_loss: 0.4259 - val_accuracy: 0.9076\n",
            "Epoch 94/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0862 - accuracy: 0.9686 - val_loss: 0.4186 - val_accuracy: 0.9087\n",
            "Epoch 95/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0860 - accuracy: 0.9687 - val_loss: 0.4169 - val_accuracy: 0.9088\n",
            "Epoch 96/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0850 - accuracy: 0.9691 - val_loss: 0.4184 - val_accuracy: 0.9093\n",
            "Epoch 97/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0848 - accuracy: 0.9692 - val_loss: 0.4162 - val_accuracy: 0.9111\n",
            "Epoch 98/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0829 - accuracy: 0.9699 - val_loss: 0.4154 - val_accuracy: 0.9111\n",
            "Epoch 99/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0824 - accuracy: 0.9700 - val_loss: 0.4138 - val_accuracy: 0.9114\n",
            "Epoch 100/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0823 - accuracy: 0.9702 - val_loss: 0.4223 - val_accuracy: 0.9104\n",
            "Epoch 101/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0835 - accuracy: 0.9695 - val_loss: 0.4174 - val_accuracy: 0.9118\n",
            "Epoch 102/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0813 - accuracy: 0.9706 - val_loss: 0.4235 - val_accuracy: 0.9094\n",
            "Epoch 103/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0799 - accuracy: 0.9710 - val_loss: 0.4275 - val_accuracy: 0.9090\n",
            "Epoch 104/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0809 - accuracy: 0.9706 - val_loss: 0.4286 - val_accuracy: 0.9095\n",
            "Epoch 105/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0782 - accuracy: 0.9715 - val_loss: 0.4288 - val_accuracy: 0.9108\n",
            "Epoch 106/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0793 - accuracy: 0.9712 - val_loss: 0.4234 - val_accuracy: 0.9112\n",
            "Epoch 107/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0777 - accuracy: 0.9719 - val_loss: 0.4274 - val_accuracy: 0.9113\n",
            "Epoch 108/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0783 - accuracy: 0.9717 - val_loss: 0.4257 - val_accuracy: 0.9109\n",
            "Epoch 109/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0779 - accuracy: 0.9719 - val_loss: 0.4203 - val_accuracy: 0.9128\n",
            "Epoch 110/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0764 - accuracy: 0.9725 - val_loss: 0.4345 - val_accuracy: 0.9114\n",
            "Epoch 111/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0755 - accuracy: 0.9728 - val_loss: 0.4318 - val_accuracy: 0.9115\n",
            "Epoch 112/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0746 - accuracy: 0.9731 - val_loss: 0.4276 - val_accuracy: 0.9126\n",
            "Epoch 113/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0749 - accuracy: 0.9730 - val_loss: 0.4329 - val_accuracy: 0.9114\n",
            "Epoch 114/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0743 - accuracy: 0.9732 - val_loss: 0.4360 - val_accuracy: 0.9123\n",
            "Epoch 115/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0735 - accuracy: 0.9734 - val_loss: 0.4277 - val_accuracy: 0.9134\n",
            "Epoch 116/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0744 - accuracy: 0.9730 - val_loss: 0.4351 - val_accuracy: 0.9126\n",
            "Epoch 117/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0729 - accuracy: 0.9738 - val_loss: 0.4355 - val_accuracy: 0.9139\n",
            "Epoch 118/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0718 - accuracy: 0.9741 - val_loss: 0.4364 - val_accuracy: 0.9126\n",
            "Epoch 119/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0730 - accuracy: 0.9736 - val_loss: 0.4397 - val_accuracy: 0.9118\n",
            "Epoch 120/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0722 - accuracy: 0.9739 - val_loss: 0.4376 - val_accuracy: 0.9131\n",
            "Epoch 121/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0711 - accuracy: 0.9744 - val_loss: 0.4374 - val_accuracy: 0.9134\n",
            "Epoch 122/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0703 - accuracy: 0.9747 - val_loss: 0.4350 - val_accuracy: 0.9139\n",
            "Epoch 123/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0696 - accuracy: 0.9749 - val_loss: 0.4409 - val_accuracy: 0.9146\n",
            "Epoch 124/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0704 - accuracy: 0.9746 - val_loss: 0.4406 - val_accuracy: 0.9124\n",
            "Epoch 125/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0700 - accuracy: 0.9748 - val_loss: 0.4409 - val_accuracy: 0.9121\n",
            "Epoch 126/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0677 - accuracy: 0.9758 - val_loss: 0.4468 - val_accuracy: 0.9123\n",
            "Epoch 127/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0682 - accuracy: 0.9756 - val_loss: 0.4455 - val_accuracy: 0.9134\n",
            "Epoch 128/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0681 - accuracy: 0.9755 - val_loss: 0.4475 - val_accuracy: 0.9130\n",
            "Epoch 129/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0685 - accuracy: 0.9755 - val_loss: 0.4452 - val_accuracy: 0.9147\n",
            "Epoch 130/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0673 - accuracy: 0.9759 - val_loss: 0.4491 - val_accuracy: 0.9146\n",
            "Epoch 131/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0666 - accuracy: 0.9762 - val_loss: 0.4432 - val_accuracy: 0.9147\n",
            "Epoch 132/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0663 - accuracy: 0.9762 - val_loss: 0.4545 - val_accuracy: 0.9129\n",
            "Epoch 133/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0663 - accuracy: 0.9763 - val_loss: 0.4474 - val_accuracy: 0.9151\n",
            "Epoch 134/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0662 - accuracy: 0.9763 - val_loss: 0.4513 - val_accuracy: 0.9118\n",
            "Epoch 135/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0665 - accuracy: 0.9762 - val_loss: 0.4502 - val_accuracy: 0.9134\n",
            "Epoch 136/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0668 - accuracy: 0.9759 - val_loss: 0.4544 - val_accuracy: 0.9124\n",
            "Epoch 137/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0653 - accuracy: 0.9765 - val_loss: 0.4521 - val_accuracy: 0.9149\n",
            "Epoch 138/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0638 - accuracy: 0.9773 - val_loss: 0.4445 - val_accuracy: 0.9154\n",
            "Epoch 139/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0647 - accuracy: 0.9770 - val_loss: 0.4545 - val_accuracy: 0.9146\n",
            "Epoch 140/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0642 - accuracy: 0.9771 - val_loss: 0.4592 - val_accuracy: 0.9126\n",
            "Epoch 141/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0636 - accuracy: 0.9772 - val_loss: 0.4432 - val_accuracy: 0.9158\n",
            "Epoch 142/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0618 - accuracy: 0.9780 - val_loss: 0.4464 - val_accuracy: 0.9163\n",
            "Epoch 143/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0626 - accuracy: 0.9776 - val_loss: 0.4550 - val_accuracy: 0.9164\n",
            "Epoch 144/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0644 - accuracy: 0.9769 - val_loss: 0.4511 - val_accuracy: 0.9158\n",
            "Epoch 145/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0649 - accuracy: 0.9766 - val_loss: 0.4530 - val_accuracy: 0.9157\n",
            "Epoch 146/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0605 - accuracy: 0.9784 - val_loss: 0.4571 - val_accuracy: 0.9153\n",
            "Epoch 147/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0618 - accuracy: 0.9779 - val_loss: 0.4585 - val_accuracy: 0.9153\n",
            "Epoch 148/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0611 - accuracy: 0.9782 - val_loss: 0.4598 - val_accuracy: 0.9153\n",
            "Epoch 149/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0604 - accuracy: 0.9784 - val_loss: 0.4554 - val_accuracy: 0.9166\n",
            "Epoch 150/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0618 - accuracy: 0.9779 - val_loss: 0.4543 - val_accuracy: 0.9152\n",
            "Epoch 151/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0606 - accuracy: 0.9786 - val_loss: 0.4610 - val_accuracy: 0.9158\n",
            "Epoch 152/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0605 - accuracy: 0.9785 - val_loss: 0.4579 - val_accuracy: 0.9148\n",
            "Epoch 153/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0609 - accuracy: 0.9782 - val_loss: 0.4617 - val_accuracy: 0.9159\n",
            "Epoch 154/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0610 - accuracy: 0.9781 - val_loss: 0.4642 - val_accuracy: 0.9159\n",
            "Epoch 155/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0597 - accuracy: 0.9787 - val_loss: 0.4586 - val_accuracy: 0.9179\n",
            "Epoch 156/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0581 - accuracy: 0.9793 - val_loss: 0.4695 - val_accuracy: 0.9160\n",
            "Epoch 157/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0592 - accuracy: 0.9789 - val_loss: 0.4639 - val_accuracy: 0.9159\n",
            "Epoch 158/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0586 - accuracy: 0.9791 - val_loss: 0.4645 - val_accuracy: 0.9163\n",
            "Epoch 159/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0584 - accuracy: 0.9791 - val_loss: 0.4658 - val_accuracy: 0.9158\n",
            "Epoch 160/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0600 - accuracy: 0.9785 - val_loss: 0.4572 - val_accuracy: 0.9171\n",
            "Epoch 161/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0568 - accuracy: 0.9797 - val_loss: 0.4687 - val_accuracy: 0.9174\n",
            "Epoch 162/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0580 - accuracy: 0.9793 - val_loss: 0.4668 - val_accuracy: 0.9165\n",
            "Epoch 163/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0591 - accuracy: 0.9789 - val_loss: 0.4681 - val_accuracy: 0.9174\n",
            "Epoch 164/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0569 - accuracy: 0.9798 - val_loss: 0.4676 - val_accuracy: 0.9171\n",
            "Epoch 165/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0570 - accuracy: 0.9797 - val_loss: 0.4681 - val_accuracy: 0.9161\n",
            "Epoch 166/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0567 - accuracy: 0.9799 - val_loss: 0.4659 - val_accuracy: 0.9165\n",
            "Epoch 167/200\n",
            "194/194 [==============================] - 13s 66ms/step - loss: 0.0578 - accuracy: 0.9794 - val_loss: 0.4703 - val_accuracy: 0.9153\n",
            "Epoch 168/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0564 - accuracy: 0.9800 - val_loss: 0.4661 - val_accuracy: 0.9161\n",
            "Epoch 169/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0545 - accuracy: 0.9806 - val_loss: 0.4651 - val_accuracy: 0.9181\n",
            "Epoch 170/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0565 - accuracy: 0.9798 - val_loss: 0.4671 - val_accuracy: 0.9175\n",
            "Epoch 171/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0560 - accuracy: 0.9800 - val_loss: 0.4644 - val_accuracy: 0.9170\n",
            "Epoch 172/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0548 - accuracy: 0.9806 - val_loss: 0.4693 - val_accuracy: 0.9166\n",
            "Epoch 173/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0552 - accuracy: 0.9803 - val_loss: 0.4711 - val_accuracy: 0.9181\n",
            "Epoch 174/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0550 - accuracy: 0.9803 - val_loss: 0.4696 - val_accuracy: 0.9172\n",
            "Epoch 175/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0546 - accuracy: 0.9807 - val_loss: 0.4739 - val_accuracy: 0.9167\n",
            "Epoch 176/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0542 - accuracy: 0.9807 - val_loss: 0.4688 - val_accuracy: 0.9163\n",
            "Epoch 177/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0536 - accuracy: 0.9810 - val_loss: 0.4749 - val_accuracy: 0.9170\n",
            "Epoch 178/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0540 - accuracy: 0.9809 - val_loss: 0.4784 - val_accuracy: 0.9185\n",
            "Epoch 179/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0542 - accuracy: 0.9808 - val_loss: 0.4769 - val_accuracy: 0.9172\n",
            "Epoch 180/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0539 - accuracy: 0.9807 - val_loss: 0.4701 - val_accuracy: 0.9176\n",
            "Epoch 181/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0529 - accuracy: 0.9812 - val_loss: 0.4686 - val_accuracy: 0.9178\n",
            "Epoch 182/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0523 - accuracy: 0.9814 - val_loss: 0.4770 - val_accuracy: 0.9178\n",
            "Epoch 183/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0527 - accuracy: 0.9812 - val_loss: 0.4796 - val_accuracy: 0.9178\n",
            "Epoch 184/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0532 - accuracy: 0.9810 - val_loss: 0.4857 - val_accuracy: 0.9150\n",
            "Epoch 185/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0562 - accuracy: 0.9800 - val_loss: 0.4699 - val_accuracy: 0.9184\n",
            "Epoch 186/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0513 - accuracy: 0.9817 - val_loss: 0.4733 - val_accuracy: 0.9181\n",
            "Epoch 187/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0527 - accuracy: 0.9813 - val_loss: 0.4778 - val_accuracy: 0.9175\n",
            "Epoch 188/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0516 - accuracy: 0.9817 - val_loss: 0.4746 - val_accuracy: 0.9189\n",
            "Epoch 189/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0509 - accuracy: 0.9819 - val_loss: 0.4733 - val_accuracy: 0.9187\n",
            "Epoch 190/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0525 - accuracy: 0.9813 - val_loss: 0.4786 - val_accuracy: 0.9181\n",
            "Epoch 191/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0510 - accuracy: 0.9819 - val_loss: 0.4873 - val_accuracy: 0.9152\n",
            "Epoch 192/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0524 - accuracy: 0.9813 - val_loss: 0.4790 - val_accuracy: 0.9184\n",
            "Epoch 193/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0495 - accuracy: 0.9825 - val_loss: 0.4838 - val_accuracy: 0.9181\n",
            "Epoch 194/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0512 - accuracy: 0.9818 - val_loss: 0.4864 - val_accuracy: 0.9181\n",
            "Epoch 195/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0517 - accuracy: 0.9816 - val_loss: 0.4827 - val_accuracy: 0.9183\n",
            "Epoch 196/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0512 - accuracy: 0.9819 - val_loss: 0.4785 - val_accuracy: 0.9181\n",
            "Epoch 197/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0508 - accuracy: 0.9819 - val_loss: 0.4826 - val_accuracy: 0.9181\n",
            "Epoch 198/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0514 - accuracy: 0.9818 - val_loss: 0.4874 - val_accuracy: 0.9173\n",
            "Epoch 199/200\n",
            "194/194 [==============================] - 12s 63ms/step - loss: 0.0488 - accuracy: 0.9827 - val_loss: 0.4846 - val_accuracy: 0.9182\n",
            "Epoch 200/200\n",
            "194/194 [==============================] - 12s 64ms/step - loss: 0.0497 - accuracy: 0.9825 - val_loss: 0.4824 - val_accuracy: 0.9193\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcb27d3d8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "model.fit(xTrain,yTrain,epochs=200,batch_size=20000,validation_data=(xTest,yTest))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"92_BinaryTest_v04.h5\")"
      ],
      "metadata": {
        "id": "Nu3DTgF_qBLr"
      },
      "execution_count": 31,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "BinaryTest_v03.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}