{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliprg0/Google_Colab/blob/main/NasQ_TV_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "!pip install tvdatafeed\n",
        "from tvDatafeed import TvDatafeed, Interval\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import random \n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from tvDatafeed import TvDatafeed, Interval\n",
        "import multiprocessing\n",
        "import time\n",
        "import glob"
      ],
      "metadata": {
        "id": "BqIra2YIBJsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nasq_syms():\n",
        "  with open(\"watchlist_NASDAQ.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  line = lines[0].split(\",\")\n",
        "  symbols = []\n",
        "  for l in line:\n",
        "    x = l.split(\":\")\n",
        "    symbols.append(x[1])\n",
        "  return symbols\n",
        "def get_binance_syms():\n",
        "  with open(\"binance_tv.txt\",\"r\")as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  nlines = []\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    nlines.append(line.split(\":\")[1])\n",
        "  \n",
        "  return nlines\n",
        "def download_data_p(tv,sym):\n",
        "    data =tv.get_hist(sym,exchange=\"binance\", interval=Interval.in_monthly, n_bars=50000)\n",
        "    try:\n",
        "        data.to_csv(\"data/{}.csv\".format(sym))\n",
        "    except:\n",
        "      pass  \n",
        "def download_data(symbols):\n",
        "  work_with_dir()\n",
        "  tv = TvDatafeed()\n",
        "  for sym in symbols:\n",
        "    print(symbols.index(sym)+1, \"/\", len(symbols))\n",
        "    p = multiprocessing.Process(target=download_data_p, name=\"dd\", args=(tv,sym,))\n",
        "    p.start()\n",
        "    time.sleep(2)\n",
        "    p.terminate()\n",
        "    p.join()"
      ],
      "metadata": {
        "id": "Cku-whunBURi"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_data(get_binance_syms())\n",
        "#download_data(get_nasq_syms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZHam_kdREbf",
        "outputId": "6bea8e27-2e3f-473b-ec2a-db002ec5caeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "you are using nologin method, data you access may be limited\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Folder Removed\n",
            "1 / 574\n",
            "2 / 574\n",
            "3 / 574\n",
            "4 / 574\n",
            "5 / 574\n",
            "6 / 574\n",
            "7 / 574\n",
            "8 / 574\n",
            "9 / 574\n",
            "10 / 574\n",
            "11 / 574\n",
            "12 / 574\n",
            "13 / 574\n",
            "14 / 574\n",
            "15 / 574\n",
            "16 / 574\n",
            "17 / 574\n",
            "18 / 574\n",
            "19 / 574\n",
            "20 / 574\n",
            "21 / 574\n",
            "22 / 574\n",
            "23 / 574\n",
            "24 / 574\n",
            "25 / 574\n",
            "26 / 574\n",
            "27 / 574\n",
            "28 / 574\n",
            "29 / 574\n",
            "30 / 574\n",
            "31 / 574\n",
            "32 / 574\n",
            "33 / 574\n",
            "34 / 574\n",
            "35 / 574\n",
            "36 / 574\n",
            "37 / 574\n",
            "38 / 574\n",
            "39 / 574\n",
            "40 / 574\n",
            "41 / 574\n",
            "42 / 574\n",
            "43 / 574\n",
            "44 / 574\n",
            "45 / 574\n",
            "46 / 574\n",
            "47 / 574\n",
            "48 / 574\n",
            "49 / 574\n",
            "50 / 574\n",
            "51 / 574\n",
            "52 / 574\n",
            "53 / 574\n",
            "54 / 574\n",
            "55 / 574\n",
            "56 / 574\n",
            "57 / 574\n",
            "58 / 574\n",
            "59 / 574\n",
            "60 / 574\n",
            "61 / 574\n",
            "62 / 574\n",
            "63 / 574\n",
            "64 / 574\n",
            "65 / 574\n",
            "66 / 574\n",
            "67 / 574\n",
            "68 / 574\n",
            "69 / 574\n",
            "70 / 574\n",
            "71 / 574\n",
            "72 / 574\n",
            "73 / 574\n",
            "74 / 574\n",
            "75 / 574\n",
            "76 / 574\n",
            "77 / 574\n",
            "78 / 574\n",
            "79 / 574\n",
            "80 / 574\n",
            "81 / 574\n",
            "82 / 574\n",
            "83 / 574\n",
            "84 / 574\n",
            "85 / 574\n",
            "86 / 574\n",
            "87 / 574\n",
            "88 / 574\n",
            "89 / 574\n",
            "90 / 574\n",
            "91 / 574\n",
            "92 / 574\n",
            "93 / 574\n",
            "94 / 574\n",
            "95 / 574\n",
            "96 / 574\n",
            "97 / 574\n",
            "98 / 574\n",
            "99 / 574\n",
            "100 / 574\n",
            "101 / 574\n",
            "102 / 574\n",
            "103 / 574\n",
            "104 / 574\n",
            "105 / 574\n",
            "106 / 574\n",
            "107 / 574\n",
            "108 / 574\n",
            "109 / 574\n",
            "110 / 574\n",
            "111 / 574\n",
            "112 / 574\n",
            "113 / 574\n",
            "114 / 574\n",
            "115 / 574\n",
            "116 / 574\n",
            "117 / 574\n",
            "118 / 574\n",
            "119 / 574\n",
            "120 / 574\n",
            "121 / 574\n",
            "122 / 574\n",
            "123 / 574\n",
            "124 / 574\n",
            "125 / 574\n",
            "126 / 574\n",
            "127 / 574\n",
            "128 / 574\n",
            "129 / 574\n",
            "130 / 574\n",
            "131 / 574\n",
            "132 / 574\n",
            "133 / 574\n",
            "134 / 574\n",
            "135 / 574\n",
            "136 / 574\n",
            "137 / 574\n",
            "138 / 574\n",
            "139 / 574\n",
            "140 / 574\n",
            "141 / 574\n",
            "142 / 574\n",
            "143 / 574\n",
            "144 / 574\n",
            "145 / 574\n",
            "146 / 574\n",
            "147 / 574\n",
            "148 / 574\n",
            "149 / 574\n",
            "150 / 574\n",
            "151 / 574\n",
            "152 / 574\n",
            "153 / 574\n",
            "154 / 574\n",
            "155 / 574\n",
            "156 / 574\n",
            "157 / 574\n",
            "158 / 574\n",
            "159 / 574\n",
            "160 / 574\n",
            "161 / 574\n",
            "162 / 574\n",
            "163 / 574\n",
            "164 / 574\n",
            "165 / 574\n",
            "166 / 574\n",
            "167 / 574\n",
            "168 / 574\n",
            "169 / 574\n",
            "170 / 574\n",
            "171 / 574\n",
            "172 / 574\n",
            "173 / 574\n",
            "174 / 574\n",
            "175 / 574\n",
            "176 / 574\n",
            "177 / 574\n",
            "178 / 574\n",
            "179 / 574\n",
            "180 / 574\n",
            "181 / 574\n",
            "182 / 574\n",
            "183 / 574\n",
            "184 / 574\n",
            "185 / 574\n",
            "186 / 574\n",
            "187 / 574\n",
            "188 / 574\n",
            "189 / 574\n",
            "190 / 574\n",
            "191 / 574\n",
            "192 / 574\n",
            "193 / 574\n",
            "194 / 574\n",
            "195 / 574\n",
            "196 / 574\n",
            "197 / 574\n",
            "198 / 574\n",
            "199 / 574\n",
            "200 / 574\n",
            "201 / 574\n",
            "202 / 574\n",
            "203 / 574\n",
            "204 / 574\n",
            "205 / 574\n",
            "206 / 574\n",
            "207 / 574\n",
            "208 / 574\n",
            "209 / 574\n",
            "210 / 574\n",
            "211 / 574\n",
            "212 / 574\n",
            "213 / 574\n",
            "214 / 574\n",
            "215 / 574\n",
            "216 / 574\n",
            "217 / 574\n",
            "218 / 574\n",
            "219 / 574\n",
            "220 / 574\n",
            "221 / 574\n",
            "222 / 574\n",
            "223 / 574\n",
            "224 / 574\n",
            "225 / 574\n",
            "226 / 574\n",
            "227 / 574\n",
            "228 / 574\n",
            "229 / 574\n",
            "230 / 574\n",
            "231 / 574\n",
            "232 / 574\n",
            "233 / 574\n",
            "234 / 574\n",
            "235 / 574\n",
            "236 / 574\n",
            "237 / 574\n",
            "238 / 574\n",
            "239 / 574\n",
            "240 / 574\n",
            "241 / 574\n",
            "242 / 574\n",
            "243 / 574\n",
            "244 / 574\n",
            "245 / 574\n",
            "246 / 574\n",
            "247 / 574\n",
            "248 / 574\n",
            "249 / 574\n",
            "250 / 574\n",
            "251 / 574\n",
            "252 / 574\n",
            "253 / 574\n",
            "254 / 574\n",
            "255 / 574\n",
            "256 / 574\n",
            "257 / 574\n",
            "258 / 574\n",
            "259 / 574\n",
            "260 / 574\n",
            "261 / 574\n",
            "262 / 574\n",
            "263 / 574\n",
            "264 / 574\n",
            "265 / 574\n",
            "266 / 574\n",
            "267 / 574\n",
            "268 / 574\n",
            "269 / 574\n",
            "270 / 574\n",
            "271 / 574\n",
            "272 / 574\n",
            "273 / 574\n",
            "274 / 574\n",
            "275 / 574\n",
            "276 / 574\n",
            "277 / 574\n",
            "278 / 574\n",
            "279 / 574\n",
            "280 / 574\n",
            "281 / 574\n",
            "282 / 574\n",
            "283 / 574\n",
            "284 / 574\n",
            "285 / 574\n",
            "286 / 574\n",
            "287 / 574\n",
            "288 / 574\n",
            "289 / 574\n",
            "290 / 574\n",
            "291 / 574\n",
            "292 / 574\n",
            "293 / 574\n",
            "294 / 574\n",
            "295 / 574\n",
            "296 / 574\n",
            "297 / 574\n",
            "298 / 574\n",
            "299 / 574\n",
            "300 / 574\n",
            "301 / 574\n",
            "302 / 574\n",
            "303 / 574\n",
            "304 / 574\n",
            "305 / 574\n",
            "306 / 574\n",
            "307 / 574\n",
            "308 / 574\n",
            "309 / 574\n",
            "310 / 574\n",
            "311 / 574\n",
            "312 / 574\n",
            "313 / 574\n",
            "314 / 574\n",
            "315 / 574\n",
            "316 / 574\n",
            "317 / 574\n",
            "318 / 574\n",
            "319 / 574\n",
            "320 / 574\n",
            "321 / 574\n",
            "322 / 574\n",
            "323 / 574\n",
            "324 / 574\n",
            "325 / 574\n",
            "326 / 574\n",
            "327 / 574\n",
            "328 / 574\n",
            "329 / 574\n",
            "330 / 574\n",
            "331 / 574\n",
            "332 / 574\n",
            "333 / 574\n",
            "334 / 574\n",
            "335 / 574\n",
            "336 / 574\n",
            "337 / 574\n",
            "338 / 574\n",
            "339 / 574\n",
            "340 / 574\n",
            "341 / 574\n",
            "342 / 574\n",
            "343 / 574\n",
            "344 / 574\n",
            "345 / 574\n",
            "346 / 574\n",
            "347 / 574\n",
            "348 / 574\n",
            "349 / 574\n",
            "350 / 574\n",
            "351 / 574\n",
            "352 / 574\n",
            "353 / 574\n",
            "354 / 574\n",
            "355 / 574\n",
            "356 / 574\n",
            "357 / 574\n",
            "358 / 574\n",
            "359 / 574\n",
            "360 / 574\n",
            "361 / 574\n",
            "362 / 574\n",
            "363 / 574\n",
            "364 / 574\n",
            "365 / 574\n",
            "366 / 574\n",
            "367 / 574\n",
            "368 / 574\n",
            "369 / 574\n",
            "370 / 574\n",
            "371 / 574\n",
            "372 / 574\n",
            "373 / 574\n",
            "374 / 574\n",
            "375 / 574\n",
            "376 / 574\n",
            "377 / 574\n",
            "378 / 574\n",
            "379 / 574\n",
            "380 / 574\n",
            "381 / 574\n",
            "382 / 574\n",
            "383 / 574\n",
            "384 / 574\n",
            "385 / 574\n",
            "386 / 574\n",
            "387 / 574\n",
            "388 / 574\n",
            "389 / 574\n",
            "390 / 574\n",
            "391 / 574\n",
            "392 / 574\n",
            "393 / 574\n",
            "394 / 574\n",
            "395 / 574\n",
            "396 / 574\n",
            "397 / 574\n",
            "398 / 574\n",
            "399 / 574\n",
            "400 / 574\n",
            "401 / 574\n",
            "402 / 574\n",
            "403 / 574\n",
            "404 / 574\n",
            "405 / 574\n",
            "406 / 574\n",
            "407 / 574\n",
            "408 / 574\n",
            "409 / 574\n",
            "410 / 574\n",
            "411 / 574\n",
            "412 / 574\n",
            "413 / 574\n",
            "414 / 574\n",
            "415 / 574\n",
            "416 / 574\n",
            "417 / 574\n",
            "418 / 574\n",
            "419 / 574\n",
            "420 / 574\n",
            "421 / 574\n",
            "422 / 574\n",
            "423 / 574\n",
            "424 / 574\n",
            "425 / 574\n",
            "426 / 574\n",
            "427 / 574\n",
            "428 / 574\n",
            "429 / 574\n",
            "430 / 574\n",
            "431 / 574\n",
            "432 / 574\n",
            "433 / 574\n",
            "434 / 574\n",
            "435 / 574\n",
            "436 / 574\n",
            "437 / 574\n",
            "438 / 574\n",
            "439 / 574\n",
            "440 / 574\n",
            "441 / 574\n",
            "442 / 574\n",
            "443 / 574\n",
            "444 / 574\n",
            "445 / 574\n",
            "446 / 574\n",
            "447 / 574\n",
            "448 / 574\n",
            "449 / 574\n",
            "450 / 574\n",
            "451 / 574\n",
            "452 / 574\n",
            "453 / 574\n",
            "454 / 574\n",
            "455 / 574\n",
            "456 / 574\n",
            "457 / 574\n",
            "458 / 574\n",
            "459 / 574\n",
            "460 / 574\n",
            "461 / 574\n",
            "462 / 574\n",
            "463 / 574\n",
            "464 / 574\n",
            "465 / 574\n",
            "466 / 574\n",
            "467 / 574\n",
            "468 / 574\n",
            "469 / 574\n",
            "470 / 574\n",
            "471 / 574\n",
            "472 / 574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clmns = [\n",
        "    \"Close 1-2\", \"Close 2-3\", \"Close 3-4\", \"Close 4-5\", \"Close 5-6\", \"Close 6-7\", \"Close 7-8\",\n",
        "    \"High 1-2\", \"High 2-3\", \"High 3-4\", \"High 4-5\", \"High 5-6\", \"High 6-7\", \"High 7-8\", \"High 8-9\", \"High 9-10\", \"High 10-11\", \"High 11-12\", \"High 12-13\", \"High 13-14\", \"High 14-15\", \"High 15-16\",\n",
        "    \"Low 1-2\", \"Low 2-3\", \n",
        "    \"suggestion\"]\n",
        "def work_with_dir():\n",
        "  if os.path.exists(\"/content/data/\"):\n",
        "    shutil.rmtree(\"/content/data/\", ignore_errors=True)\n",
        "    print(\"Data Folder Removed\")\n",
        "    os.mkdir(\"/content/data/\")\n",
        "\n",
        "  if not os.path.exists(\"/content/data/\"):\n",
        "    os.mkdir(\"/content/data/\")\n",
        "  \n",
        "  if not os.path.exists(\"/content/extracted/\"):\n",
        "    os.mkdir(\"/content/extracted/\")\n",
        "def read_txt_list():\n",
        "  with open(\"yahoo_stocklist.txt\",\"r\")as f:\n",
        "    lines = f.readlines()\n",
        "    nlines = []\n",
        "    for line in lines:\n",
        "       nlines.append(line.strip())\n",
        "    return nlines\n",
        "def read_syms_from_txt():\n",
        "  with open(\"syms.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  lst = []\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    lst.append(line)\n",
        "  symbols = lst\n",
        "  return symbols\n",
        "def get_crypto_syms():\n",
        "   # 'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in',\n",
        "   screens = [\n",
        "       'all_cryptocurrencies_us', 'all_cryptocurrencies_au', 'all_cryptocurrencies_ca', 'all_cryptocurrencies_eu', 'all_cryptocurrencies_gb', 'all_cryptocurrencies_in', ]\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   # print(len(symbols))\n",
        "   # pieces = 15\n",
        "   # new_arrays = np.array_split(symbols, pieces)\n",
        "   return symbols\n",
        "def spliting(data):\n",
        "  X = data.drop([\"yes\",\"no\"], axis=1)\n",
        "  y = data[[\"yes\",\"no\"]]\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.2)\n",
        "  print(xTrain.shape, end=\" \")\n",
        "  print(yTrain.shape)\n",
        "  print(xTest.shape, end=\" \")\n",
        "  print(yTest.shape)\n",
        "  return xTrain, xTest, yTrain, yTest\n",
        "def scaler(row):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    row = scaler.fit_transform(row)\n",
        "    return row\n",
        "def process(data):\n",
        "    data = data.dropna()\n",
        "    row = []\n",
        "    if \"symbol\" in data.columns:\n",
        "        data.drop(\"symbol\", axis=1, inplace=True)  \n",
        "    if \"Adj Close\" in data.columns:\n",
        "        data.drop(\"Adj Close\", axis=1, inplace=True)\n",
        "    if \"datetime\" in data.columns:\n",
        "        data.drop(\"datetime\",axis=1,inplace=True)\n",
        "    if len(pd.DataFrame(data).columns) == 7:\n",
        "      data = data.iloc[:, 1:]\n",
        "    data = np.array(data)  \n",
        "    llst = [0, 1, 2, 3, 4]\n",
        "\n",
        "    for i in range(15, data.shape[0]):\n",
        "        grow = []\n",
        "      \n",
        "        s12 = [ \n",
        "            data[i-12][0], data[i-12][1], data[i-12][2], data[i-12][3]\n",
        "        ]\n",
        "        s11 = [\n",
        "            data[i-11][0], data[i-11][1], data[i-11][2], data[i-11][3]\n",
        "        ]\n",
        "        s10 = [\n",
        "            data[i-10][0], data[i-10][1], data[i-10][2], data[i-10][3]\n",
        "        ]\n",
        "        s9 = [\n",
        "            data[i-9][0], data[i-9][1], data[i-9][2], data[i-9][3]\n",
        "        ]\n",
        "        s8 = [\n",
        "            data[i-8][0], data[i-8][1], data[i-8][2], data[i-8][3]\n",
        "        ]\n",
        "        s7 = [\n",
        "            data[i-7][0], data[i-7][1], data[i-7][2], data[i-7][3]\n",
        "        ]\n",
        "        s6 = [\n",
        "            data[i-6][0], data[i-6][1], data[i-6][2], data[i-6][3]\n",
        "        ]\n",
        "        s5 = [\n",
        "            data[i-5][0], data[i-5][1], data[i-5][2], data[i-5][3]\n",
        "        ]   \n",
        "        s4 = [\n",
        "            data[i-4][0], data[i-4][1], data[i-4][2], data[i-4][3]\n",
        "        ]\n",
        "        s3 = [\n",
        "            data[i-3][0], data[i-3][1], data[i-3][2], data[i-3][3]\n",
        "        ]\n",
        "        s2 = [\n",
        "            data[i-2][0], data[i-2][1], data[i-2][2], data[i-2][3]\n",
        "        ]\n",
        "        s1 = [\n",
        "            data[i-1][0], data[i-1][1], data[i-1][2], data[i-1][3]\n",
        "        ]\n",
        "        \n",
        "\n",
        "        s1 = scaler(np.array(s1).reshape(-1,1))\n",
        "        s2 = scaler(np.array(s2).reshape(-1,1))\n",
        "        s3 = scaler(np.array(s3).reshape(-1,1))\n",
        "        s4 = scaler(np.array(s4).reshape(-1,1))\n",
        "        s5 = scaler(np.array(s5).reshape(-1,1))\n",
        "        s6 = scaler(np.array(s6).reshape(-1,1))\n",
        "        s7 = scaler(np.array(s7).reshape(-1,1))\n",
        "        s8 = scaler(np.array(s8).reshape(-1,1))\n",
        "        s9 = scaler(np.array(s9).reshape(-1,1))\n",
        "        s10 = scaler(np.array(s10).reshape(-1,1))\n",
        "        s11 = scaler(np.array(s11).reshape(-1,1))\n",
        "        s12 = scaler(np.array(s12).reshape(-1,1))\n",
        " \n",
        "        s1 = np.delete(s1, 1)\n",
        "        s1 = np.delete(s1, 1)\n",
        "        s2 = np.delete(s2, 1)\n",
        "        s2 = np.delete(s2, 1)\n",
        "        s3 = np.delete(s3, 1)\n",
        "        s3 = np.delete(s3, 1)\n",
        "        s4 = np.delete(s4, 1)\n",
        "        s4 = np.delete(s4, 1)\n",
        "        s5 = np.delete(s5, 1)\n",
        "        s5 = np.delete(s5, 1)\n",
        "        s6 = np.delete(s6, 1)\n",
        "        s6 = np.delete(s6, 1)\n",
        "        s7 = np.delete(s7, 1)\n",
        "        s7 = np.delete(s7, 1)\n",
        "        s8 = np.delete(s8, 1)\n",
        "        s8 = np.delete(s8, 1)\n",
        "        s9 = np.delete(s9, 1)\n",
        "        s9 = np.delete(s9, 1)\n",
        "        s10 = np.delete(s10, 1)\n",
        "        s10 = np.delete(s10, 1)\n",
        "        s11 = np.delete(s11, 1)\n",
        "        s11 = np.delete(s11, 1)\n",
        "        s12 = np.delete(s12, 1)\n",
        "        s12 = np.delete(s12, 1)     \n",
        "\n",
        "        sugg = \"no\"\n",
        "        if data[i][3] > data[i][0]:\n",
        "            sugg = \"yes\"\n",
        "\n",
        "        grow = np.vstack((s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12))\n",
        "\n",
        "        arr = np.array(grow).flatten()\n",
        "        arr = np.append(arr, sugg)\n",
        "        row.append(arr)\n",
        "\n",
        "\n",
        "    grow = []\n",
        "    srow = []\n",
        "    llst = []\n",
        "    data = []\n",
        "    arr = []\n",
        "    mm = []\n",
        "\n",
        "    return np.array(row)"
      ],
      "metadata": {
        "id": "Yc7-yg1RMYBQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def each_file_proc(files,now,index):\n",
        "     data = []\n",
        "     unattached_dfs = []\n",
        "     files = list(files)\n",
        "     for file in files:\n",
        "        print(f\"{files.index(file)+1+index}\",end=\" \")\n",
        "        if (files.index(file)+index+1) % 40 == 0:\n",
        "          print(\" \")\n",
        "        address = f\"/content/data/{file}\"\n",
        "        unattached_dfs.append(process(pd.read_csv(address)))\n",
        "     data = np.array(unattached_dfs[0])\n",
        "     for i in unattached_dfs[1:]:\n",
        "           data = np.append(data, np.array(i), axis=0)\n",
        "\n",
        "     unattached_dfs = []\n",
        "     \n",
        "     data = pd.DataFrame(data, columns=clmns)\n",
        " \n",
        "\n",
        "     sugg = data[\"suggestion\"]\n",
        "     data.drop(\"suggestion\",axis=1,inplace=True)\n",
        "     sugg = pd.get_dummies(sugg)\n",
        "     data = pd.concat([data,sugg],axis=1)\n",
        "     data = data.astype(float)\n",
        "     right_now = datetime.now().strftime(\"%H%M%S%f\")\n",
        "     data.to_csv(f\"/content/extracted/{now}/{right_now}.csv\")  \n",
        "def extract_data(pieces):\n",
        "  pd.options.mode.chained_assignment = None\n",
        "  print(f\"Files In Data : {len(os.listdir('/content/data/'))}\")\n",
        "  files = os.listdir(\"/content/data/\")\n",
        "  new_files = np.array_split(files, pieces)\n",
        "  print(\"Processing File:\")\n",
        "  now = datetime.now().strftime(\"%H%M%S\")\n",
        "  os.mkdir(f\"/content/extracted/{now}/\")\n",
        "  print(\"Done\")\n",
        "  index = 0 \n",
        "  for files in new_files:\n",
        "     \n",
        "     each_file_proc(files,now,index)\n",
        "     index = index + len(files)\n",
        "  print(\" \")\n",
        "  return now\n",
        "def delete_all_csv(now):\n",
        "   path = f'/content/extracted/{now}/*.csv'\n",
        "   files = glob.glob(path)\n",
        "   for file in files:\n",
        "       os.remove(file)\n",
        "def make_df(now):\n",
        "   path = f'/content/extracted/{now}/*.parquet'\n",
        "   files = glob.glob(path)\n",
        "   #data = pd.DataFrame()\n",
        "   data = pd.DataFrame()\n",
        "   for adr in files:\n",
        "     data =pd.concat([data,pd.read_parquet(adr)])\n",
        "   if \"Unnamed: 0\" in data:\n",
        "     data.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
        "   print(data.shape)\n",
        "   xTrain,xTest,yTrain,yTest = spliting(data)\n",
        "   data.to_parquet(f'/content/extracted/{now}/data.parquet')\n",
        "   delete_all_csv(now)\n",
        "   data = []\n",
        "   return xTrain,xTest,yTrain,yTest\n",
        "def to_par(now,howmanyfiles): \n",
        "    files = os.listdir(f\"/content/extracted/{now}/\")\n",
        "    addresses = []\n",
        "    for file in files:\n",
        "      addresses.append(f\"/content/extracted/{now}/{file}\")\n",
        "    new_adr = np.array_split(addresses,howmanyfiles)\n",
        "    for adrs in new_adr:\n",
        "      datas = []\n",
        "      for adr in adrs:\n",
        "        datas.append(pd.read_csv(adr))\n",
        "      rnow = datetime.now().strftime(\"%H%M%S%f\")\n",
        "      datas = pd.concat(datas)\n",
        "      datas.to_parquet(f\"/content/extracted/{now}/part_{rnow}.parquet\")     "
      ],
      "metadata": {
        "id": "RwcfzqyPMq27"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_name = extract_data(25)\n",
        "to_par(folder_name,5)\n",
        "xTrain,xTest,yTrain,yTest = make_df(folder_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nacRKx3ZMy2X",
        "outputId": "a63eb688-5b25-4f60-b871-1a46bb4c26ed"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files In Data : 95\n",
            "Processing File:\n",
            "Done\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  \n",
            "41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80  \n",
            "81 82 83 84 85 86 87 88 89 90 91 92 93 94 95  \n",
            "(3169, 26)\n",
            "(2535, 24) (2535, 2)\n",
            "(634, 24) (634, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.add(Dense(512, activation='relu', input_shape=(xTrain.shape[1],)))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adamax()\n",
        "\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5asREmlN2U-",
        "outputId": "303dc925-438d-4441-8522-3a75f860b273"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_32 (Dense)            (None, 512)               12800     \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,852,418\n",
            "Trainable params: 1,852,418\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(xTrain,yTrain,epochs=50,batch_size=32,validation_data=(xTest,yTest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pjwc42r1N5yA",
        "outputId": "61980af7-6a78-4474-e0eb-dc039825d4a0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "80/80 [==============================] - 2s 19ms/step - loss: 0.6677 - accuracy: 0.5909 - val_loss: 0.6655 - val_accuracy: 0.6088\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.6120 - accuracy: 0.6655 - val_loss: 0.6156 - val_accuracy: 0.6451\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.5610 - accuracy: 0.7022 - val_loss: 0.5619 - val_accuracy: 0.7019\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.5236 - accuracy: 0.7357 - val_loss: 0.5665 - val_accuracy: 0.6987\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.4839 - accuracy: 0.7519 - val_loss: 0.5554 - val_accuracy: 0.7271\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.4500 - accuracy: 0.7811 - val_loss: 0.5574 - val_accuracy: 0.7240\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.3986 - accuracy: 0.8178 - val_loss: 0.5397 - val_accuracy: 0.7397\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.3644 - accuracy: 0.8398 - val_loss: 0.5790 - val_accuracy: 0.7240\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.3008 - accuracy: 0.8761 - val_loss: 0.5608 - val_accuracy: 0.7303\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.2489 - accuracy: 0.8970 - val_loss: 0.7248 - val_accuracy: 0.7334\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.2020 - accuracy: 0.9223 - val_loss: 0.7373 - val_accuracy: 0.7256\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.1695 - accuracy: 0.9349 - val_loss: 0.7477 - val_accuracy: 0.7240\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.1335 - accuracy: 0.9511 - val_loss: 0.9909 - val_accuracy: 0.7082\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0872 - accuracy: 0.9680 - val_loss: 1.2184 - val_accuracy: 0.7050\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0798 - accuracy: 0.9680 - val_loss: 1.1652 - val_accuracy: 0.7035\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0636 - accuracy: 0.9736 - val_loss: 1.2943 - val_accuracy: 0.7366\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0480 - accuracy: 0.9838 - val_loss: 1.2817 - val_accuracy: 0.7192\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0301 - accuracy: 0.9897 - val_loss: 1.5912 - val_accuracy: 0.7287\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0441 - accuracy: 0.9838 - val_loss: 1.5791 - val_accuracy: 0.7177\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0582 - accuracy: 0.9807 - val_loss: 1.3006 - val_accuracy: 0.7208\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0201 - accuracy: 0.9937 - val_loss: 1.6872 - val_accuracy: 0.7066\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0253 - accuracy: 0.9913 - val_loss: 1.9212 - val_accuracy: 0.6877\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0281 - accuracy: 0.9913 - val_loss: 1.6442 - val_accuracy: 0.7177\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0177 - accuracy: 0.9945 - val_loss: 1.8034 - val_accuracy: 0.7224\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0141 - accuracy: 0.9941 - val_loss: 2.0193 - val_accuracy: 0.7240\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0341 - accuracy: 0.9870 - val_loss: 1.5365 - val_accuracy: 0.7240\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0165 - accuracy: 0.9945 - val_loss: 1.7674 - val_accuracy: 0.6956\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.0499 - val_accuracy: 0.7271\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0035 - accuracy: 0.9984 - val_loss: 2.0241 - val_accuracy: 0.7271\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 4.7002e-04 - accuracy: 1.0000 - val_loss: 2.2481 - val_accuracy: 0.7256\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 2.0972e-04 - accuracy: 1.0000 - val_loss: 2.3741 - val_accuracy: 0.7208\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 1.1699e-04 - accuracy: 1.0000 - val_loss: 2.4722 - val_accuracy: 0.7256\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 7.8985e-05 - accuracy: 1.0000 - val_loss: 2.5720 - val_accuracy: 0.7271\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 5.7674e-05 - accuracy: 1.0000 - val_loss: 2.6589 - val_accuracy: 0.7287\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 4.2847e-05 - accuracy: 1.0000 - val_loss: 2.7485 - val_accuracy: 0.7303\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 3.1628e-05 - accuracy: 1.0000 - val_loss: 2.8268 - val_accuracy: 0.7303\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 2.4993e-05 - accuracy: 1.0000 - val_loss: 2.8936 - val_accuracy: 0.7303\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 1.9789e-05 - accuracy: 1.0000 - val_loss: 2.9660 - val_accuracy: 0.7303\n",
            "Epoch 39/50\n",
            "61/80 [=====================>........] - ETA: 0s - loss: 1.2124e-05 - accuracy: 1.0000"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-4fa873b8d183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myTrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "NasQ_TV_15.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}