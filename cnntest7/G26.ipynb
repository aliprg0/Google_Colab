{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_5bxbCoe9do9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f088a5-a4dd-416e-b345-6a9f14269abc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.74-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 10.4 MB/s \n",
            "\u001b[?25hCollecting requests>=2.26\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.9.1 requests-2.28.1 yfinance-0.1.74\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yahooquery\n",
            "  Downloading yahooquery-2.2.15-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.6.2 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (4.9.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.7/dist-packages (from yahooquery) (4.64.0)\n",
            "Collecting requests-futures>=1.0.0\n",
            "  Downloading requests_futures-1.0.0-py2.py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yahooquery) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yahooquery) (1.15.0)\n",
            "Requirement already satisfied: requests>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from requests-futures>=1.0.0->yahooquery) (2.28.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=1.2.0->requests-futures>=1.0.0->yahooquery) (2.1.0)\n",
            "Installing collected packages: requests-futures, yahooquery\n",
            "Successfully installed requests-futures-1.0.0 yahooquery-2.2.15\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tvdatafeed\n",
            "  Downloading tvdatafeed-1.2.1-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from tvdatafeed) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tvdatafeed) (57.4.0)\n",
            "Collecting chromedriver-autoinstaller\n",
            "  Downloading chromedriver_autoinstaller-0.3.1-py3-none-any.whl (6.3 kB)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting websocket-client\n",
            "  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tvdatafeed) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tvdatafeed) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tvdatafeed) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->tvdatafeed) (1.15.0)\n",
            "Collecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 48.6 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium->tvdatafeed) (21.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium->tvdatafeed) (2.4.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium->tvdatafeed) (2.10)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium->tvdatafeed) (2022.6.15)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium->tvdatafeed) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium->tvdatafeed) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium->tvdatafeed) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium->tvdatafeed) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, websocket-client, selenium, chromedriver-autoinstaller, tvdatafeed\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 chromedriver-autoinstaller-0.3.1 cryptography-37.0.4 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.3.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 tvdatafeed-1.2.1 urllib3-1.26.11 websocket-client-1.3.3 wsproto-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.47.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.28.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.11)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mplfinance\n",
            "  Downloading mplfinance-0.12.9b1-py3-none-any.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mplfinance) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mplfinance) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mplfinance) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mplfinance) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mplfinance) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mplfinance) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mplfinance) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mplfinance) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mplfinance) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->mplfinance) (2022.1)\n",
            "Installing collected packages: mplfinance\n",
            "Successfully installed mplfinance-0.12.9b1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cairocffi\n",
            "  Downloading cairocffi-1.3.0.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cairocffi) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1.0->cairocffi) (2.21)\n",
            "Building wheels for collected packages: cairocffi\n",
            "  Building wheel for cairocffi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cairocffi: filename=cairocffi-1.3.0-py3-none-any.whl size=89668 sha256=8b9079fcea8555c44e4bbca3fca81f3e82b5646c3eb959ab0e1f090797f60be4\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/e1/5c8a9692a27f639a07c949044bec943f26c81cd53d3805319f\n",
            "Successfully built cairocffi\n",
            "Installing collected packages: cairocffi\n",
            "Successfully installed cairocffi-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "!pip install tvdatafeed\n",
        "!pip install tensorflow\n",
        "!pip install mplfinance\n",
        "!pip install cairocffi\n",
        "from tvDatafeed import TvDatafeed, Interval\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import random \n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "import shutil\n",
        "import mplfinance as mpl \n",
        "from datetime import datetime\n",
        "import glob\n",
        "from PIL import Image\n",
        "import cv2\n",
        "%matplotlib notebook\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense,AveragePooling2D,GlobalAveragePooling2D\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IB_YMoe09qVP"
      },
      "outputs": [],
      "source": [
        "def work_with_dir():\n",
        "  if os.path.exists(\"/content/data/\"):\n",
        "    shutil.rmtree(\"/content/data/\", ignore_errors=True)\n",
        "    print(\"Data Folder Removed\")\n",
        "    os.mkdir(\"/content/data/\")\n",
        "  if not os.path.exists(\"/content/data/\"):\n",
        "    os.mkdir(\"/content/data/\")\n",
        "  if not os.path.exists(\"/content/extracted/\"):\n",
        "    os.mkdir(\"/content/extracted/\")\n",
        "  if not os.path.exists(\"/content/checkpoints/\"):\n",
        "    os.mkdir(\"/content/checkpoints/\")\n",
        "def get_crypto_syms():\n",
        "   screens = [\n",
        "       'all_cryptocurrencies_us', 'all_cryptocurrencies_au', 'all_cryptocurrencies_ca', 'all_cryptocurrencies_eu', 'all_cryptocurrencies_gb', 'all_cryptocurrencies_in', ]\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   return symbols\n",
        "def download_data(symbols, periodd, intervall):\n",
        "  indexx = 100\n",
        "  work_with_dir()\n",
        "  for symbol in symbols:\n",
        "    if ((symbols.index(symbol)+1) % 100 == 0):\n",
        "      print(f\" -- {indexx}\", end=\"\")\n",
        "      indexx = indexx + 100\n",
        "    try:\n",
        "        data = yf.download(symbol, period=periodd,\n",
        "                           interval=intervall, progress=False, show_errors=False)\n",
        "        if data.empty:\n",
        "           pass\n",
        "        else:\n",
        "            data.to_csv(f\"/content/data/{symbol}.csv\")\n",
        "    except:\n",
        "       print(\"Error!\")\n",
        "  print(\" \")\n",
        "def extract_data(how_many_future_candles, how_many_past_candles, each_row_past):\n",
        "  print(f\"Files In Data : {len(os.listdir('/content/data/'))}\")\n",
        "  pd.options.mode.chained_assignment = None\n",
        "  files = os.listdir(\"/content/data/\")\n",
        "  print(\"Processing File:\")\n",
        "  now = datetime.now().strftime(\"%H%M%S\")\n",
        "  os.mkdir(f\"/content/extracted/{now}/\")\n",
        "  index = 1\n",
        "  for file in files:\n",
        "     print(f\"File Number {index}:\", end=\" \")\n",
        "     each_file_proc(file, now, how_many_future_candles,\n",
        "                    how_many_past_candles, each_row_past)\n",
        "     index = index + 1\n",
        "  print(\" \")\n",
        "  return now\n",
        "def each_file_proc(file, now, how_many_future_candles, how_many_past_candles, each_row_past):\n",
        "    address = f\"/content/data/{file}\"\n",
        "    data = pd.read_csv(address)\n",
        "    if len(data.columns) == 7:\n",
        "      data = data.iloc[:, 1:]\n",
        "    data = np.array(data)\n",
        "    data = data.astype(float)\n",
        "    max_index = data.shape[0]-which_future_or_past\n",
        "    for i in range(each_row_past,max_index):\n",
        "        rows = data[i-each_row_past:i, :]\n",
        "\n",
        "        next_candles = []\n",
        "        for z in range(0, how_many_future_candles):\n",
        "          next_candles.append(data[i+z][3]-data[i+z][0])\n",
        "        next_candles = sum(next_candles)\n",
        "        if next_candles > 0:\n",
        "          sugg = 1\n",
        "        else:\n",
        "          sugg = 0\n",
        "\n",
        "        df = pd.DataFrame(rows, columns=[\"Open\", \"High\", \"Low\", \"Close\",\"Adj Close\",\"Volume\"])\n",
        "\n",
        "        df.index.name = \"Date\"\n",
        "\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "        right_now = datetime.now().strftime(\"%H%M%S%f\")\n",
        "        address = f\"/content/extracted/{now}/{right_now}_{sugg}.png\"\n",
        "        \n",
        "\n",
        "        fig, _a = mpl.plot(df, type=\"candle\", style=\"yahoo\", axisoff=True,\n",
        "                            returnfig=True, tight_layout=True,figsize =(1.5,1.5))\n",
        "        \n",
        "        fig.savefig(address)\n",
        "        fig.clf()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"{i}/{max_index}\", end=\" \")\n",
        "        if i % 20:\n",
        "          plt.close(\"all\")\n",
        "        if i % 270 ==0:\n",
        "          print(\"\")\n",
        "    plt.close(\"all\")\n",
        "    print(\"\")\n",
        "\n",
        "def start(how_many_future_candles, how_many_past_candles, each_row_past):\n",
        "    folder_name = extract_data(\n",
        "        how_many_future_candles, how_many_past_candles, each_row_past)\n",
        "    return folder_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMR8z1BIS-M_",
        "outputId": "106901eb-9b00-49ff-f723-ff7f872beb04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbols : 1500\n",
            "Data Folder Removed\n",
            " \n"
          ]
        }
      ],
      "source": [
        "symbols = get_crypto_syms()\n",
        "print(f\"Symbols : {len(symbols)}\")\n",
        "symbols = [\"btc-usd\",\"eth-usd\",\"trx-usd\",\"ltc-usd\",\"xrp-usd\",\"bnb-usd\"]\n",
        "download_data(symbols,\"max\",\"1d\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxTyv_osQAnY",
        "outputId": "d22ba8be-da75-47c1-d98d-52b61fab6b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files In Data : 6\n",
            "Processing File:\n",
            "File Number 1: 90/1710 100/1710 110/1710 120/1710 130/1710 140/1710 150/1710 160/1710 170/1710 180/1710 190/1710 200/1710 210/1710 220/1710 230/1710 240/1710 250/1710 260/1710 270/1710 \n",
            "280/1710 290/1710 300/1710 310/1710 320/1710 330/1710 340/1710 350/1710 360/1710 370/1710 380/1710 390/1710 400/1710 410/1710 420/1710 430/1710 440/1710 450/1710 460/1710 470/1710 480/1710 490/1710 500/1710 510/1710 520/1710 530/1710 540/1710 \n",
            "550/1710 560/1710 570/1710 580/1710 590/1710 600/1710 610/1710 620/1710 630/1710 640/1710 650/1710 660/1710 670/1710 680/1710 690/1710 700/1710 710/1710 720/1710 730/1710 740/1710 750/1710 760/1710 770/1710 780/1710 790/1710 800/1710 810/1710 \n",
            "820/1710 830/1710 840/1710 850/1710 860/1710 870/1710 880/1710 890/1710 900/1710 910/1710 920/1710 930/1710 940/1710 950/1710 960/1710 970/1710 980/1710 990/1710 1000/1710 1010/1710 1020/1710 1030/1710 1040/1710 1050/1710 1060/1710 1070/1710 1080/1710 \n",
            "1090/1710 1100/1710 1110/1710 1120/1710 1130/1710 1140/1710 1150/1710 1160/1710 1170/1710 1180/1710 1190/1710 1200/1710 1210/1710 1220/1710 1230/1710 1240/1710 1250/1710 1260/1710 1270/1710 1280/1710 1290/1710 1300/1710 1310/1710 1320/1710 1330/1710 1340/1710 1350/1710 \n",
            "1360/1710 1370/1710 1380/1710 1390/1710 1400/1710 1410/1710 1420/1710 1430/1710 1440/1710 1450/1710 1460/1710 1470/1710 1480/1710 1490/1710 1500/1710 1510/1710 1520/1710 1530/1710 1540/1710 1550/1710 1560/1710 1570/1710 1580/1710 1590/1710 1600/1710 1610/1710 1620/1710 \n",
            "1630/1710 1640/1710 1650/1710 1660/1710 1670/1710 1680/1710 1690/1710 1700/1710 \n",
            "File Number 2: 90/1710 100/1710 110/1710 120/1710 130/1710 140/1710 150/1710 160/1710 170/1710 180/1710 190/1710 200/1710 210/1710 220/1710 230/1710 240/1710 250/1710 260/1710 270/1710 \n",
            "280/1710 290/1710 300/1710 310/1710 320/1710 330/1710 340/1710 350/1710 360/1710 370/1710 380/1710 390/1710 400/1710 410/1710 420/1710 430/1710 440/1710 450/1710 460/1710 470/1710 480/1710 490/1710 500/1710 510/1710 520/1710 530/1710 540/1710 \n",
            "550/1710 560/1710 570/1710 580/1710 590/1710 600/1710 610/1710 620/1710 630/1710 640/1710 650/1710 660/1710 670/1710 680/1710 690/1710 700/1710 710/1710 720/1710 730/1710 740/1710 750/1710 760/1710 770/1710 780/1710 790/1710 800/1710 810/1710 \n",
            "820/1710 830/1710 840/1710 850/1710 860/1710 870/1710 880/1710 890/1710 900/1710 910/1710 920/1710 930/1710 940/1710 950/1710 960/1710 970/1710 980/1710 990/1710 1000/1710 1010/1710 1020/1710 1030/1710 1040/1710 1050/1710 1060/1710 1070/1710 1080/1710 \n",
            "1090/1710 1100/1710 1110/1710 1120/1710 1130/1710 1140/1710 1150/1710 1160/1710 1170/1710 1180/1710 1190/1710 1200/1710 1210/1710 1220/1710 1230/1710 1240/1710 1250/1710 1260/1710 1270/1710 1280/1710 1290/1710 1300/1710 1310/1710 1320/1710 1330/1710 1340/1710 1350/1710 \n",
            "1360/1710 1370/1710 1380/1710 1390/1710 1400/1710 1410/1710 1420/1710 1430/1710 1440/1710 1450/1710 1460/1710 1470/1710 1480/1710 1490/1710 1500/1710 1510/1710 1520/1710 1530/1710 1540/1710 1550/1710 1560/1710 1570/1710 1580/1710 1590/1710 1600/1710 1610/1710 1620/1710 \n",
            "1630/1710 1640/1710 1650/1710 1660/1710 1670/1710 1680/1710 1690/1710 1700/1710 \n",
            "File Number 3: 90/2859 100/2859 110/2859 120/2859 130/2859 140/2859 150/2859 160/2859 170/2859 180/2859 190/2859 200/2859 210/2859 220/2859 230/2859 240/2859 250/2859 260/2859 270/2859 \n",
            "280/2859 290/2859 300/2859 310/2859 320/2859 330/2859 340/2859 350/2859 360/2859 370/2859 380/2859 390/2859 400/2859 410/2859 420/2859 430/2859 440/2859 450/2859 460/2859 470/2859 480/2859 490/2859 500/2859 510/2859 520/2859 530/2859 540/2859 \n",
            "550/2859 560/2859 570/2859 580/2859 590/2859 600/2859 610/2859 620/2859 630/2859 640/2859 650/2859 660/2859 670/2859 680/2859 690/2859 700/2859 710/2859 720/2859 730/2859 740/2859 750/2859 760/2859 770/2859 780/2859 790/2859 800/2859 810/2859 \n",
            "820/2859 830/2859 840/2859 850/2859 860/2859 870/2859 880/2859 890/2859 900/2859 910/2859 920/2859 930/2859 940/2859 950/2859 960/2859 970/2859 980/2859 990/2859 1000/2859 1010/2859 1020/2859 1030/2859 1040/2859 1050/2859 1060/2859 1070/2859 1080/2859 \n",
            "1090/2859 1100/2859 1110/2859 1120/2859 1130/2859 1140/2859 1150/2859 1160/2859 1170/2859 1180/2859 1190/2859 1200/2859 1210/2859 1220/2859 1230/2859 1240/2859 1250/2859 1260/2859 1270/2859 1280/2859 1290/2859 1300/2859 1310/2859 1320/2859 1330/2859 1340/2859 1350/2859 \n",
            "1360/2859 1370/2859 1380/2859 1390/2859 1400/2859 1410/2859 1420/2859 1430/2859 1440/2859 1450/2859 1460/2859 1470/2859 1480/2859 1490/2859 1500/2859 1510/2859 1520/2859 1530/2859 1540/2859 1550/2859 1560/2859 1570/2859 1580/2859 1590/2859 1600/2859 1610/2859 1620/2859 \n",
            "1630/2859 1640/2859 1650/2859 1660/2859 1670/2859 1680/2859 1690/2859 1700/2859 1710/2859 1720/2859 1730/2859 1740/2859 1750/2859 1760/2859 1770/2859 1780/2859 1790/2859 1800/2859 1810/2859 1820/2859 1830/2859 1840/2859 1850/2859 1860/2859 1870/2859 1880/2859 1890/2859 \n",
            "1900/2859 1910/2859 1920/2859 1930/2859 1940/2859 1950/2859 1960/2859 1970/2859 1980/2859 1990/2859 2000/2859 2010/2859 2020/2859 2030/2859 2040/2859 2050/2859 2060/2859 2070/2859 2080/2859 2090/2859 2100/2859 2110/2859 2120/2859 2130/2859 2140/2859 2150/2859 2160/2859 \n",
            "2170/2859 2180/2859 2190/2859 2200/2859 2210/2859 2220/2859 2230/2859 2240/2859 2250/2859 2260/2859 2270/2859 2280/2859 2290/2859 2300/2859 2310/2859 2320/2859 2330/2859 2340/2859 2350/2859 2360/2859 2370/2859 2380/2859 2390/2859 2400/2859 2410/2859 2420/2859 2430/2859 \n",
            "2440/2859 2450/2859 2460/2859 2470/2859 2480/2859 2490/2859 2500/2859 2510/2859 2520/2859 2530/2859 2540/2859 2550/2859 2560/2859 2570/2859 2580/2859 2590/2859 2600/2859 2610/2859 2620/2859 2630/2859 2640/2859 2650/2859 2660/2859 2670/2859 2680/2859 2690/2859 2700/2859 \n",
            "2710/2859 2720/2859 2730/2859 2740/2859 2750/2859 2760/2859 2770/2859 2780/2859 2790/2859 2800/2859 2810/2859 2820/2859 2830/2859 2840/2859 2850/2859 \n",
            "File Number 4: 90/1710 100/1710 110/1710 120/1710 130/1710 140/1710 150/1710 160/1710 170/1710 180/1710 190/1710 200/1710 210/1710 220/1710 230/1710 240/1710 250/1710 260/1710 270/1710 \n",
            "280/1710 290/1710 300/1710 310/1710 320/1710 330/1710 340/1710 350/1710 360/1710 370/1710 380/1710 390/1710 400/1710 410/1710 420/1710 430/1710 440/1710 450/1710 460/1710 470/1710 480/1710 490/1710 500/1710 510/1710 520/1710 530/1710 540/1710 \n",
            "550/1710 560/1710 570/1710 580/1710 590/1710 600/1710 610/1710 620/1710 630/1710 640/1710 650/1710 660/1710 670/1710 680/1710 690/1710 700/1710 710/1710 720/1710 730/1710 740/1710 750/1710 760/1710 770/1710 780/1710 790/1710 800/1710 810/1710 \n",
            "820/1710 830/1710 840/1710 850/1710 860/1710 870/1710 880/1710 890/1710 900/1710 910/1710 920/1710 930/1710 940/1710 950/1710 960/1710 970/1710 980/1710 990/1710 1000/1710 1010/1710 1020/1710 1030/1710 1040/1710 1050/1710 1060/1710 1070/1710 1080/1710 \n",
            "1090/1710 1100/1710 1110/1710 1120/1710 1130/1710 1140/1710 1150/1710 1160/1710 1170/1710 1180/1710 1190/1710 1200/1710 1210/1710 1220/1710 1230/1710 1240/1710 1250/1710 1260/1710 1270/1710 1280/1710 1290/1710 1300/1710 1310/1710 1320/1710 1330/1710 1340/1710 1350/1710 \n",
            "1360/1710 1370/1710 1380/1710 1390/1710 1400/1710 1410/1710 1420/1710 1430/1710 1440/1710 1450/1710 1460/1710 1470/1710 1480/1710 1490/1710 1500/1710 1510/1710 1520/1710 1530/1710 1540/1710 1550/1710 1560/1710 1570/1710 1580/1710 1590/1710 1600/1710 1610/1710 1620/1710 \n",
            "1630/1710 1640/1710 1650/1710 1660/1710 1670/1710 1680/1710 1690/1710 1700/1710 \n",
            "File Number 5: 90/1710 100/1710 110/1710 120/1710 130/1710 140/1710 150/1710 160/1710 170/1710 180/1710 190/1710 200/1710 210/1710 220/1710 230/1710 240/1710 250/1710 260/1710 270/1710 \n",
            "280/1710 290/1710 300/1710 310/1710 320/1710 330/1710 340/1710 350/1710 360/1710 370/1710 380/1710 390/1710 400/1710 410/1710 420/1710 430/1710 440/1710 450/1710 460/1710 470/1710 480/1710 490/1710 500/1710 510/1710 520/1710 530/1710 540/1710 \n",
            "550/1710 560/1710 570/1710 580/1710 590/1710 600/1710 610/1710 620/1710 630/1710 640/1710 650/1710 660/1710 670/1710 680/1710 690/1710 700/1710 710/1710 720/1710 730/1710 740/1710 750/1710 760/1710 770/1710 780/1710 790/1710 800/1710 810/1710 \n",
            "820/1710 830/1710 840/1710 850/1710 860/1710 870/1710 880/1710 890/1710 900/1710 910/1710 920/1710 930/1710 940/1710 950/1710 960/1710 970/1710 980/1710 990/1710 1000/1710 1010/1710 1020/1710 1030/1710 1040/1710 1050/1710 1060/1710 1070/1710 1080/1710 \n",
            "1090/1710 1100/1710 1110/1710 1120/1710 1130/1710 1140/1710 1150/1710 1160/1710 1170/1710 1180/1710 1190/1710 1200/1710 1210/1710 1220/1710 1230/1710 1240/1710 1250/1710 1260/1710 1270/1710 1280/1710 1290/1710 1300/1710 1310/1710 1320/1710 1330/1710 1340/1710 1350/1710 \n",
            "1360/1710 1370/1710 1380/1710 1390/1710 1400/1710 1410/1710 1420/1710 1430/1710 1440/1710 1450/1710 1460/1710 1470/1710 1480/1710 1490/1710 1500/1710 1510/1710 1520/1710 1530/1710 1540/1710 1550/1710 1560/1710 1570/1710 1580/1710 1590/1710 1600/1710 1610/1710 1620/1710 \n",
            "1630/1710 1640/1710 1650/1710 1660/1710 1670/1710 1680/1710 1690/1710 1700/1710 \n",
            "File Number 6: 90/2859 100/2859 110/2859 120/2859 130/2859 140/2859 150/2859 160/2859 170/2859 180/2859 190/2859 200/2859 210/2859 220/2859 230/2859 240/2859 250/2859 260/2859 270/2859 \n",
            "280/2859 290/2859 300/2859 310/2859 320/2859 330/2859 340/2859 350/2859 360/2859 370/2859 380/2859 390/2859 400/2859 410/2859 420/2859 430/2859 440/2859 450/2859 460/2859 470/2859 480/2859 490/2859 500/2859 510/2859 520/2859 530/2859 540/2859 \n",
            "550/2859 560/2859 570/2859 580/2859 590/2859 600/2859 610/2859 620/2859 630/2859 640/2859 650/2859 660/2859 670/2859 680/2859 690/2859 700/2859 710/2859 720/2859 730/2859 740/2859 750/2859 760/2859 770/2859 780/2859 790/2859 800/2859 810/2859 \n",
            "820/2859 830/2859 840/2859 850/2859 860/2859 870/2859 880/2859 890/2859 900/2859 910/2859 920/2859 930/2859 940/2859 950/2859 960/2859 970/2859 980/2859 990/2859 1000/2859 1010/2859 1020/2859 1030/2859 1040/2859 1050/2859 1060/2859 1070/2859 1080/2859 \n",
            "1090/2859 1100/2859 1110/2859 1120/2859 1130/2859 1140/2859 1150/2859 1160/2859 1170/2859 1180/2859 1190/2859 1200/2859 1210/2859 1220/2859 1230/2859 1240/2859 1250/2859 1260/2859 1270/2859 1280/2859 1290/2859 1300/2859 1310/2859 1320/2859 1330/2859 1340/2859 1350/2859 \n",
            "1360/2859 1370/2859 1380/2859 1390/2859 1400/2859 1410/2859 1420/2859 1430/2859 1440/2859 1450/2859 1460/2859 1470/2859 1480/2859 1490/2859 1500/2859 1510/2859 1520/2859 1530/2859 1540/2859 1550/2859 1560/2859 1570/2859 1580/2859 1590/2859 1600/2859 1610/2859 1620/2859 \n",
            "1630/2859 1640/2859 1650/2859 1660/2859 1670/2859 1680/2859 1690/2859 1700/2859 1710/2859 1720/2859 1730/2859 1740/2859 1750/2859 1760/2859 1770/2859 1780/2859 1790/2859 1800/2859 1810/2859 1820/2859 1830/2859 1840/2859 1850/2859 1860/2859 1870/2859 1880/2859 1890/2859 \n",
            "1900/2859 1910/2859 1920/2859 1930/2859 1940/2859 1950/2859 1960/2859 1970/2859 1980/2859 1990/2859 2000/2859 2010/2859 2020/2859 2030/2859 2040/2859 2050/2859 2060/2859 2070/2859 2080/2859 2090/2859 2100/2859 2110/2859 2120/2859 2130/2859 2140/2859 2150/2859 2160/2859 \n",
            "2170/2859 2180/2859 2190/2859 2200/2859 2210/2859 2220/2859 2230/2859 2240/2859 2250/2859 2260/2859 2270/2859 2280/2859 2290/2859 2300/2859 2310/2859 2320/2859 2330/2859 2340/2859 2350/2859 2360/2859 2370/2859 2380/2859 2390/2859 2400/2859 2410/2859 2420/2859 2430/2859 \n",
            "2440/2859 2450/2859 2460/2859 2470/2859 2480/2859 2490/2859 2500/2859 2510/2859 2520/2859 2530/2859 2540/2859 2550/2859 2560/2859 2570/2859 2580/2859 2590/2859 2600/2859 2610/2859 2620/2859 2630/2859 2640/2859 2650/2859 2660/2859 2670/2859 2680/2859 2690/2859 2700/2859 \n",
            "2710/2859 2720/2859 2730/2859 2740/2859 2750/2859 2760/2859 2770/2859 2780/2859 2790/2859 2800/2859 2810/2859 2820/2859 2830/2859 2840/2859 2850/2859 \n",
            " \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12048"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "how_many_future_candles = 15\n",
        "how_many_past_candles = 1\n",
        "each_row_past = 85\n",
        "\n",
        "\n",
        "global which_future_or_past\n",
        "which_future_or_past = None\n",
        "if how_many_future_candles > how_many_past_candles:\n",
        "    which_future_or_past = how_many_future_candles\n",
        "else:\n",
        "    which_future_or_past = how_many_past_candles\n",
        "folder_name = start(how_many_future_candles,how_many_past_candles,each_row_past)\n",
        "len(os.listdir(f\"/content/extracted/{folder_name}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQpQvhf-pwpR",
        "outputId": "065b2295-5427-4d34-ba35-f8928d12c7e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12048, 128, 128)\n",
            "xTrain : 9638 \\ xTest : 2410\n",
            "yn: 6191 nn: 5857\n"
          ]
        }
      ],
      "source": [
        "#folder_name = \"165913\"\n",
        "\n",
        "shutil.make_archive(folder_name,\"zip\",f\"/content/extracted/{folder_name}/\")\n",
        "#shutil.unpack_archive(f\"/content/{folder_name}.zip\",f\"/content/extracted/{folder_name}\")\n",
        "label = []\n",
        "data  = []\n",
        "files = os.listdir(f\"/content/extracted/{folder_name}/\")\n",
        "for i, image_name in enumerate(files):\n",
        "  if image_name.split(\".\")[1] == \"png\":\n",
        "    image = cv2.imread(f\"/content/extracted/{folder_name}\"+\"/\"+image_name,0)\n",
        "    dim = (128, 128)\n",
        "    resized = cv2.resize(image, dim)\n",
        "    data.append(np.array(resized))\n",
        "    sugg = image_name.split(\"_\")[1].split(\".\")[0]\n",
        "    label.append(int(sugg))\n",
        "data = np.array(data)\n",
        "data = data / 255\n",
        "print(data.shape)\n",
        "xTrain , xTest , yTrain , yTest = train_test_split(data,label,test_size=0.2,random_state=99)\n",
        "data = None\n",
        "label = None\n",
        "print(f\"xTrain : {len(xTrain)} \\\\ xTest : {len(xTest)}\")\n",
        "nytrain = []\n",
        "nytest = []\n",
        "yn = 0\n",
        "nn = 0\n",
        "for i in yTrain:\n",
        "  if i == 1:\n",
        "    nytrain.append([1,0])\n",
        "    yn += 1\n",
        "  else:\n",
        "    nytrain.append([0,1])\n",
        "    nn += 1\n",
        "for i in yTest:\n",
        "  if i == 1:\n",
        "    nytest.append([1,0])\n",
        "    yn += 1\n",
        "  else:\n",
        "    nytest.append([0,1])\n",
        "    nn += 1\n",
        "yTrain = np.array(nytrain)\n",
        "yTest = np.array(nytest)\n",
        "print(f\"yn: {yn} nn: {nn}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(xTest,yTest)"
      ],
      "metadata": {
        "id": "qjRn1rKTEFSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9stbJK8Nx_0c",
        "outputId": "8f0802e8-c201-4c6d-9ff8-0655d8f6a8fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 127, 127, 128)     640       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 42, 42, 128)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 41, 41, 32)        16416     \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 20, 20, 32)       0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 19, 19, 32)        4128      \n",
            "                                                                 \n",
            " average_pooling2d_1 (Averag  (None, 9, 9, 32)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 8, 16)          2064      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 800)               820000    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 800)               640800    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 750)               600750    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 750)               563250    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2)                 1502      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,649,550\n",
            "Trainable params: 2,649,550\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(128,     (2, 2),activation=\"relu\", input_shape=(xTrain.shape[1], xTrain.shape[2],1)))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "model.add(Conv2D(32,      (2, 2),activation=\"relu\",)) \n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(32,      (2, 2),activation=\"relu\",)) \n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(16,      (2, 2),activation=\"relu\",)) \n",
        "model.add(Flatten())\n",
        "model.add(Dense(800,activation=\"relu\"))\n",
        "model.add(Dense(800,activation=\"relu\"))\n",
        "model.add(Dense(750,activation=\"relu\"))\n",
        "model.add(Dense(750,activation=\"relu\"))\n",
        "model.add(Dense(2,activation=\"sigmoid\"))\n",
        "\n",
        "adamax = tf.keras.optimizers.Adamax(\n",
        "    learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adamax,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cytWxowTyInc",
        "outputId": "3d6a87d4-699c-4d92-d650-39d7606b12b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "603/603 [==============================] - 273s 449ms/step - loss: 0.6933 - accuracy: 0.5092 - val_loss: 0.6926 - val_accuracy: 0.5191\n",
            "Epoch 2/20\n",
            "603/603 [==============================] - 270s 448ms/step - loss: 0.6930 - accuracy: 0.5126 - val_loss: 0.6925 - val_accuracy: 0.5191\n",
            "Epoch 3/20\n",
            "603/603 [==============================] - 264s 439ms/step - loss: 0.6931 - accuracy: 0.5110 - val_loss: 0.6924 - val_accuracy: 0.5191\n",
            "Epoch 4/20\n",
            "603/603 [==============================] - 269s 446ms/step - loss: 0.6833 - accuracy: 0.5525 - val_loss: 0.6731 - val_accuracy: 0.5768\n",
            "Epoch 5/20\n",
            "603/603 [==============================] - 270s 448ms/step - loss: 0.6183 - accuracy: 0.6537 - val_loss: 0.5479 - val_accuracy: 0.7270\n",
            "Epoch 6/20\n",
            "603/603 [==============================] - 271s 450ms/step - loss: 0.4584 - accuracy: 0.7894 - val_loss: 0.4222 - val_accuracy: 0.8141\n",
            "Epoch 7/20\n",
            "603/603 [==============================] - 270s 448ms/step - loss: 0.3254 - accuracy: 0.8672 - val_loss: 0.4454 - val_accuracy: 0.8170\n",
            "Epoch 8/20\n",
            "603/603 [==============================] - 270s 447ms/step - loss: 0.2523 - accuracy: 0.9003 - val_loss: 0.3297 - val_accuracy: 0.8610\n",
            "Epoch 9/20\n",
            "603/603 [==============================] - 267s 443ms/step - loss: 0.2035 - accuracy: 0.9193 - val_loss: 0.3437 - val_accuracy: 0.8768\n",
            "Epoch 10/20\n",
            "603/603 [==============================] - 266s 442ms/step - loss: 0.1710 - accuracy: 0.9298 - val_loss: 0.3196 - val_accuracy: 0.8793\n",
            "Epoch 11/20\n",
            "603/603 [==============================] - 259s 429ms/step - loss: 0.1424 - accuracy: 0.9432 - val_loss: 0.3105 - val_accuracy: 0.8900\n",
            "Epoch 12/20\n",
            " 92/603 [===>..........................] - ETA: 3:26 - loss: 0.1058 - accuracy: 0.9552"
          ]
        }
      ],
      "source": [
        "filepath = \"/content/checkpoints/{epoch:02d}-{val_accuracy:.2f}.h5\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "#model.fit(xTrain,yTrain,batch_size=64,epochs=30,validation_data=(xTest,yTest), callbacks=model_checkpoint_callback)\n",
        "model.fit(xTrain,yTrain,batch_size=16,epochs=20,validation_data=(xTest,yTest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgUhFkNfwDtr"
      },
      "outputs": [],
      "source": [
        "model.save(f\"1.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgz7BAj5wwlN"
      },
      "outputs": [],
      "source": [
        "symbol,period,interval=\"btc-usd\",\"10d\",\"5m\"\n",
        "data = yf.download(tickers=symbol,period=period,interval=interval)\n",
        "print(data)\n",
        "data = np.array(data)\n",
        "data = data.astype(float)\n",
        "i = -1\n",
        "rows = data[i-each_row_past:i, :]\n",
        "df = pd.DataFrame(rows, columns=[\"Open\", \"High\", \"Low\", \"Close\",\"Adj Close\",\"Volume\"])\n",
        "df.index.name = \"Date\"\n",
        "df.index = pd.to_datetime(df.index)\n",
        "fig, _a = mpl.plot(df, type=\"candle\", style=\"yahoo\", axisoff=True,\n",
        "                            returnfig=True, tight_layout=True,figsize =(1.5,1.5))\n",
        "fig.savefig(\"picture.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdb2r1l8yB-0"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread(\"/content/picture.png\",0)\n",
        "dim = (128, 128)\n",
        "resized = cv2.resize(image, dim)\n",
        "data = np.array(resized)\n",
        "model.predict([[data.reshape(1,128,128,1)]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8nJ2wI-ynZw"
      },
      "outputs": [],
      "source": [
        "tv = TvDatafeed()\n",
        "data = tv.get_hist(symbol=\"btcusdt\",exchange=\"binance\",interval=Interval.in_5_minute,n_bars=1000)\n",
        "data = np.array(data)\n",
        "i = -1\n",
        "rows = data[i-each_row_past:i, 1:5]\n",
        "rows.shape\n",
        "df = pd.DataFrame(rows, columns=[\"Open\", \"High\", \"Low\", \"Close\"])\n",
        "df.index.name = \"Date\"\n",
        "df.index = pd.to_datetime(df.index)\n",
        "df = df.apply(lambda col:pd.to_numeric(col, errors='coerce'))\n",
        "fig, _a = mpl.plot(df, type=\"candle\", style=\"yahoo\", axisoff=True,\n",
        "                            returnfig=True, tight_layout=True,figsize =(1.5,1.5))\n",
        "fig.savefig(\"picture1.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpTFxbUd5mXt"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread(\"/content/picture1.png\",0)\n",
        "dim = (128, 128)\n",
        "resized = cv2.resize(image, dim)\n",
        "data = np.array(resized)\n",
        "model.predict([[data.reshape(1,128,128,1)]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B7lUM_NBNxj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3WBt2p6M86D"
      },
      "outputs": [],
      "source": [
        "lst = []\n",
        "while True:\n",
        "  ans = input()\n",
        "  if ans == \"exit\":\n",
        "    break\n",
        "  lst.append(int(ans))\n",
        "print(sum(max),len(lst))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "G26",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}