{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5bxbCoe9do9"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance\n",
        "!pip install yahooquery\n",
        "from yahooquery import Screener\n",
        "import yfinance as yf   \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import tensorflow as tf\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sys import getsizeof\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB_YMoe09qVP"
      },
      "outputs": [],
      "source": [
        "def read_syms_from_txt():  \n",
        "  with open(\"syms.txt\",\"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "  lst = []\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    lst.append(line)\n",
        "  symbols = lst\n",
        "  return symbols\n",
        "def get_crypto_syms():\n",
        "  #'all_cryptocurrencies_au','all_cryptocurrencies_ca','all_cryptocurrencies_eu','all_cryptocurrencies_gb','all_cryptocurrencies_in',\n",
        "   screens = [\n",
        " 'all_cryptocurrencies_us']\n",
        "   s = Screener()\n",
        "   symbols = []\n",
        "   for i in screens:\n",
        "      data = s.get_screeners(i, count=250)\n",
        "      dicts = data[i]['quotes']\n",
        "      syms = [d['symbol'] for d in dicts]\n",
        "      for sym in syms:\n",
        "        symbols.append(sym)\n",
        "   #print(len(symbols))\n",
        "   #pieces = 15\n",
        "   # new_arrays = np.array_split(symbols, pieces)\n",
        "   return symbols\n",
        "def spliting(data):\n",
        "  X = data.drop([\"suggestion\"],axis=1)\n",
        "  y = data[\"suggestion\"]\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.1)\n",
        "  print(xTrain.shape,end=\" \")\n",
        "  print(yTrain.shape)\n",
        "  print(xTest.shape,end=\" \")\n",
        "  print(yTest.shape)\n",
        "  return xTrain, xTest, yTrain, yTest\n",
        "def extract_data(df):\n",
        "    rows = []\n",
        "    for each in range(13,df.shape[0]-1):\n",
        "        sugg = 0\n",
        "        if df[each][3] > df[each][0]:\n",
        "          sugg = 1\n",
        "        row = [\n",
        "                df[each-12][3],\n",
        "                df[each-11][3],\n",
        "                df[each-10][3],\n",
        "                df[each-9][3],\n",
        "                df[each-8][3],\n",
        "                df[each-7][3],\n",
        "                df[each-6][3],\n",
        "                df[each-5][3],\n",
        "                df[each-4][3],\n",
        "                df[each-3][3],\n",
        "                df[each-2][3],\n",
        "                df[each-1][3],\n",
        "                sugg\n",
        "        ]\n",
        "        rows.append(row)\n",
        "    return rows\n",
        "def row_scaler(df):\n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  last_column = df.iloc[: , -1]\n",
        "  df = df.drop(columns=df.columns[-1], axis=1)\n",
        "  scaled = pd.DataFrame(scaler.fit_transform(df.T).T,dtype=object,columns=[\"12_candels_close\",\"11_candels_close\",\"10_candels_close\",\"9_candels_close\",\"8_candels_close\",\"7_candels_close\", \"6_candels_close\",\"5_candels_close\",\"4_candels_close\",\"3_candels_close\",\"2_candels_close\",\"1_candels_close\"])\n",
        "  scaled[\"suggestion\"] = last_column\n",
        "  return scaled\n",
        "def column_scaler(df):\n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  last_column = df.iloc[: , -1]\n",
        "  df = df.drop(columns=df.columns[-1], axis=1)\n",
        "  df_scaled = scaler.fit_transform(df.to_numpy())\n",
        "  df_scaled = pd.DataFrame(df_scaled, columns=[\"12_candels_close\",\"11_candels_close\",\"10_candels_close\",\"9_candels_close\",\"8_candels_close\",\"7_candels_close\", \"6_candels_close\",\"5_candels_close\",\"4_candels_close\",\"3_candels_close\",\"2_candels_close\",\"1_candels_close\"])\n",
        "  df_scaled[\"suggestion\"] = last_column\n",
        "  return df_scaled\n",
        "def stick_dfs(dfs):\n",
        "  dataframe = dfs[0]\n",
        "  for i in range(1,len(dfs)):\n",
        "     dataframe = pd.concat([dataframe,dfs[i]], ignore_index = True)\n",
        "  dataframe = dataframe.dropna()\n",
        "  dataframe = dataframe.astype(float)\n",
        "  return dataframe\n",
        "def process(dfs): \n",
        "   fixed_dfs = []\n",
        "   for df in dfs:\n",
        "      df = np.array(df)\n",
        "      df = extract_data(df)\n",
        "      df = pd.DataFrame(df,columns = [\"12_candels_close\",\"11_candels_close\",\"10_candels_close\",\"9_candels_close\",\"8_candels_close\",\"7_candels_close\", \"6_candels_close\",\"5_candels_close\",\"4_candels_close\",\"3_candels_close\",\"2_candels_close\",\"1_candels_close\",\"suggestion\"])\n",
        "      df = row_scaler(df)\n",
        "      #df = column_scaler(df)\n",
        "      fixed_dfs.append(df)\n",
        "   df = stick_dfs(fixed_dfs)\n",
        "   return df   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIAuU_ILbU27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "8799060a-75e3-4a00-a75b-7d94b567f2c0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
            "Traceback (most recent call last):\n",
            "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "symbols = [\"MSFT\",\"AAPL\",\"GOOG\",\"TSLA\",\"AMZN\"]\n",
        "#symbols = [\"BTC-USD\",\"LTC-USD\",\"TRX-USD\",\"XRP-USD\",\"ETH-USD\",\"BNB-USD\",\"DASH-USD\",\"VET-USD\",\"LINK-USD\",\"ADA-USD\",\"DOT-USD\",\"SOL-USD\",\"BCH-USD\",\"FTT-USD\",\"FIL-USD\",\"XMR-USD\"]\n",
        "#symbols = [\"AAPL\",\"MSFT\",\"TSLA\",\"GOOG\"]\n",
        "#symbols = [\"BTC-USD\",\"ETH-USD\"]\n",
        "#symbols = [\"BTC-USD\"]\n",
        "\n",
        "#symbols = get_crypto_syms()\n",
        "dfs = []\n",
        "for symbol in symbols:\n",
        "           data = yf.download(symbol,period=\"MAX\",interval=\"1d\",progress=False)\n",
        "           if data.empty :\n",
        "             print(\"Passing...\")\n",
        "           else:\n",
        "               dfs.append(data)\n",
        "ndfs = dfs\n",
        "dfs = []\n",
        "for df in ndfs:\n",
        "        if df.shape[0] > 14:\n",
        "          dfs.append(df)\n",
        "data = process(dfs)\n",
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%H%M%S\")\n",
        "data.to_csv(f\"{current_time}.csv\")\n",
        "xTrain, xTest, yTrain, yTest = spliting(data)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rI1oRc9VRwO"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "adam = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.0004,\n",
        "    amsgrad=True,\n",
        "    name=\"Adam\")\n",
        "\n",
        "model.add(Dense(512, activation='relu', input_shape=(xTrain.shape[1],)))\n",
        "#model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDTaW4WnX330"
      },
      "outputs": [],
      "source": [
        "model.fit(xTrain,yTrain,epochs=333,batch_size=512,validation_data=(xTest,yTest))\n",
        "model.save(f\"{model.evaluate(xTest,yTest)}.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46M8IzrdKgf8"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(xTrain,yTrain)\n",
        "print(f\"Random Forest :  {clf.score(xTest,yTest)}\")\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(xTrain, yTrain)\n",
        "print(f\"Logistic Regression: {logisticRegr.score(xTest, yTest)}\")\n",
        "nc = NearestCentroid()\n",
        "nc.fit(xTrain,yTrain)\n",
        "print(f\"Nearest Centroid: {nc.score(xTest,yTest)}\")\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(xTrain, yTrain)\n",
        "print(f\"GaussianNB: {gnb.score(xTest,yTest)}\")\n",
        "clf2 = KNeighborsClassifier(n_neighbors=2)\n",
        "clf2.fit(xTrain, yTrain)\n",
        "print(f\"K-Neighbors: {clf2.score(xTest,yTest)}\")\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(xTrain, yTrain)\n",
        "print(f\"Decision Tree: {tree.score(xTest,yTest)}\")\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
        "   max_depth=1, random_state=0).fit(xTrain, yTrain)\n",
        "gb.fit(xTrain,yTrain)\n",
        "print(f\"Gradient Boost: {gb.score(xTest, yTest)}\")\n",
        "svm = SVC()\n",
        "svm.fit(xTrain,yTrain)\n",
        "print(f\"SVM: {svm.score(xTest,yTest)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "files = os.listdir(\"/content/gdrive/MyDrive/Colab Files/\")\n",
        "\n",
        "data = pd.DataFrame()\n",
        "for file in files:\n",
        "  address = f\"/content/gdrive/MyDrive/Colab Files/{file}\"\n",
        "  df = pd.read_csv(address)\n",
        "  data = pd.concat([data,df])"
      ],
      "metadata": {
        "id": "SFwh6-67na1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symbols = read_syms_from_txt()\n",
        "len(symbols)\n",
        "pieces = 15\n",
        "new_arrays = np.array_split(symbols, pieces)\n",
        "for i in new_arrays:\n",
        "  dfs = []\n",
        "  for x in i:\n",
        "        data = yf.download(x,period=\"MAX\",interval=\"1d\",progress=False)\n",
        "        if data.empty :\n",
        "             print(\"Passing...\")\n",
        "        else:\n",
        "               dfs.append(data)\n",
        "  ndfs = dfs\n",
        "  dfs = []\n",
        "  for df in ndfs:\n",
        "        if df.shape[0] > 12:\n",
        "          dfs.append(df)\n",
        "  data = process(dfs)\n",
        "  data.to_csv(f\"{x}.csv\")         "
      ],
      "metadata": {
        "id": "IL0Gg7OMYIQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")\n"
      ],
      "metadata": {
        "id": "Nu3DTgF_qBLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "address = \"/content/gdrive/MyDrive/Colab Files/063821.csv\"\n",
        "data = pd.read_csv(address)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "RF0pHswQw7nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop([\"Unnamed: 0\"],axis=1)\n",
        "xTrain, xTest, yTrain, yTest = spliting(data)\n",
        "data"
      ],
      "metadata": {
        "id": "V83nKaaTxAhO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Binary_v42.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}